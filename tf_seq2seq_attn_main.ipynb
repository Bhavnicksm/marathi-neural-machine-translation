{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_seq2seq_attn_main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pCor_QpNH2v2",
        "Z5kGD2XaVs_I",
        "oRlvnjN2brFN",
        "BD2K-vW6YVSX",
        "MxTYP4xOZSEb",
        "PyUwjTBBDYUz"
      ],
      "authorship_tag": "ABX9TyN6KJhc8g/Bxel5O+BVrJv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5669bd3563f4a249e8b0c61970e6373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_19b95e37990b4f6784e89872e5504093",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4a3a2cc4fa1e41c2b65e1487b59533c3",
              "IPY_MODEL_65769da98a59446fb39ea88c25ed735a"
            ]
          }
        },
        "19b95e37990b4f6784e89872e5504093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a3a2cc4fa1e41c2b65e1487b59533c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e97cc506abb84c0ea8acbad031cc47a1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 40751,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 40751,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94a8d44dff4e48c7bcecf15b5984104f"
          }
        },
        "65769da98a59446fb39ea88c25ed735a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de92aaaedeb848719abc00718b0bb91f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 40751/40751 [41:59&lt;00:00, 16.17it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c85fe942a224a2d926c49e6ebb8164c"
          }
        },
        "e97cc506abb84c0ea8acbad031cc47a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94a8d44dff4e48c7bcecf15b5984104f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de92aaaedeb848719abc00718b0bb91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c85fe942a224a2d926c49e6ebb8164c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devashish-Siwatch/marathi-neural-machine-translation/blob/main/tf_seq2seq_attn_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m8OV0wClQDQ"
      },
      "source": [
        "Before beginning this notebook, ensure that you have data.csv in available in the working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diGgpmn6_hO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b697704-ac85-4542-8976-9996f2375ce8"
      },
      "source": [
        "!pip install torchtext==0.8.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKUGWddSF2c"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdbql_pf8PhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c3214e-3234-4112-f718-56603d1cdfe6"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (53.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkCLyMjMK0g_"
      },
      "source": [
        "## Hyperparameter declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VxIa3PK5ZA"
      },
      "source": [
        "from argparse import Namespace"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnp0_Z41K9OS"
      },
      "source": [
        "hype = Namespace(\r\n",
        "    LR = 0.0001,\r\n",
        "    BATCH_SIZE = 64,\r\n",
        "    NUM_EPOCHS = 100,\r\n",
        "    CLIP = 1,\r\n",
        ")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILw_JUj5MGTP"
      },
      "source": [
        "model_hype = Namespace(\r\n",
        "    EMBEDDING_SIZE = 128,\r\n",
        "    GRU_UNITS = 512,\r\n",
        "    ATTN_SIZE = 10,\r\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDHMJQQALwNZ",
        "outputId": "421e5757-c256-4dcd-da07-3f8aee7f1f3a"
      },
      "source": [
        "#example usage\r\n",
        "hype.BATCH_SIZE"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqxGUlxQNzEo",
        "outputId": "69f01e1e-fc73-489c-80d0-833e3faa4c7e"
      },
      "source": [
        "#to dict\r\n",
        "vars(hype)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BATCH_SIZE': 64, 'CLIP': 1, 'LR': 0.001, 'NUM_EPOCHS': 100}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wznA4tHCk3ZN"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRM-bUZo60qp"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2-a9XNPlOym"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "K1ftPojC5LWC",
        "outputId": "539c8284-3c0f-4515-b747-9fd95c7a934f"
      },
      "source": [
        "data = pd.read_csv('data.csv', header=None)\r\n",
        "data.columns = ['english', 'marathi']\r\n",
        "data.tail()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>marathi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40746</th>\n",
              "      <td>Just saying you don't like fish because of the...</td>\n",
              "      <td>हड्डींमुळे मासे आवडत नाही असं म्हणणं हे काय मा...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40747</th>\n",
              "      <td>The Japanese Parliament today officially elect...</td>\n",
              "      <td>आज जपानी संसदेने अधिकृतरित्या र्‍यौतारौ हाशिमो...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40748</th>\n",
              "      <td>Tom tried to sell his old VCR instead of throw...</td>\n",
              "      <td>टॉमने त्याचा जुना व्ही.सी.आर फेकून टाकण्याऐवजी...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40749</th>\n",
              "      <td>You can't view Flash content on an iPad. Howev...</td>\n",
              "      <td>आयपॅडवर फ्लॅश आशय बघता येत नाही. पण तुम्ही त्य...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40750</th>\n",
              "      <td>In 1969, Roger Miller recorded a song called \"...</td>\n",
              "      <td>१९६९मध्ये रॉजर मिलरने \"यू डोन्ट वॉन्ट माय लव्ह...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 english                                            marathi\n",
              "40746  Just saying you don't like fish because of the...  हड्डींमुळे मासे आवडत नाही असं म्हणणं हे काय मा...\n",
              "40747  The Japanese Parliament today officially elect...  आज जपानी संसदेने अधिकृतरित्या र्‍यौतारौ हाशिमो...\n",
              "40748  Tom tried to sell his old VCR instead of throw...  टॉमने त्याचा जुना व्ही.सी.आर फेकून टाकण्याऐवजी...\n",
              "40749  You can't view Flash content on an iPad. Howev...  आयपॅडवर फ्लॅश आशय बघता येत नाही. पण तुम्ही त्य...\n",
              "40750  In 1969, Roger Miller recorded a song called \"...  १९६९मध्ये रॉजर मिलरने \"यू डोन्ट वॉन्ट माय लव्ह..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La4DKCxV65JZ"
      },
      "source": [
        "### Building tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0le-1MgY5N3",
        "outputId": "331c5da9-0ad5-4c5c-f43f-2db661513f5a"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing import text\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "\r\n",
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvkz5RLnbBnq"
      },
      "source": [
        "def sep_punk(text):\r\n",
        "  for punk in string.punctuation:\r\n",
        "    text = text.replace(punk,\" \"+punk+\" \")\r\n",
        "  return text\r\n",
        "\r\n",
        "def add_init_token(sent_list):\r\n",
        "  new_sent_list = []\r\n",
        "  for sent in sent_list:\r\n",
        "    sent = sep_punk(sent)\r\n",
        "    sent = '<sos> ' + sent + ' <eos>'\r\n",
        "    new_sent_list.append(sent)\r\n",
        "\r\n",
        "  return new_sent_list    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rvtzx8peSqI"
      },
      "source": [
        "mar_list = list(data['marathi'])\r\n",
        "eng_list = list(data['english'])\r\n",
        "\r\n",
        "mar_list = add_init_token(mar_list)\r\n",
        "eng_list = add_init_token(eng_list)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MEWxWBge5KQ",
        "outputId": "f448b1ca-8cbb-4cf0-9b4d-8401d2b9c0d9"
      },
      "source": [
        "print(mar_list[100])\r\n",
        "print(eng_list[100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> मी आहे !  <eos>\n",
            "<sos> It ' s me !  <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ0NLlyiYy-a"
      },
      "source": [
        "def tokenize(sent_list):\r\n",
        "  tokenizer = text.Tokenizer(filters='', oov_token='<unk>')\r\n",
        "  tokenizer.fit_on_texts(sent_list)\r\n",
        "\r\n",
        "  tensor_list = tokenizer.texts_to_sequences(sent_list)\r\n",
        "  tensor_list = sequence.pad_sequences(tensor_list, padding='post')\r\n",
        "  \r\n",
        "  return {'Tensors': tensor_list, 'Tokenizer': tokenizer} "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fWvfmqviaYG"
      },
      "source": [
        "marathi = tokenize(sent_list=mar_list)\r\n",
        "english = tokenize(sent_list=eng_list)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvatkmpznpnx",
        "outputId": "84105517-8572-4e1a-c6be-318bda6e2243"
      },
      "source": [
        "mar_tokenizer = marathi['Tokenizer']\r\n",
        "eng_tokenizer = english['Tokenizer']\r\n",
        "\r\n",
        "print(f'The length of marathi vocab: {len(mar_tokenizer.word_index)}')\r\n",
        "print(f'The length of english vocab: {len(eng_tokenizer.word_index)}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of marathi vocab: 13842\n",
            "The length of english vocab: 5716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhYYsIzKnvY8",
        "outputId": "8b858564-a8de-4153-b90a-8bcef87bb719"
      },
      "source": [
        "mar_tensors = marathi['Tensors']\r\n",
        "eng_tensors = english['Tensors']\r\n",
        "\r\n",
        "print(f'Max length of sequence: {len(mar_tensors[0])}')\r\n",
        "print(f'Max length of sequence: {len(eng_tensors[0])}')\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length of sequence: 44\n",
            "Max length of sequence: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V2vLbF8sXd6",
        "outputId": "84740928-ddb3-4d35-cbd1-931bc31487a8"
      },
      "source": [
        "print(mar_tensors[0])\r\n",
        "print(eng_tensors[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  2 707   4   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0]\n",
            "[ 2 49  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SOsuFHSpZbA"
      },
      "source": [
        "### Implimenting TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM20hTegrjI3"
      },
      "source": [
        "from tensorflow.data import Dataset"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTSwVN5gsLY6"
      },
      "source": [
        "BUFFER_SIZE = len(mar_tensors)\r\n",
        "BATCH_SIZE = hype.BATCH_SIZE\r\n",
        "\r\n",
        "dataset = Dataset.from_tensor_slices((mar_tensors, eng_tensors)).shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\r\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDvbQm1DuduE",
        "outputId": "326d34c3-29eb-4bd0-8e2b-75d9b0ba603e"
      },
      "source": [
        "ex_mar_batch, ex_eng_batch = next(iter(dataset))\r\n",
        "print(ex_mar_batch.shape)\r\n",
        "print(ex_eng_batch.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 44)\n",
            "(64, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C35L-D1utri"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LadoJbptvE4a"
      },
      "source": [
        "from tensorflow import nn\r\n",
        "from tensorflow.keras import layers, Model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7gTP9YY64sM"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMw78u1kvHu2"
      },
      "source": [
        "class Encoder(Model):\r\n",
        "  def __init__(self, vocab_size, embedding_size, enc_units, batch_size):\r\n",
        "    \r\n",
        "    super(Encoder, self).__init__()\r\n",
        "\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.enc_units = enc_units\r\n",
        "    self.embedding = layers.Embedding(vocab_size, embedding_size)\r\n",
        "    self.gru = layers.GRU(enc_units, return_sequences= True, return_state= True, recurrent_initializer='glorot_uniform')\r\n",
        "\r\n",
        "\r\n",
        "  def call(self, x, hidden):\r\n",
        "    x = self.embedding(x)\r\n",
        "    output, state = self.gru(x, initial_state=hidden)\r\n",
        "    return output,state\r\n",
        "\r\n",
        "  def initialize_hidden_state(self):\r\n",
        "    return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKAKKKLC9a5l"
      },
      "source": [
        "mar_vocab_size = len(mar_tokenizer.word_index) + 1\r\n",
        "EMBEDDING_SIZE = model_hype.EMBEDDING_SIZE\r\n",
        "GRU_UNITS = model_hype.GRU_UNITS\r\n",
        "\r\n",
        "encoder = Encoder(mar_vocab_size, EMBEDDING_SIZE, GRU_UNITS, BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kugg8Jj1AgPV",
        "outputId": "0f19af5d-7f63-4c47-e67e-2f7f0e8ebe0c"
      },
      "source": [
        "#Example input\r\n",
        "\r\n",
        "ex_hidden = encoder.initialize_hidden_state()\r\n",
        "sample_out, sample_hidden = encoder(ex_mar_batch, ex_hidden)\r\n",
        "\r\n",
        "print(sample_out.shape)\r\n",
        "print(sample_hidden.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 44, 512)\n",
            "(64, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BwKYo4qBz0_"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqR_Q13dB5lf"
      },
      "source": [
        "class Attention(layers.Layer):\r\n",
        "  def __init__(self, units):\r\n",
        "\r\n",
        "    super(Attention, self).__init__()\r\n",
        "    self.W1 = layers.Dense(units)\r\n",
        "    self.W2 = layers.Dense(units)\r\n",
        "    self.V = layers.Dense(1)\r\n",
        "\r\n",
        "  def call(self, q, val):\r\n",
        "    #make q i.e. the hidden value into the same shape\r\n",
        "    q_with_time_axis = tf.expand_dims(q, 1)\r\n",
        "\r\n",
        "    score = self.V( nn.tanh( self.W1(q_with_time_axis) + self.W2(val) ) )\r\n",
        "\r\n",
        "    attention_w = nn.softmax(score, axis=1)\r\n",
        "\r\n",
        "    context_vec = attention_w * val\r\n",
        "    context_vec = tf.reduce_sum(context_vec, axis=1)\r\n",
        "\r\n",
        "    return context_vec, attention_w"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5mexiuZB6t5"
      },
      "source": [
        "ATTN_SIZE = model_hype.ATTN_SIZE\r\n",
        "attention = Attention(ATTN_SIZE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoyIpD_JHsn2",
        "outputId": "6a6f51c1-72b0-4203-e0fb-2c36db0d7651"
      },
      "source": [
        "#example code\r\n",
        "attn_res ,  attn_w = attention(sample_hidden, sample_out)\r\n",
        "print(attn_res.shape)\r\n",
        "print(attn_w.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 512)\n",
            "(64, 44, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCor_QpNH2v2"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMpa-eiBIALo"
      },
      "source": [
        "class Decoder(Model):\r\n",
        "\r\n",
        "  def __init__(self, vocab_size, embedding_size, dec_units, batch_size):\r\n",
        "    super(Decoder,self).__init__()\r\n",
        "\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.dec_units = dec_units\r\n",
        "    \r\n",
        "    self.embedding = layers.Embedding(vocab_size, embedding_size)\r\n",
        "    self.gru = layers.GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\r\n",
        "\r\n",
        "    self.fc = layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    self.attention = Attention(dec_units)\r\n",
        "\r\n",
        "  def call(self, x, hidden, enc_out):\r\n",
        "    context_vec, attention_w = self.attention(hidden, enc_out)\r\n",
        "\r\n",
        "    x = self.embedding(x)\r\n",
        "\r\n",
        "    x = tf.concat([tf.expand_dims(context_vec, 1), x], axis=-1)\r\n",
        "\r\n",
        "    output, state = self.gru(x)\r\n",
        "\r\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\r\n",
        "\r\n",
        "    x = self.fc(output)\r\n",
        "\r\n",
        "    return x, state, attention_w"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiZ7ouwZIBAV"
      },
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "decoder = Decoder(eng_vocab_size, EMBEDDING_SIZE, GRU_UNITS, BATCH_SIZE)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaQLmkzWVB9I",
        "outputId": "4d4b4a68-872d-4533-985c-7f741a42ab7d"
      },
      "source": [
        "#example code\r\n",
        "\r\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),sample_hidden, sample_out)\r\n",
        "\r\n",
        "print (sample_decoder_output.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 5717)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwffpIeIVLqn"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5kGD2XaVs_I"
      },
      "source": [
        "### Optimizers and Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnh5QPXBVwW9"
      },
      "source": [
        "from tensorflow.keras import optimizers as optim\r\n",
        "from tensorflow.keras import losses"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD9cEv6fV_d7"
      },
      "source": [
        "optimizer = optim.Adam(learning_rate=hype.LR)\r\n",
        "criteria = losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  loss = criteria(real, pred)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\r\n",
        "  loss = loss*mask\r\n",
        "\r\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyA6rWvdU3qP",
        "outputId": "a9a73106-08e0-4357-8f03-e89961923953"
      },
      "source": [
        "optimizer.learning_rate"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRlvnjN2brFN"
      },
      "source": [
        "### Azure Blob set-up and loading\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-be280Axbu7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7107871d-47e3-44c1-80f4-0c05e3e0f290"
      },
      "source": [
        "!pip install azure-storage-blob"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.6/dist-packages (12.7.1)\n",
            "Requirement already satisfied: msrest>=0.6.18 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob) (0.6.21)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob) (1.11.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob) (3.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob) (1.3.0)\n",
            "Requirement already satisfied: requests~=2.16 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob) (2.23.0)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob) (0.6.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from azure-core<2.0.0,>=1.10.0->azure-storage-blob) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.14.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.18->azure-storage-blob) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.18->azure-storage-blob) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.18->azure-storage-blob) (1.24.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSo4NhBweZma",
        "outputId": "ebece978-5c70-41e5-be23-972d858190dd"
      },
      "source": [
        "import os, uuid\r\n",
        "from azure.storage import blob\r\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\r\n",
        "print(__version__)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vMdZ7xGDflY"
      },
      "source": [
        "if \"tf_checkpoint\" not in os.listdir():\r\n",
        "  os.mkdir(\"tf_checkpoint\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PDZyfMTb2OF"
      },
      "source": [
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=tfmodel;AccountKey=PinzJZWJy/mFOWDgkBcCTPA9Fnfr7/qvaZSbjxQVH4YGrBt4MseqbKYjUGNKYX9PpBh+zgAk6uDrVpmvejBCiw==;EndpointSuffix=core.windows.net\""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sc3KtIbeM05"
      },
      "source": [
        "blob_service_client =  BlobServiceClient.from_connection_string(connect_str)\r\n",
        "container_client = blob_service_client.get_container_client(\"tf-ckpt\")\r\n",
        "blob_list = [blob.name for blob in container_client.list_blobs()]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYKiqdOUmDLr"
      },
      "source": [
        "for file in blob_list:\r\n",
        "  blob_client = blob_service_client.get_blob_client('tf-ckpt', file)\r\n",
        "  with open('./tf_checkpoint/'+file, \"wb\") as f:\r\n",
        "    f.write(blob_client.download_blob().readall())"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD2K-vW6YVSX"
      },
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLZIwVbXYZri"
      },
      "source": [
        "checkpoint_path = './tf_checkpoint'\r\n",
        "checkpoint = tf.train.Checkpoint(epoch=tf.Variable(1),\r\n",
        "                                 optimizer = optimizer,\r\n",
        "                                 encoder=encoder,\r\n",
        "                                 decoder=decoder,\r\n",
        "                                 )\r\n",
        "manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=1)\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clFCC5V3Qk9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "229eefe4-11f8-4b1d-d434-9f77f402dc83"
      },
      "source": [
        " manager.save()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./tf_checkpoint/ckpt-1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7faMmu3U56K",
        "outputId": "dff80aa7-2929-44f3-c7d9-d7828a276e4c"
      },
      "source": [
        "# manager.checkpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./tf_checkpoint/ckpt-10']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "CWrLTO4JWLBk",
        "outputId": "927a84ff-a61e-4f68-b32f-cd30cdcbfa3c"
      },
      "source": [
        "manager.restore_or_initialize()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-4f91b5853664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_or_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_management.py\u001b[0m in \u001b[0;36mrestore_or_initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \"\"\"\n\u001b[1;32m    844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_interval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_checkpoint_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2260\u001b[0;31m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2261\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m       raise errors_impl.NotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2146\u001b[0m     \"\"\"\n\u001b[1;32m   2147\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcheckpoint_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         options=options)\n\u001b[1;32m   1336\u001b[0m     base.CheckpointPosition(\n\u001b[0;32m-> 1337\u001b[0;31m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m     \u001b[0;31m# Attached dependencies are not attached to the root, so should be restored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m    965\u001b[0m           ._single_restoration_from_checkpoint_position(\n\u001b[1;32m    966\u001b[0m               \u001b[0mcheckpoint_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m               visit_queue=visit_queue))\n\u001b[0m\u001b[1;32m    968\u001b[0m       \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m       \u001b[0mtensor_saveables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tensor_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_single_restoration_from_checkpoint_position\u001b[0;34m(self, checkpoint_position, visit_queue)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                                                []).append(child_position)\n\u001b[1;32m   1001\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mchild_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m           \u001b[0;31m# This object's correspondence is new, so dependencies need to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m           \u001b[0;31m# visited. Delay doing it so that we get a breadth-first dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mbind_object\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    305\u001b[0m                   proto_id=slot_restoration.slot_variable_id),\n\u001b[1;32m    306\u001b[0m               \u001b[0mvariable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m               slot_name=slot_restoration.slot_name)\n\u001b[0m\u001b[1;32m    308\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# New assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_create_or_restore_slot_variable\u001b[0;34m(self, slot_variable_position, slot_name, variable)\u001b[0m\n\u001b[1;32m   1315\u001b[0m           \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m           slot_name=slot_name)\n\u001b[0m\u001b[1;32m   1318\u001b[0m       \u001b[0;31m# Slot variables are not owned by any one object (because we don't want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;31m# save the slot variable if the optimizer is saved without the non-slot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36madd_slot\u001b[0;34m(self, var, slot_name, initializer)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             initial_value=initial_value)\n\u001b[0m\u001b[1;32m    852\u001b[0m       \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0mslot_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kws)\u001b[0m\n\u001b[1;32m    235\u001b[0m                         shape=None):\n\u001b[1;32m    236\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m       shape=shape)\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1583\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1710\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m               \u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointInitialValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_initialize_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, shard_info)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# base_layer_utils.py's make_variable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     return CheckpointInitialValue(\n\u001b[0;32m---> 82\u001b[0;31m         self._checkpoint_position, shape, shard_info=shard_info)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, checkpoint_position, shape, shard_info)\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0;31m# We need to set the static shape information on the initializer if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0;31m# possible so we don't get a variable with an unknown shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m   1215\u001b[0m       raise ValueError(\n\u001b[1;32m   1216\u001b[0m           \u001b[0;34m\"Tensor's shape %s is not compatible with supplied shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m           (self.shape, shape))\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[0;31m# Methods not supported / implemented for Eager Tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Tensor's shape (5716, 128) is not compatible with supplied shape (5717, 128)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxTYP4xOZSEb"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtJK_B3xcks6"
      },
      "source": [
        "import time"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fFng4yJZVtN"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(src, trg, enc_hidden):\r\n",
        "  loss = 0\r\n",
        "\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    enc_out, enc_hidden = encoder(src, enc_hidden)\r\n",
        "\r\n",
        "    dec_hidden = enc_hidden\r\n",
        "\r\n",
        "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<sos>']]*BATCH_SIZE, 1)\r\n",
        "\r\n",
        "    for t in range(1, trg.shape[1]):\r\n",
        "\r\n",
        "      pred, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\r\n",
        "\r\n",
        "      loss += loss_function(trg[:,t], pred)\r\n",
        "\r\n",
        "      dec_input = tf.expand_dims(trg[:,t],1)\r\n",
        "\r\n",
        "  \r\n",
        "  batch_loss = loss/ int(trg.shape[1])\r\n",
        "\r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "\r\n",
        "  grads = tape.gradient(loss,variables)\r\n",
        "\r\n",
        "  optimizer.apply_gradients(zip(grads, variables))\r\n",
        "\r\n",
        "  return batch_loss"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7JXc0-gxb_TV",
        "outputId": "86073389-dbb2-4b29-9577-f666304615af"
      },
      "source": [
        "EPOCHS = 50\r\n",
        "steps_per_epoch = len(mar_tensors)//BATCH_SIZE\r\n",
        "\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  enc_hidden = encoder.initialize_hidden_state()\r\n",
        "\r\n",
        "  total_loss = 0\r\n",
        "\r\n",
        "  for (batch, (src,trg)) in enumerate(dataset.take(steps_per_epoch)):\r\n",
        "\r\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\r\n",
        "\r\n",
        "    total_loss += batch_loss\r\n",
        "\r\n",
        "    if batch%100 == 0:\r\n",
        "      print(f'Epoch {epoch} Batch {batch} Loss{batch_loss.numpy():.4f}')\r\n",
        "\r\n",
        "  \r\n",
        "  if (epoch+1)%2 == 0:\r\n",
        "    manager.save()\r\n",
        "\r\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\r\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 0 Loss0.0049\n",
            "Epoch 0 Batch 100 Loss0.0050\n",
            "Epoch 0 Batch 200 Loss0.0048\n",
            "Epoch 0 Batch 300 Loss0.0062\n",
            "Epoch 0 Batch 400 Loss0.0039\n",
            "Epoch 0 Batch 500 Loss0.0018\n",
            "Epoch 0 Batch 600 Loss0.0035\n",
            "Epoch 1 Loss 0.0053\n",
            "Time taken for 1 epoch 170.73022413253784 sec\n",
            "\n",
            "Epoch 1 Batch 0 Loss0.0020\n",
            "Epoch 1 Batch 100 Loss0.0029\n",
            "Epoch 1 Batch 200 Loss0.0038\n",
            "Epoch 1 Batch 300 Loss0.0036\n",
            "Epoch 1 Batch 400 Loss0.0029\n",
            "Epoch 1 Batch 500 Loss0.0013\n",
            "Epoch 1 Batch 600 Loss0.0038\n",
            "Epoch 2 Loss 0.0028\n",
            "Time taken for 1 epoch 121.86319851875305 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss0.0016\n",
            "Epoch 2 Batch 100 Loss0.0017\n",
            "Epoch 2 Batch 200 Loss0.0052\n",
            "Epoch 2 Batch 300 Loss0.0023\n",
            "Epoch 2 Batch 400 Loss0.0029\n",
            "Epoch 2 Batch 500 Loss0.0044\n",
            "Epoch 2 Batch 600 Loss0.0021\n",
            "Epoch 3 Loss 0.0022\n",
            "Time taken for 1 epoch 121.60438752174377 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss0.0016\n",
            "Epoch 3 Batch 100 Loss0.0019\n",
            "Epoch 3 Batch 200 Loss0.0034\n",
            "Epoch 3 Batch 300 Loss0.0014\n",
            "Epoch 3 Batch 400 Loss0.0014\n",
            "Epoch 3 Batch 500 Loss0.0008\n",
            "Epoch 3 Batch 600 Loss0.0013\n",
            "Epoch 4 Loss 0.0019\n",
            "Time taken for 1 epoch 121.96021747589111 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss0.0015\n",
            "Epoch 4 Batch 100 Loss0.0015\n",
            "Epoch 4 Batch 200 Loss0.0017\n",
            "Epoch 4 Batch 300 Loss0.0014\n",
            "Epoch 4 Batch 400 Loss0.0018\n",
            "Epoch 4 Batch 500 Loss0.0007\n",
            "Epoch 4 Batch 600 Loss0.0026\n",
            "Epoch 5 Loss 0.0017\n",
            "Time taken for 1 epoch 121.90962409973145 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss0.0012\n",
            "Epoch 5 Batch 100 Loss0.0020\n",
            "Epoch 5 Batch 200 Loss0.0031\n",
            "Epoch 5 Batch 300 Loss0.0007\n",
            "Epoch 5 Batch 400 Loss0.0026\n",
            "Epoch 5 Batch 500 Loss0.0015\n",
            "Epoch 5 Batch 600 Loss0.0008\n",
            "Epoch 6 Loss 0.0017\n",
            "Time taken for 1 epoch 121.85828590393066 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss0.0015\n",
            "Epoch 6 Batch 100 Loss0.0027\n",
            "Epoch 6 Batch 200 Loss0.0004\n",
            "Epoch 6 Batch 300 Loss0.0030\n",
            "Epoch 6 Batch 400 Loss0.0004\n",
            "Epoch 6 Batch 500 Loss0.0026\n",
            "Epoch 6 Batch 600 Loss0.0031\n",
            "Epoch 7 Loss 0.0016\n",
            "Time taken for 1 epoch 122.13868570327759 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss0.0010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-5dc85fd6dc0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mOCVNMNqC9Km",
        "outputId": "30c35742-6294-4a32-f6f7-fc58bc7373eb"
      },
      "source": [
        "manager.save()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./tf_checkpoint/ckpt-17'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyUwjTBBDYUz"
      },
      "source": [
        "### Saving in blob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Snhyf3pmiR"
      },
      "source": [
        "**Caution:** Only change this in case you wish to permanently change the model file. Do not change this otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxkjZlsTi_Ks",
        "outputId": "75b0e3e3-0538-4882-8bbe-4c162f75cec7"
      },
      "source": [
        "blob_list = [blob.name for blob in container_client.list_blobs()]\r\n",
        "blob_list"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['checkpoint', 'ckpt-17.data-00000-of-00001', 'ckpt-17.index']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7YFVlchDcDG"
      },
      "source": [
        "# clear the blob\r\n",
        "for file_name in blob_list:\r\n",
        "  container_client.delete_blob(blob=file_name)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukrq_eQNEVfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1c4431-7e14-41e5-c3a3-51ee4c7b8454"
      },
      "source": [
        "# getting the file names\r\n",
        "files = os.listdir('./tf_checkpoint')\r\n",
        "files"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['checkpoint', 'ckpt-17.data-00000-of-00001', 'ckpt-17.index']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FegNMHjxEjGf"
      },
      "source": [
        "# uploading the files\r\n",
        "for file in files:\r\n",
        "  blob_client = container_client.get_blob_client(file)\r\n",
        "  with open(\"./tf_checkpoint/\" + file,\"rb\") as data:\r\n",
        "    blob_client.upload_blob(data)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0irQlukDUrU"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp4TCi25u8Xq"
      },
      "source": [
        "from matplotlib import ticker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IBuyk-Dekcx"
      },
      "source": [
        "def evaluate(sentence):\r\n",
        "  attention_plot = np.zeros((eng_tensors.shape[1],mar_tensors.shape[1]))\r\n",
        "\r\n",
        "  sentence = sep_punk(sentence)\r\n",
        "\r\n",
        "  inputs = []\r\n",
        "  for i in sentence.lower().split(' '):\r\n",
        "    if i != '' :\r\n",
        "      if (i in mar_tokenizer.word_docs.keys()):\r\n",
        "        inputs.append(mar_tokenizer.word_index[i])\r\n",
        "      else: inputs.append(mar_tokenizer.word_index['<unk>'])\r\n",
        "\r\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n",
        "                                                         maxlen=mar_tensors.shape[1],\r\n",
        "                                                         padding='post')\r\n",
        "  inputs = tf.convert_to_tensor(inputs)\r\n",
        "\r\n",
        "  result = []\r\n",
        "\r\n",
        "  hidden = [tf.zeros((1, GRU_UNITS))]\r\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\r\n",
        "\r\n",
        "  dec_hidden = enc_hidden\r\n",
        "  dec_input = tf.expand_dims([eng_tokenizer.word_index['<sos>']], 0)\r\n",
        "\r\n",
        "  for t in range(eng_tensors.shape[1]):\r\n",
        "    predictions, dec_hidden, _ = decoder(dec_input,  dec_hidden, enc_out)\r\n",
        "\r\n",
        "    # storing the attention weights to plot later on\r\n",
        "    # attention_weights = tf.reshape(attention_weights, (-1, ))\r\n",
        "    # attention_plot[t] = attention_weights.numpy()\r\n",
        "\r\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\r\n",
        "\r\n",
        "    result.append(eng_tokenizer.index_word[predicted_id])\r\n",
        "\r\n",
        "    if eng_tokenizer.index_word[predicted_id] == '<eos>':\r\n",
        "      return result, sentence\r\n",
        "\r\n",
        "    # the predicted ID is fed back into the model\r\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "  return result, sentence"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbwQr01qsZI-",
        "outputId": "e013ffd0-3d89-4d36-c4c7-d2b1e35b4baf"
      },
      "source": [
        "start = time.time()\r\n",
        "res, sentence  = evaluate(data['marathi'][40749])\r\n",
        "end  = time.time()\r\n",
        "\r\n",
        "print(end-start)\r\n",
        "print(sentence)\r\n",
        "print(data['english'][40749])\r\n",
        "print(res)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.26405930519104004\n",
            "आयपॅडवर फ्लॅश आशय बघता येत नाही .  पण तुम्ही त्या वेब पानांचे यूआरएल स्वतःला ईमेल करून तोच आशय घरी पोहोचल्यावर आपल्या रोजच्या संगणकावर पाहू शकता . \n",
            "You can't view Flash content on an iPad. However, you can easily email yourself the URLs of these web pages and view that content on your regular computer when you get home.\n",
            "['you', 'can', \"'\", 't', 'view', 'flash', 'content', 'on', 'an', 'ipad', '.', 'however', ',', 'you', 'can', 'easily', 'email', 'yourself', 'the', 'urls', 'of', 'these', 'web', 'pages', 'and', 'view', 'that', 'content', 'on', 'your', 'regular', 'computer', 'when', 'you', 'get', 'home', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM5HtwZa5rpG",
        "outputId": "74d99f0e-d701-4038-fbc7-1319b08f9957"
      },
      "source": [
        "res, sentence = evaluate(\"random stuff here\")\r\n",
        "\r\n",
        "print(res)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['death', 'to', 'get', 'out', 'of', 'death', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoyusI6ZQuFx"
      },
      "source": [
        "# BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW451gOXxAzQ"
      },
      "source": [
        "import torchtext\r\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwg9YX5dHXge"
      },
      "source": [
        "from tqdm.notebook import trange"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS3GjYiXS2I6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a5669bd3563f4a249e8b0c61970e6373",
            "19b95e37990b4f6784e89872e5504093",
            "4a3a2cc4fa1e41c2b65e1487b59533c3",
            "65769da98a59446fb39ea88c25ed735a",
            "e97cc506abb84c0ea8acbad031cc47a1",
            "94a8d44dff4e48c7bcecf15b5984104f",
            "de92aaaedeb848719abc00718b0bb91f",
            "3c85fe942a224a2d926c49e6ebb8164c"
          ]
        },
        "outputId": "9eae6889-d51d-4ef9-ac27-24b67083e4bf"
      },
      "source": [
        "trgs = []\r\n",
        "preds = []\r\n",
        "\r\n",
        "for i in trange(len(data)):\r\n",
        "  \r\n",
        "  src = data['marathi'][i]\r\n",
        "  trg = data['english'][i]\r\n",
        "\r\n",
        "  trg = [tok.lower() for tok in sep_punk(trg).split(\" \") if tok!='']\r\n",
        "\r\n",
        "  pred, _ = evaluate(src)\r\n",
        "\r\n",
        "  #preds.append(trg)\r\n",
        "  preds.append(pred[:-1])\r\n",
        "  trgs.append([trg])\r\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5669bd3563f4a249e8b0c61970e6373",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=40751.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjIhGoMiIh7p",
        "outputId": "275971fd-94c8-432a-8c22-e4fabe827b0e"
      },
      "source": [
        "trgs"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['go', '.']],\n",
              " [['run', '!']],\n",
              " [['run', '!']],\n",
              " [['run', '!']],\n",
              " [['run', '!']],\n",
              " [['who', '?']],\n",
              " [['wow', '!']],\n",
              " [['fire', '!']],\n",
              " [['fire', '!']],\n",
              " [['help', '!']],\n",
              " [['help', '!']],\n",
              " [['jump', '!']],\n",
              " [['jump', '!']],\n",
              " [['jump', '.']],\n",
              " [['jump', '.']],\n",
              " [['stop', '!']],\n",
              " [['stop', '!']],\n",
              " [['wait', '!']],\n",
              " [['wait', '!']],\n",
              " [['hello', '!']],\n",
              " [['hurry', '!']],\n",
              " [['hurry', '!']],\n",
              " [['hurry', '!']],\n",
              " [['i', 'won', '!']],\n",
              " [['i', 'won', '!']],\n",
              " [['get', 'up', '.']],\n",
              " [['got', 'it', '!']],\n",
              " [['got', 'it', '?']],\n",
              " [['got', 'it', '?']],\n",
              " [['got', 'it', '?']],\n",
              " [['got', 'it', '?']],\n",
              " [['he', 'ran', '.']],\n",
              " [['he', 'ran', '.']],\n",
              " [['he', 'ran', '.']],\n",
              " [['he', 'ran', '.']],\n",
              " [['i', 'fell', '.']],\n",
              " [['i', 'fell', '.']],\n",
              " [['i', 'fell', '.']],\n",
              " [['i', 'fell', '.']],\n",
              " [['i', 'know', '.']],\n",
              " [['i', 'know', '.']],\n",
              " [['i', 'know', '.']],\n",
              " [['i', 'lost', '.']],\n",
              " [['i', 'lost', '.']],\n",
              " [['i', 'work', '.']],\n",
              " [['i', 'work', '.']],\n",
              " [['i', \"'\", 'm', 'ok', '.']],\n",
              " [['listen', '.']],\n",
              " [['listen', '.']],\n",
              " [['no', 'way', '!']],\n",
              " [['really', '?']],\n",
              " [['really', '?']],\n",
              " [['really', '?']],\n",
              " [['thanks', '.']],\n",
              " [['we', 'won', '.']],\n",
              " [['we', 'won', '.']],\n",
              " [['why', 'me', '?']],\n",
              " [['why', 'me', '?']],\n",
              " [['ask', 'tom', '.']],\n",
              " [['ask', 'tom', '.']],\n",
              " [['call', 'me', '.']],\n",
              " [['call', 'me', '.']],\n",
              " [['call', 'me', '.']],\n",
              " [['call', 'me', '.']],\n",
              " [['call', 'us', '.']],\n",
              " [['call', 'us', '.']],\n",
              " [['come', 'in', '.']],\n",
              " [['come', 'on', '!']],\n",
              " [['come', 'on', '!']],\n",
              " [['fold', 'it', '.']],\n",
              " [['fold', 'it', '.']],\n",
              " [['get', 'tom', '.']],\n",
              " [['get', 'tom', '.']],\n",
              " [['get', 'tom', '.']],\n",
              " [['get', 'out', '.']],\n",
              " [['get', 'out', '.']],\n",
              " [['go', 'home', '.']],\n",
              " [['he', 'came', '.']],\n",
              " [['he', 'came', '.']],\n",
              " [['he', 'left', '.']],\n",
              " [['he', 'left', '.']],\n",
              " [['he', 'runs', '.']],\n",
              " [['he', 'runs', '.']],\n",
              " [['he', 'runs', '.']],\n",
              " [['he', 'runs', '.']],\n",
              " [['help', 'me', '!']],\n",
              " [['help', 'me', '!']],\n",
              " [['help', 'me', '!']],\n",
              " [['help', 'me', '.']],\n",
              " [['help', 'me', '.']],\n",
              " [['help', 'us', '.']],\n",
              " [['help', 'us', '.']],\n",
              " [['help', 'us', '.']],\n",
              " [['i', \"'\", 'm', 'tom', '.']],\n",
              " [['i', \"'\", 'm', 'tom', '.']],\n",
              " [['i', \"'\", 'm', 'tom', '.']],\n",
              " [['i', \"'\", 'm', 'fat', '.']],\n",
              " [['i', \"'\", 'm', 'fat', '.']],\n",
              " [['i', \"'\", 'm', 'ill', '.']],\n",
              " [['it', \"'\", 's', 'ok', '.']],\n",
              " [['it', \"'\", 's', 'me', '!']],\n",
              " [['it', \"'\", 's', 'me', '.']],\n",
              " [['me', ',', 'too', '.']],\n",
              " [['me', ',', 'too', '.']],\n",
              " [['open', 'up', '.']],\n",
              " [['open', 'up', '.']],\n",
              " [['perfect', '!']],\n",
              " [['show', 'me', '.']],\n",
              " [['show', 'me', '.']],\n",
              " [['shut', 'up', '!']],\n",
              " [['shut', 'up', '!']],\n",
              " [['shut', 'up', '!']],\n",
              " [['tell', 'me', '.']],\n",
              " [['tell', 'me', '.']],\n",
              " [['tom', 'ran', '.']],\n",
              " [['tom', 'ran', '.']],\n",
              " [['tom', 'won', '.']],\n",
              " [['wake', 'up', '!']],\n",
              " [['wake', 'up', '!']],\n",
              " [['wake', 'up', '!']],\n",
              " [['we', 'care', '.']],\n",
              " [['we', 'care', '.']],\n",
              " [['we', 'know', '.']],\n",
              " [['we', 'know', '.']],\n",
              " [['we', 'lost', '.']],\n",
              " [['we', 'lost', '.']],\n",
              " [['welcome', '.']],\n",
              " [['welcome', '.']],\n",
              " [['welcome', '.']],\n",
              " [['welcome', '.']],\n",
              " [['who', 'ate', '?']],\n",
              " [['who', 'ran', '?']],\n",
              " [['who', 'ran', '?']],\n",
              " [['who', 'won', '?']],\n",
              " [['why', 'not', '?']],\n",
              " [['you', 'won', '.']],\n",
              " [['you', 'won', '.']],\n",
              " [['you', 'won', '.']],\n",
              " [['back', 'off', '!']],\n",
              " [['be', 'quiet', '.']],\n",
              " [['be', 'quiet', '.']],\n",
              " [['beats', 'me', '.']],\n",
              " [['beats', 'me', '.']],\n",
              " [['call', 'tom', '.']],\n",
              " [['call', 'tom', '.']],\n",
              " [['call', 'tom', '.']],\n",
              " [['call', 'tom', '.']],\n",
              " [['get', 'down', '.']],\n",
              " [['grab', 'tom', '.']],\n",
              " [['grab', 'tom', '.']],\n",
              " [['grab', 'him', '.']],\n",
              " [['have', 'fun', '.']],\n",
              " [['he', 'spoke', '.']],\n",
              " [['he', 'spoke', '.']],\n",
              " [['i', 'can', 'go', '.']],\n",
              " [['i', 'can', 'go', '.']],\n",
              " [['i', 'forgot', '.']],\n",
              " [['i', 'forgot', '.']],\n",
              " [['i', 'got', 'it', '.']],\n",
              " [['i', 'got', 'it', '.']],\n",
              " [['i', 'looked', '.']],\n",
              " [['i', 'looked', '.']],\n",
              " [['i', 'phoned', '.']],\n",
              " [['i', 'prayed', '.']],\n",
              " [['i', 'shaved', '.']],\n",
              " [['i', 'use', 'it', '.']],\n",
              " [['i', 'use', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'try', '.']],\n",
              " [['i', \"'\", 'm', 'full', '.']],\n",
              " [['i', \"'\", 'm', 'game', '.']],\n",
              " [['i', \"'\", 'm', 'late', '.']],\n",
              " [['i', \"'\", 'm', 'lazy', '.']],\n",
              " [['i', \"'\", 'm', 'poor', '.']],\n",
              " [['i', \"'\", 've', 'won', '.']],\n",
              " [['i', \"'\", 've', 'won', '.']],\n",
              " [['it', \"'\", 's', 'hot', '.']],\n",
              " [['it', \"'\", 's', 'new', '.']],\n",
              " [['it', \"'\", 's', 'new', '.']],\n",
              " [['it', \"'\", 's', 'old', '.']],\n",
              " [['it', \"'\", 's', 'old', '.']],\n",
              " [['it', \"'\", 's', 'old', '.']],\n",
              " [['kiss', 'tom', '.']],\n",
              " [['leave', 'it', '.']],\n",
              " [['leave', 'it', '.']],\n",
              " [['leave', 'it', '.']],\n",
              " [['leave', 'it', '.']],\n",
              " [['leave', 'me', '.']],\n",
              " [['leave', 'me', '.']],\n",
              " [['leave', 'me', '.']],\n",
              " [['look', 'out', '!']],\n",
              " [['marry', 'me', '.']],\n",
              " [['marry', 'me', '.']],\n",
              " [['she', 'came', '.']],\n",
              " [['she', 'came', '.']],\n",
              " [['she', 'died', '.']],\n",
              " [['she', 'died', '.']],\n",
              " [['she', 'died', '.']],\n",
              " [['she', 'died', '.']],\n",
              " [['sit', 'down', '!']],\n",
              " [['sit', 'down', '!']],\n",
              " [['sit', 'here', '.']],\n",
              " [['sit', 'here', '.']],\n",
              " [['stand', 'up', '!']],\n",
              " [['stand', 'up', '!']],\n",
              " [['stand', 'up', '!']],\n",
              " [['stop', 'tom', '.']],\n",
              " [['stop', 'tom', '.']],\n",
              " [['terrific', '!']],\n",
              " [['they', 'won', '.']],\n",
              " [['they', 'won', '.']],\n",
              " [['tom', 'came', '.']],\n",
              " [['tom', 'died', '.']],\n",
              " [['tom', 'fell', '.']],\n",
              " [['tom', 'knew', '.']],\n",
              " [['tom', 'left', '.']],\n",
              " [['tom', 'left', '.']],\n",
              " [['tom', 'left', '.']],\n",
              " [['tom', 'lied', '.']],\n",
              " [['tom', 'lies', '.']],\n",
              " [['tom', 'lost', '.']],\n",
              " [['tom', 'wept', '.']],\n",
              " [['tom', \"'\", 's', 'up', '.']],\n",
              " [['use', 'this', '.']],\n",
              " [['use', 'this', '.']],\n",
              " [['warn', 'tom', '.']],\n",
              " [['warn', 'tom', '.']],\n",
              " [['watch', 'me', '.']],\n",
              " [['watch', 'me', '.']],\n",
              " [['watch', 'us', '.']],\n",
              " [['watch', 'us', '.']],\n",
              " [['we', \"'\", 'll', 'go', '.']],\n",
              " [['we', \"'\", 'll', 'go', '.']],\n",
              " [['what', 'for', '?']],\n",
              " [['what', 'for', '?']],\n",
              " [['who', 'am', 'i', '?']],\n",
              " [['who', 'came', '?']],\n",
              " [['who', 'died', '?']],\n",
              " [['who', 'fell', '?']],\n",
              " [['who', \"'\", 's', 'he', '?']],\n",
              " [['who', \"'\", 's', 'he', '?']],\n",
              " [['answer', 'me', '.']],\n",
              " [['answer', 'me', '.']],\n",
              " [['birds', 'fly', '.']],\n",
              " [['calm', 'down', '!']],\n",
              " [['catch', 'him', '.']],\n",
              " [['catch', 'him', '.']],\n",
              " [['catch', 'him', '.']],\n",
              " [['catch', 'him', '.']],\n",
              " [['come', 'here', '.']],\n",
              " [['come', 'here', '.']],\n",
              " [['come', 'home', '.']],\n",
              " [['come', 'home', '.']],\n",
              " [['did', 'i', 'win', '?']],\n",
              " [['did', 'i', 'win', '?']],\n",
              " [['dogs', 'bark', '.']],\n",
              " [['don', \"'\", 't', 'ask', '.']],\n",
              " [['don', \"'\", 't', 'ask', '.']],\n",
              " [['don', \"'\", 't', 'cry', '.']],\n",
              " [['don', \"'\", 't', 'cry', '.']],\n",
              " [['don', \"'\", 't', 'die', '.']],\n",
              " [['don', \"'\", 't', 'die', '.']],\n",
              " [['don', \"'\", 't', 'lie', '.']],\n",
              " [['don', \"'\", 't', 'lie', '.']],\n",
              " [['don', \"'\", 't', 'run', '.']],\n",
              " [['don', \"'\", 't', 'run', '.']],\n",
              " [['don', \"'\", 't', 'run', '.']],\n",
              " [['don', \"'\", 't', 'run', '.']],\n",
              " [['forget', 'it', '.']],\n",
              " [['forget', 'it', '.']],\n",
              " [['forget', 'me', '.']],\n",
              " [['get', 'ready', '.']],\n",
              " [['go', 'inside', '.']],\n",
              " [['grab', 'that', '.']],\n",
              " [['grab', 'that', '.']],\n",
              " [['grab', 'that', '.']],\n",
              " [['grab', 'that', '.']],\n",
              " [['grab', 'this', '.']],\n",
              " [['grab', 'this', '.']],\n",
              " [['he', 'is', 'ill', '.']],\n",
              " [['he', 'is', 'ill', '.']],\n",
              " [['he', \"'\", 's', 'a', 'dj', '.']],\n",
              " [['he', \"'\", 's', 'a', 'dj', '.']],\n",
              " [['he', \"'\", 's', 'mine', '.']],\n",
              " [['he', \"'\", 's', 'mine', '.']],\n",
              " [['he', \"'\", 's', 'sexy', '.']],\n",
              " [['he', \"'\", 's', 'sexy', '.']],\n",
              " [['hold', 'this', '.']],\n",
              " [['hold', 'this', '.']],\n",
              " [['how', 'is', 'it', '?']],\n",
              " [['how', 'is', 'it', '?']],\n",
              " [['how', 'is', 'it', '?']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'ski', '.']],\n",
              " [['i', 'can', 'ski', '.']],\n",
              " [['i', 'can', 'ski', '.']],\n",
              " [['i', 'can', 'win', '.']],\n",
              " [['i', 'can', 'win', '.']],\n",
              " [['i', 'fainted', '.']],\n",
              " [['i', 'got', 'fat', '.']],\n",
              " [['i', 'got', 'fat', '.']],\n",
              " [['i', 'hit', 'tom', '.']],\n",
              " [['i', 'hit', 'tom', '.']],\n",
              " [['i', 'hit', 'tom', '.']],\n",
              " [['i', 'laughed', '.']],\n",
              " [['i', 'laughed', '.']],\n",
              " [['i', 'met', 'him', '.']],\n",
              " [['i', 'met', 'him', '.']],\n",
              " [['i', 'saw', 'tom', '.']],\n",
              " [['i', 'saw', 'tom', '.']],\n",
              " [['i', 'saw', 'tom', '.']],\n",
              " [['i', 'saw', 'you', '.']],\n",
              " [['i', 'saw', 'you', '.']],\n",
              " [['i', 'saw', 'you', '.']],\n",
              " [['i', 'saw', 'you', '.']],\n",
              " [['i', 'shouted', '.']],\n",
              " [['i', 'shouted', '.']],\n",
              " [['i', 'want', 'it', '.']],\n",
              " [['i', 'want', 'it', '.']],\n",
              " [['i', 'want', 'it', '.']],\n",
              " [['i', 'want', 'it', '.']],\n",
              " [['i', 'will', 'go', '.']],\n",
              " [['i', \"'\", 'll', 'call', '.']],\n",
              " [['i', \"'\", 'll', 'sing', '.']],\n",
              " [['i', \"'\", 'll', 'stop', '.']],\n",
              " [['i', \"'\", 'll', 'talk', '.']],\n",
              " [['i', \"'\", 'll', 'walk', '.']],\n",
              " [['i', \"'\", 'll', 'work', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'man', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'man', '.']],\n",
              " [['i', \"'\", 'm', 'awake', '.']],\n",
              " [['i', \"'\", 'm', 'awake', '.']],\n",
              " [['i', \"'\", 'm', 'bored', '.']],\n",
              " [['i', \"'\", 'm', 'clean', '.']],\n",
              " [['i', \"'\", 'm', 'dying', '.']],\n",
              " [['i', \"'\", 'm', 'dying', '.']],\n",
              " [['i', \"'\", 'm', 'going', '.']],\n",
              " [['i', \"'\", 'm', 'going', '.']],\n",
              " [['i', \"'\", 'm', 'quiet', '.']],\n",
              " [['i', \"'\", 'm', 'right', '.']],\n",
              " [['i', \"'\", 'm', 'young', '.']],\n",
              " [['it', 'burned', '.']],\n",
              " [['it', 'burned', '.']],\n",
              " [['it', 'burned', '.']],\n",
              " [['it', 'rained', '.']],\n",
              " [['it', 'snowed', '.']],\n",
              " [['it', \"'\", 's', '7', ':', '45', '.']],\n",
              " [['it', \"'\", 's', 'cold', '.']],\n",
              " [['it', \"'\", 's', 'easy', '.']],\n",
              " [['it', \"'\", 's', 'food', '.']],\n",
              " [['it', \"'\", 's', 'good', '.']],\n",
              " [['it', \"'\", 's', 'here', '.']],\n",
              " [['it', \"'\", 's', 'here', '.']],\n",
              " [['it', \"'\", 's', 'mine', '.']],\n",
              " [['it', \"'\", 's', 'ours', '.']],\n",
              " [['it', \"'\", 's', 'ours', '.']],\n",
              " [['it', \"'\", 's', 'sand', '.']],\n",
              " [['it', \"'\", 's', 'time', '.']],\n",
              " [['it', \"'\", 's', 'work', '.']],\n",
              " [['keep', 'them', '.']],\n",
              " [['keep', 'them', '.']],\n",
              " [['leave', 'now', '.']],\n",
              " [['leave', 'now', '.']],\n",
              " [['let', 'me', 'go', '!']],\n",
              " [['let', 'me', 'go', '!']],\n",
              " [['let', 'me', 'go', '.']],\n",
              " [['let', 'me', 'go', '.']],\n",
              " [['let', 'me', 'in', '.']],\n",
              " [['let', 'me', 'in', '.']],\n",
              " [['let', 'us', 'go', '.']],\n",
              " [['let', 'us', 'go', '.']],\n",
              " [['let', 'us', 'go', '.']],\n",
              " [['let', 'us', 'go', '.']],\n",
              " [['let', 'us', 'in', '.']],\n",
              " [['let', 'us', 'in', '.']],\n",
              " [['let', \"'\", 's', 'ask', '.']],\n",
              " [['look', 'back', '!']],\n",
              " [['look', 'back', '!']],\n",
              " [['look', 'back', '.']],\n",
              " [['look', 'back', '.']],\n",
              " [['read', 'this', '.']],\n",
              " [['read', 'this', '.']],\n",
              " [['search', 'me', '.']],\n",
              " [['search', 'me', '.']],\n",
              " [['see', 'above', '.']],\n",
              " [['see', 'below', '.']],\n",
              " [['she', 'cried', '.']],\n",
              " [['she', 'cried', '.']],\n",
              " [['she', 'tried', '.']],\n",
              " [['she', 'walks', '.']],\n",
              " [['she', 'walks', '.']],\n",
              " [['sign', 'here', '.']],\n",
              " [['sign', 'here', '.']],\n",
              " [['sit', 'there', '.']],\n",
              " [['sit', 'there', '.']],\n",
              " [['start', 'now', '.']],\n",
              " [['start', 'now', '.']],\n",
              " [['stay', 'away', '.']],\n",
              " [['stay', 'back', '.']],\n",
              " [['stay', 'home', '.']],\n",
              " [['stay', 'thin', '.']],\n",
              " [['stop', 'them', '.']],\n",
              " [['stop', 'them', '.']],\n",
              " [['take', 'care', '!']],\n",
              " [['take', 'care', '!']],\n",
              " [['take', 'care', '.']],\n",
              " [['take', 'care', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['thank', 'you', '.']],\n",
              " [['that', \"'\", 's', 'it', '.']],\n",
              " [['that', \"'\", 's', 'it', '.']],\n",
              " [['they', 'fell', '.']],\n",
              " [['they', 'fell', '.']],\n",
              " [['they', 'left', '.']],\n",
              " [['they', 'left', '.']],\n",
              " [['they', 'lied', '.']],\n",
              " [['they', 'lied', '.']],\n",
              " [['they', 'lost', '.']],\n",
              " [['they', 'lost', '.']],\n",
              " [['tom', 'cried', '.']],\n",
              " [['tom', 'drank', '.']],\n",
              " [['tom', 'knits', '.']],\n",
              " [['tom', 'knows', '.']],\n",
              " [['tom', 'moved', '.']],\n",
              " [['tom', 'tried', '.']],\n",
              " [['tom', 'tried', '.']],\n",
              " [['tom', 'tries', '.']],\n",
              " [['tom', 'voted', '.']],\n",
              " [['tom', 'voted', '.']],\n",
              " [['tom', 'walks', '.']],\n",
              " [['tom', 'works', '.']],\n",
              " [['try', 'again', '.']],\n",
              " [['try', 'again', '.']],\n",
              " [['try', 'it', 'on', '.']],\n",
              " [['try', 'it', 'on', '.']],\n",
              " [['wait', 'here', '.']],\n",
              " [['wait', 'here', '.']],\n",
              " [['watch', 'tom', '.']],\n",
              " [['watch', 'tom', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'talked', '.']],\n",
              " [['we', 'waited', '.']],\n",
              " [['we', 'waited', '.']],\n",
              " [['we', 'waited', '.']],\n",
              " [['we', 'waited', '.']],\n",
              " [['we', \"'\", 're', 'shy', '.']],\n",
              " [['we', \"'\", 're', 'shy', '.']],\n",
              " [['well', 'done', '!']],\n",
              " [['what', \"'\", 's', 'up', '?']],\n",
              " [['what', \"'\", 's', 'up', '?']],\n",
              " [['what', \"'\", 's', 'up', '?']],\n",
              " [['who', 'is', 'he', '?']],\n",
              " [['who', 'is', 'he', '?']],\n",
              " [['who', 'is', 'it', '?']],\n",
              " [['who', 'knows', '?']],\n",
              " [['who', 'knows', '?']],\n",
              " [['who', \"'\", 'll', 'go', '?']],\n",
              " [['who', \"'\", 's', 'tom', '?']],\n",
              " [['who', \"'\", 's', 'she', '?']],\n",
              " [['who', \"'\", 's', 'she', '?']],\n",
              " [['who', \"'\", 's', 'she', '?']],\n",
              " [['who', \"'\", 's', 'she', '?']],\n",
              " [['you', 'drive', '.']],\n",
              " [['you', 'drive', '.']],\n",
              " [['you', 'idiot', '!']],\n",
              " [['you', 'tried', '.']],\n",
              " [['you', 'tried', '.']],\n",
              " [['are', 'you', 'ok', '?']],\n",
              " [['are', 'you', 'ok', '?']],\n",
              " [['are', 'you', 'ok', '?']],\n",
              " [['ask', 'anyone', '.']],\n",
              " [['be', 'careful', '!']],\n",
              " [['be', 'on', 'time', '.']],\n",
              " [['be', 'on', 'time', '.']],\n",
              " [['birds', 'sing', '.']],\n",
              " [['bring', 'wine', '.']],\n",
              " [['bring', 'wine', '.']],\n",
              " [['come', 'again', '.']],\n",
              " [['come', 'again', '.']],\n",
              " [['come', 'on', 'in', '.']],\n",
              " [['come', 'quick', '!']],\n",
              " [['come', 'quick', '!']],\n",
              " [['definitely', '!']],\n",
              " [['do', 'men', 'cry', '?']],\n",
              " [['do', 'men', 'cry', '?']],\n",
              " [['don', \"'\", 't', 'look', '.']],\n",
              " [['don', \"'\", 't', 'look', '.']],\n",
              " [['don', \"'\", 't', 'move', '.']],\n",
              " [['don', \"'\", 't', 'move', '.']],\n",
              " [['don', \"'\", 't', 'talk', '!']],\n",
              " [['don', \"'\", 't', 'talk', '!']],\n",
              " [['don', \"'\", 't', 'talk', '.']],\n",
              " [['don', \"'\", 't', 'talk', '.']],\n",
              " [['fill', 'it', 'up', '.']],\n",
              " [['fire', 'burns', '.']],\n",
              " [['fire', 'burns', '.']],\n",
              " [['forget', 'tom', '.']],\n",
              " [['forget', 'tom', '.']],\n",
              " [['forget', 'tom', '.']],\n",
              " [['forget', 'him', '.']],\n",
              " [['forget', 'him', '.']],\n",
              " [['forget', 'him', '.']],\n",
              " [['forget', 'him', '.']],\n",
              " [['god', 'exists', '.']],\n",
              " [['he', 'is', 'here', '!']],\n",
              " [['he', 'is', 'here', '!']],\n",
              " [['he', 'is', 'here', '!']],\n",
              " [['he', 'is', 'late', '.']],\n",
              " [['he', 'is', 'nice', '.']],\n",
              " [['he', 'is', 'nice', '.']],\n",
              " [['he', 'laughed', '.']],\n",
              " [['he', 'laughed', '.']],\n",
              " [['he', \"'\", 's', 'swiss', '.']],\n",
              " [['he', \"'\", 's', 'smart', '.']],\n",
              " [['he', \"'\", 's', 'smart', '.']],\n",
              " [['hold', 'still', '.']],\n",
              " [['hold', 'still', '.']],\n",
              " [['i', 'am', 'a', 'man', '.']],\n",
              " [['i', 'am', 'a', 'man', '.']],\n",
              " [['i', 'am', 'ready', '.']],\n",
              " [['i', 'can', 'read', '.']],\n",
              " [['i', 'can', 'read', '.']],\n",
              " [['i', 'can', 'read', '.']],\n",
              " [['i', 'can', 'swim', '.']],\n",
              " [['i', 'can', 'swim', '.']],\n",
              " [['i', 'can', 'swim', '.']],\n",
              " [['i', 'can', 'walk', '.']],\n",
              " [['i', 'can', 'walk', '.']],\n",
              " [['i', 'eat', 'here', '.']],\n",
              " [['i', 'eat', 'here', '.']],\n",
              " [['i', 'eat', 'meat', '.']],\n",
              " [['i', 'eat', 'meat', '.']],\n",
              " [['i', 'eat', 'rice', '.']],\n",
              " [['i', 'eat', 'rice', '.']],\n",
              " [['i', 'help', 'tom', '.']],\n",
              " [['i', 'help', 'tom', '.']],\n",
              " [['i', 'just', 'ate', '.']],\n",
              " [['i', 'like', 'him', '.']],\n",
              " [['i', 'like', 'him', '.']],\n",
              " [['i', 'like', 'tea', '.']],\n",
              " [['i', 'like', 'you', '.']],\n",
              " [['i', 'like', 'you', '.']],\n",
              " [['i', 'like', 'you', '.']],\n",
              " [['i', 'love', 'you', '.']],\n",
              " [['i', 'love', 'you', '.']],\n",
              " [['i', 'made', 'tea', '.']],\n",
              " [['i', 'miss', 'you', '.']],\n",
              " [['i', 'miss', 'you', '.']],\n",
              " [['i', 'ran', 'away', '.']],\n",
              " [['i', 'ran', 'away', '.']],\n",
              " [['i', 'remember', '.']],\n",
              " [['i', 'remember', '.']],\n",
              " [['i', 'remember', '.']],\n",
              " [['i', 'screamed', '.']],\n",
              " [['i', 'screamed', '.']],\n",
              " [['i', 'see', 'that', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'shot', 'tom', '.']],\n",
              " [['i', 'want', 'tom', '.']],\n",
              " [['i', 'want', 'tom', '.']],\n",
              " [['i', 'want', 'you', '.']],\n",
              " [['i', 'want', 'you', '.']],\n",
              " [['i', 'want', 'you', '.']],\n",
              " [['i', 'was', 'good', '.']],\n",
              " [['i', 'was', 'good', '.']],\n",
              " [['i', 'was', 'late', '.']],\n",
              " [['i', 'was', 'sick', '.']],\n",
              " [['i', 'was', 'sick', '.']],\n",
              " [['i', 'work', 'out', '.']],\n",
              " [['i', 'work', 'out', '.']],\n",
              " [['i', \"'\", 'll', 'leave', '.']],\n",
              " [['i', \"'\", 'll', 'start', '.']],\n",
              " [['i', \"'\", 'll', 'start', '.']],\n",
              " [['i', \"'\", 'll', 'start', '.']],\n",
              " [['i', \"'\", 'm', '30', 'now', '.']],\n",
              " [['i', \"'\", 'm', '30', 'now', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'liar', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'liar', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'poet', '.']],\n",
              " [['i', \"'\", 'm', 'coming', '.']],\n",
              " [['i', \"'\", 'm', 'coming', '.']],\n",
              " [['i', \"'\", 'm', 'hungry', '!']],\n",
              " [['i', \"'\", 'm', 'hungry', '.']],\n",
              " [['i', \"'\", 'm', 'scared', '.']],\n",
              " [['i', \"'\", 'm', 'scared', '.']],\n",
              " [['i', \"'\", 'm', 'sleepy', '!']],\n",
              " [['i', \"'\", 'm', 'so', 'fat', '.']],\n",
              " [['i', \"'\", 'm', 'so', 'fat', '.']],\n",
              " [['i', \"'\", 'm', 'trying', '.']],\n",
              " [['i', \"'\", 'm', 'trying', '.']],\n",
              " [['is', 'tom', 'big', '?']],\n",
              " [['is', 'he', 'tall', '?']],\n",
              " [['is', 'he', 'tall', '?']],\n",
              " [['is', 'it', 'done', '?']],\n",
              " [['is', 'it', 'free', '?']],\n",
              " [['is', 'it', 'free', '?']],\n",
              " [['is', 'it', 'hard', '?']],\n",
              " [['is', 'it', 'here', '?']],\n",
              " [['is', 'it', 'nice', '?']],\n",
              " [['is', 'it', 'time', '?']],\n",
              " [['is', 'it', 'true', '?']],\n",
              " [['is', 'that', 'so', '?']],\n",
              " [['is', 'this', 'it', '?']],\n",
              " [['is', 'this', 'it', '?']],\n",
              " [['is', 'this', 'it', '?']],\n",
              " [['is', 'this', 'it', '?']],\n",
              " [['it', \"'\", 's', 'alive', '.']],\n",
              " [['it', \"'\", 's', 'my', 'cd', '.']],\n",
              " [['it', \"'\", 's', 'my', 'cd', '.']],\n",
              " [['it', \"'\", 's', 'night', '.']],\n",
              " [['it', \"'\", 's', 'ready', '.']],\n",
              " [['it', \"'\", 's', 'white', '.']],\n",
              " [['it', \"'\", 's', 'white', '.']],\n",
              " [['it', \"'\", 's', 'white', '.']],\n",
              " [['it', \"'\", 's', 'white', '.']],\n",
              " [['it', \"'\", 's', 'yours', '.']],\n",
              " [['it', \"'\", 's', 'yours', '.']],\n",
              " [['it', \"'\", 's', 'yours', '.']],\n",
              " [['it', \"'\", 's', 'yours', '.']],\n",
              " [['jesus', 'wept', '.']],\n",
              " [['keep', 'quiet', '!']],\n",
              " [['keep', 'quiet', '!']],\n",
              " [['keep', 'quiet', '!']],\n",
              " [['keep', 'quiet', '.']],\n",
              " [['keep', 'quiet', '.']],\n",
              " [['keep', 'quiet', '.']],\n",
              " [['let', 'tom', 'go', '.']],\n",
              " [['let', 'tom', 'go', '.']],\n",
              " [['let', 'tom', 'go', '.']],\n",
              " [['let', 'tom', 'go', '.']],\n",
              " [['let', 'tom', 'in', '.']],\n",
              " [['let', 'tom', 'in', '.']],\n",
              " [['let', 'him', 'go', '!']],\n",
              " [['let', 'him', 'go', '!']],\n",
              " [['let', 'me', 'see', '.']],\n",
              " [['let', 'me', 'see', '.']],\n",
              " [['let', \"'\", 's', 'chat', '.']],\n",
              " [['let', \"'\", 's', 'kiss', '.']],\n",
              " [['let', \"'\", 's', 'talk', '.']],\n",
              " [['look', 'again', '.']],\n",
              " [['look', 'again', '.']],\n",
              " [['look', 'ahead', '.']],\n",
              " [['look', 'ahead', '.']],\n",
              " [['look', 'ahead', '.']],\n",
              " [['look', 'at', 'me', '.']],\n",
              " [['look', 'at', 'me', '.']],\n",
              " [['look', 'there', '.']],\n",
              " [['look', 'there', '.']],\n",
              " [['love', 'lasts', '.']],\n",
              " [['never', 'mind', '!']],\n",
              " [['never', 'mind', '!']],\n",
              " [['never', 'mind', '!']],\n",
              " [['never', 'mind', '!']],\n",
              " [['no', 'one', 'ran', '.']],\n",
              " [['quiet', 'down', '.']],\n",
              " [['quiet', 'down', '.']],\n",
              " [['shadow', 'him', '.']],\n",
              " [['shadow', 'him', '.']],\n",
              " [['shadow', 'him', '.']],\n",
              " [['shadow', 'him', '.']],\n",
              " [['she', 'is', 'old', '.']],\n",
              " [['she', 'is', 'old', '.']],\n",
              " [['smell', 'this', '.']],\n",
              " [['smell', 'this', '.']],\n",
              " [['stand', 'back', '!']],\n",
              " [['stand', 'back', '!']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['stay', 'alert', '.']],\n",
              " [['stay', 'alert', '.']],\n",
              " [['stay', 'still', '.']],\n",
              " [['step', 'aside', '.']],\n",
              " [['step', 'aside', '.']],\n",
              " [['study', 'hard', '.']],\n",
              " [['study', 'hard', '.']],\n",
              " [['take', 'a', 'bus', '.']],\n",
              " [['take', 'a', 'bus', '.']],\n",
              " [['take', 'a', 'bus', '.']],\n",
              " [['take', 'these', '.']],\n",
              " [['take', 'these', '.']],\n",
              " [['take', 'these', '.']],\n",
              " [['take', 'these', '.']],\n",
              " [['talk', 'to', 'me', '.']],\n",
              " [['talk', 'to', 'us', '.']],\n",
              " [['that', 'a', 'boy', '!']],\n",
              " [['that', \"'\", 's', 'tom', '.']],\n",
              " [['that', \"'\", 's', 'tom', '.']],\n",
              " [['that', \"'\", 's', 'wet', '.']],\n",
              " [['they', 'stood', '.']],\n",
              " [['they', 'stood', '.']],\n",
              " [['they', 'tried', '.']],\n",
              " [['this', 'is', 'me', '.']],\n",
              " [['this', 'is', 'me', '.']],\n",
              " [['time', 'is', 'up', '.']],\n",
              " [['time', 'is', 'up', '.']],\n",
              " [['tom', 'bit', 'me', '.']],\n",
              " [['tom', 'burped', '.']],\n",
              " [['tom', 'called', '.']],\n",
              " [['tom', 'called', '.']],\n",
              " [['tom', 'danced', '.']],\n",
              " [['tom', 'drinks', '.']],\n",
              " [['tom', 'failed', '.']],\n",
              " [['tom', 'failed', '.']],\n",
              " [['tom', 'forgot', '.']],\n",
              " [['tom', 'fought', '.']],\n",
              " [['tom', 'helped', '.']],\n",
              " [['tom', 'is', 'fat', '.']],\n",
              " [['tom', 'is', 'ill', '.']],\n",
              " [['tom', 'is', 'out', '.']],\n",
              " [['tom', 'is', 'out', '.']],\n",
              " [['tom', 'jumped', '.']],\n",
              " [['tom', 'looked', '.']],\n",
              " [['tom', 'moaned', '.']],\n",
              " [['tom', 'phoned', '.']],\n",
              " [['tom', 'saw', 'me', '.']],\n",
              " [['tom', 'saw', 'me', '.']],\n",
              " [['tom', 'shaved', '.']],\n",
              " [['tom', 'snores', '.']],\n",
              " [['tom', 'talked', '.']],\n",
              " [['tom', 'waited', '.']],\n",
              " [['tom', 'waited', '.']],\n",
              " [['tom', 'yawned', '.']],\n",
              " [['tom', 'yelled', '.']],\n",
              " [['tom', \"'\", 'll', 'die', '.']],\n",
              " [['tom', \"'\", 's', 'deaf', '.']],\n",
              " [['tom', \"'\", 's', 'sick', '.']],\n",
              " [['tom', \"'\", 's', 'ugly', '.']],\n",
              " [['turn', 'right', '.']],\n",
              " [['turn', 'right', '.']],\n",
              " [['watch', 'this', '.']],\n",
              " [['watch', 'this', '.']],\n",
              " [['we', 'are', 'men', '.']],\n",
              " [['we', 'are', 'men', '.']],\n",
              " [['we', 'had', 'fun', '.']],\n",
              " [['we', 'had', 'fun', '.']],\n",
              " [['we', 'laughed', '.']],\n",
              " [['we', 'laughed', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'want', 'it', '.']],\n",
              " [['we', 'want', 'it', '.']],\n",
              " [['we', 'want', 'it', '.']],\n",
              " [['we', \"'\", 'll', 'help', '.']],\n",
              " [['we', \"'\", 'll', 'help', '.']],\n",
              " [['we', \"'\", 'll', 'sing', '.']],\n",
              " [['we', \"'\", 'll', 'sing', '.']],\n",
              " [['we', \"'\", 'll', 'wait', '.']],\n",
              " [['we', \"'\", 'll', 'wait', '.']],\n",
              " [['we', \"'\", 'll', 'work', '.']],\n",
              " [['we', \"'\", 're', 'back', '.']],\n",
              " [['we', \"'\", 're', 'back', '.']],\n",
              " [['we', \"'\", 're', 'boys', '.']],\n",
              " [['we', \"'\", 're', 'boys', '.']],\n",
              " [['we', \"'\", 're', 'fine', '.']],\n",
              " [['we', \"'\", 're', 'fine', '.']],\n",
              " [['we', \"'\", 're', 'fine', '.']],\n",
              " [['we', \"'\", 're', 'fine', '.']],\n",
              " [['where', 'am', 'i', '?']],\n",
              " [['who', 'are', 'we', '?']],\n",
              " [['who', 'are', 'we', '?']],\n",
              " [['who', 'did', 'it', '?']],\n",
              " [['who', 'has', 'it', '?']],\n",
              " [['who', 'has', 'it', '?']],\n",
              " [['who', 'is', 'tom', '?']],\n",
              " [['who', 'is', 'she', '?']],\n",
              " [['who', 'is', 'she', '?']],\n",
              " [['who', 'phoned', '?']],\n",
              " [['who', 'talked', '?']],\n",
              " [['who', 'was', 'it', '?']],\n",
              " [['who', 'yelled', '?']],\n",
              " [['who', \"'\", 's', 'here', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['wood', 'burns', '.']],\n",
              " [['you', 'decide', '.']],\n",
              " [['you', 'decide', '.']],\n",
              " [['you', 'decide', '.']],\n",
              " [['you', 'decide', '.']],\n",
              " [['you', 'may', 'go', '.']],\n",
              " [['you', 'may', 'go', '.']],\n",
              " [['you', 'may', 'go', '.']],\n",
              " [['you', 'may', 'go', '.']],\n",
              " [['you', \"'\", 've', 'won', '.']],\n",
              " [['you', \"'\", 've', 'won', '.']],\n",
              " [['you', \"'\", 've', 'won', '.']],\n",
              " [['anyone', 'home', '?']],\n",
              " [['anyone', 'hurt', '?']],\n",
              " [['are', 'we', 'done', '?']],\n",
              " [['are', 'you', 'tom', '?']],\n",
              " [['are', 'you', 'tom', '?']],\n",
              " [['are', 'you', 'tom', '?']],\n",
              " [['are', 'you', 'tom', '?']],\n",
              " [['are', 'you', 'new', '?']],\n",
              " [['are', 'you', 'new', '?']],\n",
              " [['are', 'you', 'new', '?']],\n",
              " [['be', 'prepared', '.']],\n",
              " [['breathe', 'out', '.']],\n",
              " [['breathe', 'out', '.']],\n",
              " [['come', 'inside', '.']],\n",
              " [['come', 'inside', '.']],\n",
              " [['did', 'tom', 'die', '?']],\n",
              " [['did', 'tom', 'die', '?']],\n",
              " [['did', 'tom', 'eat', '?']],\n",
              " [['did', 'tom', 'win', '?']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'fight', '.']],\n",
              " [['don', \"'\", 't', 'fight', '.']],\n",
              " [['don', \"'\", 't', 'fight', '.']],\n",
              " [['don', \"'\", 't', 'leave', '.']],\n",
              " [['don', \"'\", 't', 'leave', '.']],\n",
              " [['don', \"'\", 't', 'leave', '.']],\n",
              " [['don', \"'\", 't', 'leave', '.']],\n",
              " [['don', \"'\", 't', 'shout', '.']],\n",
              " [['don', \"'\", 't', 'shout', '.']],\n",
              " [['eat', 'with', 'us', '.']],\n",
              " [['forgive', 'tom', '.']],\n",
              " [['forgive', 'tom', '.']],\n",
              " [['get', 'changed', '.']],\n",
              " [['get', 'changed', '.']],\n",
              " [['get', 'started', '.']],\n",
              " [['get', 'started', '.']],\n",
              " [['get', 'started', '.']],\n",
              " [['get', 'started', '.']],\n",
              " [['go', 'have', 'fun', '.']],\n",
              " [['go', 'have', 'fun', '.']],\n",
              " [['go', 'help', 'tom', '.']],\n",
              " [['have', 'a', 'seat', '.']],\n",
              " [['have', 'a', 'seat', '.']],\n",
              " [['he', 'can', 'come', '.']],\n",
              " [['he', 'can', 'come', '.']],\n",
              " [['he', 'can', 'read', '.']],\n",
              " [['he', 'can', 'read', '.']],\n",
              " [['he', 'found', 'it', '.']],\n",
              " [['he', 'found', 'it', '.']],\n",
              " [['he', 'has', 'wine', '.']],\n",
              " [['he', 'is', 'happy', '.']],\n",
              " [['he', 'is', 'happy', '.']],\n",
              " [['he', 'is', 'lying', '.']],\n",
              " [['he', 'is', 'lying', '.']],\n",
              " [['he', 'is', 'young', '.']],\n",
              " [['he', 'resigned', '.']],\n",
              " [['he', 'resigned', '.']],\n",
              " [['he', 'stood', 'up', '.']],\n",
              " [['he', 'stood', 'up', '.']],\n",
              " [['he', 'stood', 'up', '.']],\n",
              " [['he', 'stood', 'up', '.']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'strange', '!']],\n",
              " [['i', 'am', 'taller', '.']],\n",
              " [['i', 'came', 'back', '.']],\n",
              " [['i', 'came', 'back', '.']],\n",
              " [['i', 'can', 'dance', '.']],\n",
              " [['i', 'can', 'dance', '.']],\n",
              " [['i', 'can', 'dance', '.']],\n",
              " [['i', 'can', 'do', 'it', '.']],\n",
              " [['i', 'can', 'do', 'it', '.']],\n",
              " [['i', 'can', \"'\", 't', 'fly', '.']],\n",
              " [['i', 'can', \"'\", 't', 'say', '.']],\n",
              " [['i', 'can', \"'\", 't', 'say', '.']],\n",
              " [['i', 'can', \"'\", 't', 'say', '.']],\n",
              " [['i', 'don', \"'\", 't', 'cry', '.']],\n",
              " [['i', 'don', \"'\", 't', 'eat', '.']],\n",
              " [['i', 'drank', 'tea', '.']],\n",
              " [['i', 'drank', 'tea', '.']],\n",
              " [['i', 'eat', 'bread', '.']],\n",
              " [['i', 'eat', 'bread', '.']],\n",
              " [['i', 'eat', 'bread', '.']],\n",
              " [['i', 'eat', 'bread', '.']],\n",
              " [['i', 'eat', 'fruit', '.']],\n",
              " [['i', 'eat', 'fruit', '.']],\n",
              " [['i', 'exercised', '.']],\n",
              " [['i', 'feel', 'fine', '.']],\n",
              " [['i', 'felt', 'that', '.']],\n",
              " [['i', 'forgot', 'it', '.']],\n",
              " [['i', 'forgot', 'it', '.']],\n",
              " [['i', 'had', 'a', 'cat', '.']],\n",
              " [['i', 'have', 'time', '.']],\n",
              " [['i', 'have', 'wine', '.']],\n",
              " [['i', 'knew', 'that', '.']],\n",
              " [['i', 'know', 'this', '.']],\n",
              " [['i', 'left', 'home', '.']],\n",
              " [['i', 'left', 'home', '.']],\n",
              " [['i', 'left', 'home', '.']],\n",
              " [['i', 'left', 'home', '.']],\n",
              " [['i', 'like', 'beer', '.']],\n",
              " [['i', 'like', 'both', '.']],\n",
              " [['i', 'like', 'cake', '.']],\n",
              " [['i', 'like', 'cats', '.']],\n",
              " [['i', 'like', 'fish', '.']],\n",
              " [['i', 'like', 'math', '.']],\n",
              " [['i', 'like', 'rice', '.']],\n",
              " [['i', 'like', 'that', '.']],\n",
              " [['i', 'like', 'that', '.']],\n",
              " [['i', 'like', 'that', '.']],\n",
              " [['i', 'made', 'that', '.']],\n",
              " [['i', 'made', 'that', '.']],\n",
              " [['i', 'need', 'time', '.']],\n",
              " [['i', 'never', 'cry', '.']],\n",
              " [['i', 'saved', 'you', '.']],\n",
              " [['i', 'saved', 'you', '.']],\n",
              " [['i', 'smell', 'gas', '.']],\n",
              " [['i', 'trust', 'tom', '.']],\n",
              " [['i', 'want', 'more', '.']],\n",
              " [['i', 'want', 'more', '.']],\n",
              " [['i', 'want', 'them', '.']],\n",
              " [['i', 'want', 'them', '.']],\n",
              " [['i', 'want', 'them', '.']],\n",
              " [['i', 'want', 'this', '.']],\n",
              " [['i', 'want', 'this', '.']],\n",
              " [['i', 'want', 'time', '.']],\n",
              " [['i', 'was', 'alone', '.']],\n",
              " [['i', 'was', 'alone', '.']],\n",
              " [['i', 'was', 'naked', '.']],\n",
              " [['i', 'was', 'naked', '.']],\n",
              " [['i', 'was', 'naked', '.']],\n",
              " [['i', 'was', 'naked', '.']],\n",
              " [['i', 'was', 'stuck', '.']],\n",
              " [['i', 'was', 'stuck', '.']],\n",
              " [['i', 'was', 'tired', '.']],\n",
              " [['i', 'was', 'tired', '.']],\n",
              " [['i', 'went', ',', 'too', '.']],\n",
              " [['i', 'went', ',', 'too', '.']],\n",
              " [['i', 'will', 'work', '.']],\n",
              " [['i', 'won', \"'\", 't', 'lie', '.']],\n",
              " [['i', \"'\", 'll', 'buy', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'decide', '.']],\n",
              " [['i', \"'\", 'll', 'decide', '.']],\n",
              " [['i', \"'\", 'll', 'eat', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'by', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'scream', '.']],\n",
              " [['i', \"'\", 'm', 'dancing', '.']],\n",
              " [['i', \"'\", 'm', 'dancing', '.']],\n",
              " [['i', \"'\", 'm', 'dieting', '.']],\n",
              " [['i', \"'\", 'm', 'dieting', '.']],\n",
              " [['i', \"'\", 'm', 'falling', '.']],\n",
              " [['i', \"'\", 'm', 'falling', '.']],\n",
              " [['i', \"'\", 'm', 'healthy', '.']],\n",
              " [['i', \"'\", 'm', 'healthy', '.']],\n",
              " [['i', \"'\", 'm', 'in', 'jail', '.']],\n",
              " [['i', \"'\", 'm', 'in', 'jail', '.']],\n",
              " [['i', \"'\", 'm', 'not', 'tom', '.']],\n",
              " [['i', \"'\", 'm', 'popular', '.']],\n",
              " [['i', \"'\", 'm', 'reading', '.']],\n",
              " [['i', \"'\", 'm', 'reading', '.']],\n",
              " [['i', \"'\", 'm', 'resting', '.']],\n",
              " [['i', \"'\", 'm', 'resting', '.']],\n",
              " [['i', \"'\", 'm', 'so', 'full', '.']],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRl-xuzjbu-3",
        "outputId": "0f727a7f-d8b5-453b-9480-fab32f8f237a"
      },
      "source": [
        "preds"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['go', '.'],\n",
              " ['jump', '!'],\n",
              " ['jump', '!'],\n",
              " ['jump', '!'],\n",
              " ['jump', '!'],\n",
              " ['who', \"'\", 's', 'up', '?'],\n",
              " ['never', 'open', '!'],\n",
              " ['come', '!'],\n",
              " ['go', '!'],\n",
              " ['help', '!'],\n",
              " ['help', 'me', '!'],\n",
              " ['jump', '!'],\n",
              " ['jump', '!'],\n",
              " ['jump', '.'],\n",
              " ['jump', '.'],\n",
              " ['come', '!'],\n",
              " ['come', '!'],\n",
              " ['come', '!'],\n",
              " ['come', '!'],\n",
              " ['it', \"'\", 's', 'popularity', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['i', 'won', '!'],\n",
              " ['i', 'won', '!'],\n",
              " ['get', 'up', '.'],\n",
              " ['come', 'on', '!'],\n",
              " ['who', 'did', 'it', 'go', '?'],\n",
              " ['who', 'did', 'you', 'get', '?'],\n",
              " ['did', 'you', 'see', '?'],\n",
              " ['did', 'you', 'see', 'it', '?'],\n",
              " ['he', 'ran', '.'],\n",
              " ['he', 'ran', '.'],\n",
              " ['he', 'ran', '.'],\n",
              " ['he', 'ran', '.'],\n",
              " ['i', 'fell', '.'],\n",
              " ['i', 'fell', '.'],\n",
              " ['i', 'fell', '.'],\n",
              " ['i', 'fell', '.'],\n",
              " ['i', 'know', '.'],\n",
              " ['i', 'know', '.'],\n",
              " ['there', 'knows', 'don', \"'\", 't', 'know', '.'],\n",
              " ['i', 'lost', '.'],\n",
              " ['i', 'lost', '.'],\n",
              " ['i', 'work', '.'],\n",
              " ['i', 'work', '.'],\n",
              " ['i', \"'\", 'm', 'ok', '.'],\n",
              " ['get', 'this', '.'],\n",
              " ['listen', 'to', 'me', '.'],\n",
              " ['no', 'one', 'has', '!'],\n",
              " ['what', \"'\", 't', 'you', '?'],\n",
              " ['what', \"'\", 't', 'you', '?'],\n",
              " ['really', '?'],\n",
              " ['do', 'you', 'do', '.'],\n",
              " ['we', 'won', '.'],\n",
              " ['we', 'won', '.'],\n",
              " ['can', \"'\", 's', 'it', 'show', 'me', '?'],\n",
              " ['why', 'should', 'i', 'show', 'me', '?'],\n",
              " ['ask', 'tom', '.'],\n",
              " ['ask', 'tom', '.'],\n",
              " ['call', 'me', '.'],\n",
              " ['call', 'me', '.'],\n",
              " ['call', 'me', '.'],\n",
              " ['call', 'me', '.'],\n",
              " ['call', 'us', '.'],\n",
              " ['call', 'us', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['come', 'on', '!'],\n",
              " ['come', 'on', '!'],\n",
              " ['fold', 'it', '.'],\n",
              " ['fold', 'it', '.'],\n",
              " ['take', 'tom', 'to', 'tom', '.'],\n",
              " ['get', 'tom', '.'],\n",
              " ['grab', 'tom', '.'],\n",
              " ['get', 'out', '.'],\n",
              " ['get', 'out', '.'],\n",
              " ['let', \"'\", 's', 'home', '.'],\n",
              " ['he', 'came', '.'],\n",
              " ['he', 'came', '.'],\n",
              " ['he', 'left', '.'],\n",
              " ['let', 'go', 'out', '.'],\n",
              " ['he', 'runs', '.'],\n",
              " ['he', 'runs', '.'],\n",
              " ['he', 'runs', '.'],\n",
              " ['he', 'runs', '.'],\n",
              " ['help', '!'],\n",
              " ['help', 'me', '!'],\n",
              " ['help', 'me', '!'],\n",
              " ['help', 'me', '.'],\n",
              " ['help', 'me', '.'],\n",
              " ['help', 'us', '.'],\n",
              " ['help', 'us', '.'],\n",
              " ['help', 'us', '.'],\n",
              " ['i', \"'\", 'm', 'tom', '.'],\n",
              " ['tom', \"'\", 'm', 'tom', '.'],\n",
              " ['i', \"'\", 'm', 'tom', '.'],\n",
              " ['i', \"'\", 'm', 'fat', '.'],\n",
              " ['i', \"'\", 'm', 'fat', '.'],\n",
              " ['i', \"'\", 'm', 'ill', '.'],\n",
              " ['it', \"'\", 's', 'fine', '.'],\n",
              " ['that', \"'\", 'm', 'my', 'lawyer', '.'],\n",
              " ['it', \"'\", 'm', '.'],\n",
              " ['i', ',', 'i', '.'],\n",
              " ['i', 'can', 'me', '.'],\n",
              " ['open', '.'],\n",
              " ['open', '.'],\n",
              " ['never', 'mind', '!'],\n",
              " ['show', 'me', 'to', 'me', '.'],\n",
              " ['show', 'me', 'to', 'me', '.'],\n",
              " ['be', '!'],\n",
              " ['shut', 'up', '!'],\n",
              " ['shut', 'up', '!'],\n",
              " ['tell', 'me', 'to', 'me', '.'],\n",
              " ['tell', 'me', 'to', 'me', '.'],\n",
              " ['tom', 'ran', 'away', '.'],\n",
              " ['tom', 'ran', '.'],\n",
              " ['tom', 'won', '.'],\n",
              " ['wake', 'up', '!'],\n",
              " ['wake', 'up', '!'],\n",
              " ['wake', 'up', '!'],\n",
              " ['we', 'care', '.'],\n",
              " ['we', 'care', '.'],\n",
              " ['we', 'know', '.'],\n",
              " ['we', 'know', '.'],\n",
              " ['we', 'lost', '.'],\n",
              " ['we', 'lost', '.'],\n",
              " ['welcome', '.'],\n",
              " ['welcome', '.'],\n",
              " ['welcome', '.'],\n",
              " ['welcome', '.'],\n",
              " ['who', 'ate', '?'],\n",
              " ['who', 'won', '?'],\n",
              " ['who', \"'\", 's', 'shot', '?'],\n",
              " ['who', \"'\", 's', 'why', '?'],\n",
              " ['why', 'don', \"'\", 't', 'you', 'do', 'that', '?'],\n",
              " ['you', 'won', '.'],\n",
              " ['you', 'won', '.'],\n",
              " ['you', 'won', '.'],\n",
              " ['back', 'away', '!'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['there', 'can', 'know', 'what', '.'],\n",
              " ['i', 'know', 'so', '.'],\n",
              " ['call', 'tom', 'to', 'tom', '.'],\n",
              " ['call', 'tom', '.'],\n",
              " ['call', 'tom', 'called', '.'],\n",
              " ['call', 'tom', 'called', '.'],\n",
              " ['write', 'down', '.'],\n",
              " ['grab', 'tom', '.'],\n",
              " ['grab', 'tom', '.'],\n",
              " ['grab', 'him', '.'],\n",
              " ['money', 'have', 'fun', '.'],\n",
              " ['he', 'said', '.'],\n",
              " ['he', 'did', 'that', '.'],\n",
              " ['i', 'can', 'go', '.'],\n",
              " ['i', 'can', 'go', '.'],\n",
              " ['i', 'forgot', '.'],\n",
              " ['i', 'forgot', '.'],\n",
              " ['i', 'got', 'it', '.'],\n",
              " ['it', 'went', 've', 'to', '.'],\n",
              " ['i', 'looked', '.'],\n",
              " ['i', 'looked', '.'],\n",
              " ['i', 'phoned', '.'],\n",
              " ['i', 'prayed', '.'],\n",
              " ['i', 'shaved', '.'],\n",
              " ['i', 'use', 'it', '.'],\n",
              " ['i', 'use', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'try', '.'],\n",
              " ['i', \"'\", 'm', 'full', '.'],\n",
              " ['i', \"'\", 'm', 'game', '.'],\n",
              " ['i', \"'\", 'll', 'be', 'late', '.'],\n",
              " ['i', \"'\", 'm', 'lazy', '.'],\n",
              " ['i', \"'\", 'm', 'poor', '.'],\n",
              " ['i', \"'\", 've', 'won', '.'],\n",
              " ['i', \"'\", 've', 'won', \"'\", 've', 'arrived', '.'],\n",
              " ['it', \"'\", 's', 'hot', '.'],\n",
              " ['it', \"'\", 's', 'new', '.'],\n",
              " ['that', 'is', 'new', '.'],\n",
              " ['it', \"'\", 's', 'old', '.'],\n",
              " ['it', \"'\", 's', 'old', '.'],\n",
              " ['it', \"'\", 's', 'old', '.'],\n",
              " ['kiss', 'tom', '.'],\n",
              " ['leave', 'it', '.'],\n",
              " ['leave', 'it', '.'],\n",
              " ['leave', 'it', '.'],\n",
              " ['leave', 'it', '.'],\n",
              " ['leave', 'me', 'to', 'me', '.'],\n",
              " ['leave', 'me', '.'],\n",
              " ['leave', 'me', 'to', 'me', '.'],\n",
              " ['come', 'on', '!'],\n",
              " ['do', 'it', 'to', 'me', '.'],\n",
              " ['come', 'with', 'me', '.'],\n",
              " ['she', 'came', '.'],\n",
              " ['she', 'came', '.'],\n",
              " ['she', 'died', '.'],\n",
              " ['she', 'died', '.'],\n",
              " ['she', 'died', '.'],\n",
              " ['she', 'died', '.'],\n",
              " ['sit', 'down', '!'],\n",
              " ['sit', 'down', '!'],\n",
              " ['here', \"'\", 's', 'room', '.'],\n",
              " ['here', \"'\", 's', 'room', '.'],\n",
              " ['come', 'back', '!'],\n",
              " ['stand', 'up', '!'],\n",
              " ['stand', 'up', '!'],\n",
              " ['stop', 'tom', '.'],\n",
              " ['stop', 'tom', '.'],\n",
              " ['terrific', '!'],\n",
              " ['they', 'won', '.'],\n",
              " ['they', 'won', '.'],\n",
              " ['tom', 'came', '.'],\n",
              " ['tom', 'died', '.'],\n",
              " ['tom', 'fell', '.'],\n",
              " ['tom', 'knew', 'tom', '.'],\n",
              " ['tom', 'left', '.'],\n",
              " ['tom', 'left', '.'],\n",
              " ['tom', 'left', '.'],\n",
              " ['tom', 'lied', '.'],\n",
              " ['tom', 'lies', '.'],\n",
              " ['tom', 'lost', '.'],\n",
              " ['tom', 'cried', '.'],\n",
              " ['tom', \"'\", 's', 'up', '.'],\n",
              " ['use', 'this', '.'],\n",
              " ['use', 'this', '.'],\n",
              " ['warn', 'tom', '.'],\n",
              " ['warn', 'tom', '.'],\n",
              " ['watch', 'me', '.'],\n",
              " ['watch', 'me', '.'],\n",
              " ['watch', 'us', '.'],\n",
              " ['watch', 'us', '.'],\n",
              " ['we', \"'\", 'll', 'go', '.'],\n",
              " ['we', \"'\", 'll', 'go', '.'],\n",
              " ['what', 'for', '?'],\n",
              " ['what', 'for', '?'],\n",
              " ['who', 'am', 'i', 'am', '?'],\n",
              " ['who', 'was', '?'],\n",
              " ['who', \"'\", 's', 'why', '?'],\n",
              " ['who', 'was', 'it', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['answer', 'me', '.'],\n",
              " ['answer', 'me', '.'],\n",
              " ['children', 'fly', '.'],\n",
              " ['never', 'run', '!'],\n",
              " ['grab', 'him', '.'],\n",
              " ['grab', 'him', '.'],\n",
              " ['catch', 'him', '.'],\n",
              " ['catch', 'him', '.'],\n",
              " ['come', 'here', '.'],\n",
              " ['come', 'here', '.'],\n",
              " ['come', 'home', '.'],\n",
              " ['come', 'home', '.'],\n",
              " ['did', 'i', 'win', '?'],\n",
              " ['did', 'i', 'win', '?'],\n",
              " ['dogs', 'dogs', ',', 'too', '.'],\n",
              " ['don', \"'\", 't', 'ask', 'me', '.'],\n",
              " ['don', \"'\", 't', 'ask', 'me', '.'],\n",
              " ['don', \"'\", 't', 'drink', '.'],\n",
              " ['don', \"'\", 't', 'drink', '.'],\n",
              " ['don', \"'\", 't', 'die', '.'],\n",
              " ['don', \"'\", 't', 'die', '.'],\n",
              " ['don', \"'\", 't', 'lie', '.'],\n",
              " ['don', \"'\", 't', 'lie', '.'],\n",
              " ['don', \"'\", 't', 'run', '.'],\n",
              " ['don', \"'\", 't', 'run', '.'],\n",
              " ['don', \"'\", 't', 'run', '.'],\n",
              " ['don', \"'\", 't', 'run', '.'],\n",
              " ['forget', 'it', '.'],\n",
              " ['forget', 'it', '.'],\n",
              " ['forget', 'me', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['go', 'in', '.'],\n",
              " ['grab', 'that', '.'],\n",
              " ['grab', 'that', '.'],\n",
              " ['grab', 'that', '.'],\n",
              " ['grab', 'that', '.'],\n",
              " ['grab', 'this', '.'],\n",
              " ['grab', 'this', '.'],\n",
              " ['he', 'is', 'ill', '.'],\n",
              " ['they', 'are', 'sick', '.'],\n",
              " ['he', \"'\", 's', 'a', 'dj', '.'],\n",
              " ['he', \"'\", 's', 'a', 'dj', '.'],\n",
              " ['he', \"'\", 's', 'mine', '.'],\n",
              " ['that', 'are', 'mine', '.'],\n",
              " ['he', \"'\", 's', 'sexy', '.'],\n",
              " ['he', \"'\", 's', 'sexy', '.'],\n",
              " ['hold', 'this', '.'],\n",
              " ['hold', 'this', '.'],\n",
              " ['how', 'is', 'it', '?'],\n",
              " ['how', \"'\", 's', 'there', '?'],\n",
              " ['how', \"'\", 's', 'there', '?'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'ski', '.'],\n",
              " ['i', 'can', 'ski', '.'],\n",
              " ['i', 'can', 'ski', '.'],\n",
              " ['i', 'can', 'win', '.'],\n",
              " ['i', 'can', 'win', '.'],\n",
              " ['i', 'fainted', '.'],\n",
              " ['i', 'got', 'fat', '.'],\n",
              " ['i', 'got', 'fat', '.'],\n",
              " ['i', 'hit', 'tom', '.'],\n",
              " ['i', 'hit', 'tom', '.'],\n",
              " ['i', 'hit', 'tom', '.'],\n",
              " ['i', 'laughed', '.'],\n",
              " ['i', 'laughed', '.'],\n",
              " ['i', 'met', 'him', 'him', '.'],\n",
              " ['i', 'met', 'him', 'him', '.'],\n",
              " ['i', 'saw', 'tom', '.'],\n",
              " ['i', 'saw', 'tom', '.'],\n",
              " ['i', 'saw', 'tom', '.'],\n",
              " ['i', 'saw', 'you', '.'],\n",
              " ['i', 'saw', 'you', '.'],\n",
              " ['i', 'saw', 'you', '.'],\n",
              " ['i', 'saw', 'you', '.'],\n",
              " ['i', 'shouted', '.'],\n",
              " ['i', 'shouted', '.'],\n",
              " ['i', 'want', 'it', '.'],\n",
              " ['i', 'want', 'it', '.'],\n",
              " ['i', 'want', 'it', '.'],\n",
              " ['i', 'like', 'i', '.'],\n",
              " ['i', 'will', 'go', '.'],\n",
              " ['i', \"'\", 'll', 'call', '.'],\n",
              " ['i', \"'\", 'll', 'sing', '.'],\n",
              " ['i', \"'\", 'll', 'wait', '.'],\n",
              " ['i', \"'\", 'll', 'catch', 'him', '.'],\n",
              " ['i', \"'\", 'll', 'walk', '.'],\n",
              " ['i', \"'\", 'll', 'work', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'man', '.'],\n",
              " ['i', 'am', 'a', 'man', '.'],\n",
              " ['i', \"'\", 'm', 'awake', '.'],\n",
              " ['i', \"'\", 'm', 'awake', '.'],\n",
              " ['i', \"'\", 'm', 'bored', '.'],\n",
              " ['i', \"'\", 'm', 'clean', '.'],\n",
              " ['i', \"'\", 'm', 'dying', '.'],\n",
              " ['i', \"'\", 'm', 'dying', '.'],\n",
              " ['i', \"'\", 'm', 'going', 'to', 'me', '.'],\n",
              " ['i', \"'\", 'm', 'going', 'to', 'me', '.'],\n",
              " ['i', \"'\", 'm', 'quiet', '.'],\n",
              " ['i', \"'\", 'm', 'right', '.'],\n",
              " ['i', \"'\", 'm', 'young', '.'],\n",
              " ['it', 'went', '.'],\n",
              " ['it', 'went', '.'],\n",
              " ['it', 'went', '.'],\n",
              " ['it', 'rained', '.'],\n",
              " ['it', 'went', '.'],\n",
              " ['it', \"'\", 'clock', '.'],\n",
              " ['it', 'is', 'cold', '.'],\n",
              " ['it', \"'\", 's', 'easy', '.'],\n",
              " ['there', \"'\", 's', 'food', '.'],\n",
              " ['it', \"'\", 's', 'good', '.'],\n",
              " ['it', \"'\", 's', 'here', '.'],\n",
              " ['it', \"'\", 's', 'here', '.'],\n",
              " ['it', \"'\", 's', 'mine', '.'],\n",
              " ['it', \"'\", 's', 'ours', '.'],\n",
              " ['it', \"'\", 's', 'ours', '.'],\n",
              " ['it', \"'\", 's', 'dead', '.'],\n",
              " ['it', \"'\", 's', 'time', '.'],\n",
              " ['it', \"'\", 's', 'work', '.'],\n",
              " ['keep', 'it', '.'],\n",
              " ['keep', 'them', '.'],\n",
              " ['leave', 'now', '.'],\n",
              " ['leave', 'now', '.'],\n",
              " ['let', 'me', 'go', 'and', 'go', '.'],\n",
              " ['let', 'me', 'go', '!'],\n",
              " ['let', 'me', 'go', '.'],\n",
              " ['let', 'me', 'go', '.'],\n",
              " ['let', 'me', 'in', '.'],\n",
              " ['let', 'me', 'in', '.'],\n",
              " ['let', 'us', 'go', '.'],\n",
              " ['let', 'us', 'go', '.'],\n",
              " ['let', 'us', 'go', '.'],\n",
              " ['let', 'us', 'go', '.'],\n",
              " ['let', 'us', 'in', '.'],\n",
              " ['let', 'us', 'in', '.'],\n",
              " ['either', 'ask', 'me', '.'],\n",
              " ['look', 'back', '!'],\n",
              " ['stop', 'back', '!'],\n",
              " ['look', 'behind', 'you', '.'],\n",
              " ['stop', 'you', '.'],\n",
              " ['read', 'this', '.'],\n",
              " ['read', 'this', '.'],\n",
              " ['search', 'me', '.'],\n",
              " ['search', 'me', '.'],\n",
              " ['stop', 'again', '.'],\n",
              " ['see', 'it', 'down', '.'],\n",
              " ['she', 'cried', '.'],\n",
              " ['she', 'cried', '.'],\n",
              " ['she', 'tried', '.'],\n",
              " ['she', 'walks', '.'],\n",
              " ['she', 'walks', '.'],\n",
              " ['here', 'here', '.'],\n",
              " ['here', 'here', '.'],\n",
              " ['let', \"'\", 's', 'there', \"'\", 's', 'go', '.'],\n",
              " ['come', 'there', 'there', '.'],\n",
              " ['come', 'now', '.'],\n",
              " ['come', 'now', '.'],\n",
              " ['stay', 'away', '.'],\n",
              " ['stay', 'back', '.'],\n",
              " ['stay', 'home', '.'],\n",
              " ['stay', 'thin', '.'],\n",
              " ['stop', 'him', '.'],\n",
              " ['stop', 'them', '.'],\n",
              " ['take', 'care', '!'],\n",
              " ['take', 'care', '!'],\n",
              " ['take', 'care', '.'],\n",
              " ['take', 'care', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', '.'],\n",
              " ['here', '.'],\n",
              " ['do', 'you', 'do', '.'],\n",
              " ['that', \"'\", 's', '.'],\n",
              " ['that', \"'\", 's', 'that', '.'],\n",
              " ['they', 'fell', '.'],\n",
              " ['they', 'fell', '.'],\n",
              " ['let', 'go', 'out', '.'],\n",
              " ['they', 'left', '.'],\n",
              " ['they', 'lied', '.'],\n",
              " ['they', 'lied', '.'],\n",
              " ['they', 'lost', '.'],\n",
              " ['they', 'lost', '.'],\n",
              " ['tom', 'cried', '.'],\n",
              " ['tom', 'drank', 'the', 'hat', '.'],\n",
              " ['tom', 'knits', '.'],\n",
              " ['tom', 'knows', '.'],\n",
              " ['tom', 'moved', 'to', 'me', '.'],\n",
              " ['tom', 'tried', '.'],\n",
              " ['tom', 'tried', '.'],\n",
              " ['tom', 'tries', '.'],\n",
              " ['tom', 'voted', '.'],\n",
              " ['tom', 'voted', '.'],\n",
              " ['tom', 'walks', '.'],\n",
              " ['tom', 'works', '.'],\n",
              " ['try', 'again', '.'],\n",
              " ['try', 'again', '.'],\n",
              " ['i', 'try', 'it', 'on', '.'],\n",
              " ['i', \"'\", 's', 'on', '.'],\n",
              " ['wait', 'here', '.'],\n",
              " ['wait', 'here', '.'],\n",
              " ['watch', 'tom', '.'],\n",
              " ['give', 'tom', '.'],\n",
              " ['we', 'saw', 'it', '.'],\n",
              " ['we', 'saw', 'you', '.'],\n",
              " ['we', 'saw', 'it', '.'],\n",
              " ['we', 'saw', 'you', '.'],\n",
              " ['we', 'saw', 'it', '.'],\n",
              " ['we', 'saw', 'it', '.'],\n",
              " ['we', 'talked', '.'],\n",
              " ['we', 'waited', '.'],\n",
              " ['we', 'waited', '.'],\n",
              " ['we', 'waited', '.'],\n",
              " ['we', 'waited', '.'],\n",
              " ['we', \"'\", 're', 'shy', '.'],\n",
              " ['we', \"'\", 're', 'shy', '.'],\n",
              " ['that', 'a', 'aunt', 'loves', '!'],\n",
              " ['what', \"'\", 's', 'going', 'on', '?'],\n",
              " ['what', \"'\", 's', 'up', '?'],\n",
              " ['what', 'did', 'you', 'do', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'the', 'pain', '?'],\n",
              " ['who', 'do', 'you', 'know', '?'],\n",
              " ['who', 'knows', 'anybody', '?'],\n",
              " ['who', \"'\", 's', 'go', '?'],\n",
              " ['who', \"'\", 's', 'tom', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'she', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['you', 'drive', '.'],\n",
              " ['you', 'drive', '.'],\n",
              " ['you', 'idiot', '!'],\n",
              " ['you', 'tried', '.'],\n",
              " ['you', 'tried', '.'],\n",
              " ['are', 'you', 'all', 'right', '?'],\n",
              " ['are', 'you', 'all', 'right', '?'],\n",
              " ['are', 'you', 'all', 'right', '?'],\n",
              " ['ask', 'anyone', '.'],\n",
              " ['be', 'careful', '!'],\n",
              " ['come', 'on', 'time', '.'],\n",
              " ['come', 'on', 'time', '.'],\n",
              " ['birds', 'sing', '.'],\n",
              " ['bring', 'wine', '.'],\n",
              " ['leave', 'wine', '.'],\n",
              " ['come', 'again', '.'],\n",
              " ['come', 'again', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['come', 'back', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['do', 'men', 'cry', '?'],\n",
              " ['do', 'people', 'cry', '?'],\n",
              " ['don', \"'\", 't', 'you', 'don', \"'\", 't', 'look', 'at', 'it', '.'],\n",
              " ['don', \"'\", 't', 'forget', 'it', 'to', '.'],\n",
              " ['don', \"'\", 't', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'talk', 'of', 'me', '!'],\n",
              " ['don', \"'\", 't', 'talk', 'of', 'me', '!'],\n",
              " ['don', \"'\", 't', 'talk', 'to', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'talk', 'to', 'tom', '.'],\n",
              " ['do', 'it', 'up', '.'],\n",
              " ['fire', 'burns', '.'],\n",
              " ['fire', 'burns', '.'],\n",
              " ['forget', 'tom', '.'],\n",
              " ['forget', 'tom', '.'],\n",
              " ['forget', 'tom', '.'],\n",
              " ['forget', 'him', '.'],\n",
              " ['forget', 'him', '.'],\n",
              " ['forget', 'him', '.'],\n",
              " ['forget', 'him', '.'],\n",
              " ['there', \"'\", 't', 'get', '.'],\n",
              " ['he', 'is', 'here', '!'],\n",
              " ['here', 'is', 'here', '.'],\n",
              " ['it', 'is', 'here', '!'],\n",
              " ['he', 'is', 'late', '.'],\n",
              " ['he', 'is', 'nice', '.'],\n",
              " ['they', \"'\", 're', 'nice', '.'],\n",
              " ['he', 'laughed', '.'],\n",
              " ['they', 'laughed', '.'],\n",
              " ['he', \"'\", 's', 'swiss', '.'],\n",
              " ['he', \"'\", 's', 'smart', '.'],\n",
              " ['they', \"'\", 're', 'smart', '.'],\n",
              " ['don', \"'\", 't', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'tom', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'man', '.'],\n",
              " ['i', 'am', 'a', 'man', '.'],\n",
              " ['i', \"'\", 'm', 'game', '.'],\n",
              " ['i', 'can', 'read', '.'],\n",
              " ['i', 'can', 'read', '.'],\n",
              " ['i', 'can', 'read', '.'],\n",
              " ['i', 'can', 'swim', '.'],\n",
              " ['i', 'can', 'swim', '.'],\n",
              " ['i', 'can', 'swim', '.'],\n",
              " ['i', 'can', 'walk', '.'],\n",
              " ['i', 'can', 'walk', '.'],\n",
              " ['i', 'eat', 'here', '.'],\n",
              " ['i', 'eat', 'here', '.'],\n",
              " ['i', 'eat', 'meat', '.'],\n",
              " ['i', 'eat', 'meat', '.'],\n",
              " ['i', 'eat', 'rice', '.'],\n",
              " ['i', 'eat', 'rice', '.'],\n",
              " ['i', 'help', 'tom', '.'],\n",
              " ['i', 'help', 'tom', '.'],\n",
              " ['i', 'just', 'ate', '.'],\n",
              " ['i', 'like', 'that', 'i', '.'],\n",
              " ['i', 'like', 'them', '.'],\n",
              " ['i', 'like', 'tea', '.'],\n",
              " ['i', 'like', 'you', '.'],\n",
              " ['i', 'like', 'you', '.'],\n",
              " ['i', 'like', 'you', '.'],\n",
              " ['i', 'love', 'you', '.'],\n",
              " ['i', 'love', 'you', '.'],\n",
              " ['i', 'made', 'tea', '.'],\n",
              " ['i', 'miss', 'you', '.'],\n",
              " ['i', 'miss', 'you', '.'],\n",
              " ['i', 'ran', 'away', '.'],\n",
              " ['i', 'ran', 'away', '.'],\n",
              " ['i', 'remember', 'it', '.'],\n",
              " ['i', 'remember', '.'],\n",
              " ['i', 'remember', '.'],\n",
              " ['i', 'screamed', '.'],\n",
              " ['i', 'screamed', '.'],\n",
              " ['i', 'see', 'me', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'shot', 'tom', '.'],\n",
              " ['i', 'want', 'tom', '.'],\n",
              " ['i', 'want', 'tom', '.'],\n",
              " ['i', 'want', 'you', 'back', '.'],\n",
              " ['i', 'want', 'you', '.'],\n",
              " ['i', 'want', 'you', '.'],\n",
              " ['i', 'was', 'good', '.'],\n",
              " ['i', 'was', 'good', '.'],\n",
              " ['i', 'was', 'late', '.'],\n",
              " ['i', 'was', 'sick', '.'],\n",
              " ['i', 'was', 'sick', '.'],\n",
              " ['i', 'work', 'out', '.'],\n",
              " ['i', 'work', 'out', '.'],\n",
              " ['i', \"'\", 'll', 'leave', '.'],\n",
              " ['i', \"'\", 'll', 'start', '.'],\n",
              " ['i', \"'\", 'll', 'start', '.'],\n",
              " ['i', \"'\", 'll', 'start', '.'],\n",
              " ['i', 'am', '30', 'now', '.'],\n",
              " ['i', 'am', '30', 'now', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'liar', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'liar', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'poet', '.'],\n",
              " ['i', \"'\", 'm', 'coming', '.'],\n",
              " ['i', \"'\", 'm', 'coming', '.'],\n",
              " ['i', \"'\", 'm', 'hungry', '!'],\n",
              " ['i', \"'\", 'm', 'hungry', '.'],\n",
              " ['i', \"'\", 'm', 'scared', '.'],\n",
              " ['i', \"'\", 'm', 'scared', 'of', 'it', '.'],\n",
              " ['i', \"'\", 'm', 'sleepy', '!'],\n",
              " ['i', \"'\", 'm', 'so', 'fat', '.'],\n",
              " ['i', \"'\", 'm', 'so', 'fat', '.'],\n",
              " ['i', \"'\", 'm', 'trying', '.'],\n",
              " ['i', \"'\", 'm', 'trying', '.'],\n",
              " ['is', 'tom', 'big', '?'],\n",
              " ['is', 'he', 'tall', '?'],\n",
              " ['is', 'he', 'tall', '?'],\n",
              " ['is', 'it', 'done', '?'],\n",
              " ['is', 'it', 'free', '?'],\n",
              " ['is', 'it', 'free', '?'],\n",
              " ['is', 'it', 'hard', 'to', 'do', '?'],\n",
              " ['is', 'it', 'here', '?'],\n",
              " ['is', 'it', 'nice', '?'],\n",
              " ['is', 'there', 'time', '?'],\n",
              " ['is', 'this', 'true', '?'],\n",
              " ['is', 'this', '?'],\n",
              " ['is', 'this', '?'],\n",
              " ['is', 'this', '?'],\n",
              " ['is', 'it', 'true', '?'],\n",
              " ['is', 'this', '?'],\n",
              " ['it', \"'\", 's', 'alive', '.'],\n",
              " ['it', \"'\", 's', 'my', 'cd', '.'],\n",
              " ['it', \"'\", 's', 'my', 'cd', '.'],\n",
              " ['it', 'is', 'already', '.'],\n",
              " ['it', \"'\", 's', 'ready', '.'],\n",
              " ['it', 'is', 'white', '.'],\n",
              " ['it', \"'\", 's', 'white', '.'],\n",
              " ['it', 'is', 'white', '.'],\n",
              " ['it', \"'\", 's', 'white', '.'],\n",
              " ['it', \"'\", 's', 'not', 'yours', '.'],\n",
              " ['it', \"'\", 's', 'yours', '.'],\n",
              " ['it', \"'\", 's', 'yours', '.'],\n",
              " ['it', \"'\", 's', 'yours', 'is', '.'],\n",
              " ['eat', 'half', 'of', 'me', '.'],\n",
              " ['never', 'run', '!'],\n",
              " ['keep', 'quiet', '!'],\n",
              " ['keep', 'calm', '!'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['let', 'tom', 'to', 'tom', '.'],\n",
              " ['let', 'tom', 'go', '.'],\n",
              " ['let', 'tom', 'go', '.'],\n",
              " ['let', 'tom', 'go', '.'],\n",
              " ['let', 'tom', 'in', '.'],\n",
              " ['let', 'tom', 'in', '.'],\n",
              " ['let', 'him', '!'],\n",
              " ['let', 'him', 'go', '!'],\n",
              " ['let', 'me', 'take', 'it', '.'],\n",
              " ['let', 'me', 'take', 'it', '.'],\n",
              " ['let', \"'\", 's', 'chat', '.'],\n",
              " ['let', \"'\", 's', 'kiss', '.'],\n",
              " ['let', \"'\", 's', 'talk', 'about', '.'],\n",
              " ['leave', 'again', '.'],\n",
              " ['leave', 'tom', '.'],\n",
              " ['stop', 'it', '.'],\n",
              " ['stop', 'it', '.'],\n",
              " ['take', 'it', '.'],\n",
              " ['look', 'at', 'me', '.'],\n",
              " ['look', 'at', 'me', '.'],\n",
              " ['look', 'there', '.'],\n",
              " ['look', 'there', '.'],\n",
              " ['love', 'love', '.'],\n",
              " ['never', 'leave', '!'],\n",
              " ['let', 'go', '!'],\n",
              " ['let', \"'\", 's', 'go', 'out', '.'],\n",
              " ['let', \"'\", 's', 'go', 'out', '.'],\n",
              " ['no', 'one', 'ran', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['shadow', 'him', '.'],\n",
              " ['shadow', 'him', '.'],\n",
              " ['shadow', 'him', '.'],\n",
              " ['shadow', 'him', '.'],\n",
              " ['she', 'is', 'old', '.'],\n",
              " ['she', 'is', 'old', '.'],\n",
              " ['smell', 'this', '.'],\n",
              " ['smell', 'this', '.'],\n",
              " ['stand'],\n",
              " ['stand', 'back', '.'],\n",
              " ['here', '.'],\n",
              " ['here', '.'],\n",
              " ['here', '.'],\n",
              " ['here', '.'],\n",
              " ['start', 'here', '.'],\n",
              " ['start', 'here', '.'],\n",
              " ['stay', 'away', '.'],\n",
              " ['stay', 'away', '.'],\n",
              " ['stay', 'still', '.'],\n",
              " ['shut', 'it', '!'],\n",
              " ['shut', 'up', '!'],\n",
              " ['study', 'hard', '.'],\n",
              " ['study', 'hard', '.'],\n",
              " ['take', 'a', 'bus', '.'],\n",
              " ['take', 'a', 'bus', '.'],\n",
              " ['take', 'a', 'bus', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['take', 'these', '.'],\n",
              " ['take', 'these', '.'],\n",
              " ['talk', 'to', 'me', '.'],\n",
              " ['come', 'with', 'us', 'with', 'us', '.'],\n",
              " ['that', 'a', 'aunt', 'loves', '!'],\n",
              " ['he', 'is', 'tom', '.'],\n",
              " ['he', \"'\", 's', 'tom', '.'],\n",
              " ['that', \"'\", 's', 'wet', '.'],\n",
              " ['they', 'stood', '.'],\n",
              " ['they', 'stood', '.'],\n",
              " ['they', 'tried', '.'],\n",
              " ['i', \"'\", 'll', 'catch', 'me', '.'],\n",
              " ['i', \"'\", 'll', 'catch', 'me', '.'],\n",
              " ['time', 'is', 'time', '.'],\n",
              " ['it', \"'\", 's', 'time', '.'],\n",
              " ['tom', 'bit', 'me', '.'],\n",
              " ['tom', 'burped', '.'],\n",
              " ['tom', 'phoned', '.'],\n",
              " ['tom', 'called', '.'],\n",
              " ['tom', 'danced', '.'],\n",
              " ['tom', 'drinks', '.'],\n",
              " ['tom', 'failed', '.'],\n",
              " ['tom', 'failed', '.'],\n",
              " ['tom', 'forgot', '.'],\n",
              " ['tom', 'fought', '.'],\n",
              " ['tom', 'helped', '.'],\n",
              " ['tom', 'is', 'fat', '.'],\n",
              " ['tom', 'is', 'ill', '.'],\n",
              " ['tom', 'is', 'outside', '.'],\n",
              " ['tom', 'is', 'out', '.'],\n",
              " ['tom', 'jumped', '.'],\n",
              " ['tom', 'looked', '.'],\n",
              " ['tom', 'moaned', '.'],\n",
              " ['tom', 'phoned', '.'],\n",
              " ['tom', 'saw', 'me', '.'],\n",
              " ['tom', 'saw', 'me', '.'],\n",
              " ['tom', 'shaved', '.'],\n",
              " ['tom', 'snores', '.'],\n",
              " ['tom', 'talked', '.'],\n",
              " ['tom', 'waited', '.'],\n",
              " ['tom', 'waited', '.'],\n",
              " ['tom', 'yawned', '.'],\n",
              " ['tom', 'yelled', '.'],\n",
              " ['tom', \"'\", 'll', 'die', '.'],\n",
              " ['tom', \"'\", 's', 'deaf', '.'],\n",
              " ['tom', 'is', 'ill', '.'],\n",
              " ['tom', \"'\", 's', 'ugly', '.'],\n",
              " ['go', '.'],\n",
              " ['go', 'home', '.'],\n",
              " ['take', 'a', 'look', '.'],\n",
              " ['take', 'you', '.'],\n",
              " ['we', 'are', 'men', '.'],\n",
              " ['we', 'are', 'men', '.'],\n",
              " ['we', 'had', 'fun', '.'],\n",
              " ['we', 'had', 'fun', '.'],\n",
              " ['we', 'laughed', '.'],\n",
              " ['we', 'laughed', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'want', 'it', '.'],\n",
              " ['we', 'want', 'it', '.'],\n",
              " ['we', 'want', 'it', '.'],\n",
              " ['we', \"'\", 'll', 'help', '.'],\n",
              " ['we', \"'\", 'll', 'help', '.'],\n",
              " ['we', \"'\", 'll', 'sing', '.'],\n",
              " ['we', \"'\", 'll', 'sing', '.'],\n",
              " ['we', \"'\", 'll', 'wait', '.'],\n",
              " ['we', \"'\", 'll', 'wait', '.'],\n",
              " ['we', \"'\", 'll', 'work', '.'],\n",
              " ['we', \"'\", 're', 'back', 'now', '.'],\n",
              " ['we', \"'\", 're', 'back', 'back', '.'],\n",
              " ['we', \"'\", 're', 'boys', '.'],\n",
              " ['we', \"'\", 're', 'boys', '.'],\n",
              " ['we', \"'\", 're', 'fine', '.'],\n",
              " ['we', \"'\", 're', 'fine', '.'],\n",
              " ['we', \"'\", 're', 'fine', '.'],\n",
              " ['we', \"'\", 're', 'going', 'anywhere', '.'],\n",
              " ['where', 'am', 'i', 'have', '?'],\n",
              " ['who', 'are', 'we', '?'],\n",
              " ['who', 'are', 'we', '?'],\n",
              " ['who', 'did', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', 'has', 'anybody', '?'],\n",
              " ['who', \"'\", 's', 'tom', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'she', '?'],\n",
              " ['who', 'phoned', '?'],\n",
              " ['who', 'was', '?'],\n",
              " ['who', 'was', 'it', '?'],\n",
              " ['who', \"'\", 's', 'shot', '?'],\n",
              " ['who', \"'\", 's', 'here', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['wood', 'burns', '.'],\n",
              " ['you', 'do', 'you', '.'],\n",
              " ['you', 'decide', 'why', '.'],\n",
              " ['you', 'decide', '.'],\n",
              " ['you', 'decide', '.'],\n",
              " ['you', 'may', 'go', '.'],\n",
              " ['you', 'may', 'go', '.'],\n",
              " ['you', 'can', 'leave', '.'],\n",
              " ['you', 'may', 'go', '.'],\n",
              " ['you', \"'\", 've', 'won', '.'],\n",
              " ['you', \"'\", 've', 'won', '.'],\n",
              " ['you', \"'\", 've', 'won', '.'],\n",
              " ['is', 'anybody', 'home', '?'],\n",
              " ['why', 'did', 'anyone', 'hurt', '?'],\n",
              " ['is', 'it', 'done', '?'],\n",
              " ['are', 'you', 'tom', '?'],\n",
              " ['are', 'you', 'tom', '?'],\n",
              " ['are', 'you', 'tom', '?'],\n",
              " ['are', 'you', 'tom', '?'],\n",
              " ['are', 'you', 'new', '?'],\n",
              " ['are', 'you', 'new', '?'],\n",
              " ['are', 'you', 'new', '?'],\n",
              " ['be', 'prepared', '.'],\n",
              " ['breathe', 'out', '.'],\n",
              " ['breathe', 'out', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['did', 'tom', 'die', '?'],\n",
              " ['did', 'tom', 'die', '?'],\n",
              " ['did', 'tom', 'eat', '?'],\n",
              " ['did', 'tom', 'win', '?'],\n",
              " ['don', \"'\", 't', 'argue', '?'],\n",
              " ['don', \"'\", 't', 'argue', '?'],\n",
              " ['don', \"'\", 't', 'argue', 'with', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'argue', '.'],\n",
              " ['don', \"'\", 't', 'argue', '.'],\n",
              " ['don', \"'\", 't', 'you', 'don', \"'\", 's', 'broken', '.'],\n",
              " ['don', \"'\", 't', 'you', 'don', \"'\", 's', 'broken', '.'],\n",
              " ['don', \"'\", 't', 'fight', '.'],\n",
              " ['don', \"'\", 't', 'leave', 'it', '.'],\n",
              " ['don', \"'\", 't', 'leave', '.'],\n",
              " ['don', \"'\", 't', 'leave', 'it', '.'],\n",
              " ['don', \"'\", 't', 'leave', '.'],\n",
              " ['don', \"'\", 't', 'shout', '.'],\n",
              " ['don', \"'\", 't', 'shout', '.'],\n",
              " ['let', \"'\", 's', 'with', 'us', 'with', 'us', '.'],\n",
              " ['forgive', 'tom', '.'],\n",
              " ['forgive', 'tom', '.'],\n",
              " ['get', 'using', 'cake', '.'],\n",
              " ['change', 'your', 'clothes', '.'],\n",
              " ['either', '.'],\n",
              " ['get', 'started', '.'],\n",
              " ['get', 'started', '.'],\n",
              " ['get', 'started', '.'],\n",
              " ['go', 'to', '.'],\n",
              " ['go', 'have', 'come', 'to', 'go', '.'],\n",
              " ['go', 'help', 'tom', '.'],\n",
              " ['let', \"'\", 's', 'bus', '.'],\n",
              " ['come', 'put', '.'],\n",
              " ['he', 'can', 'come', '.'],\n",
              " ['he', 'can', 'come', '.'],\n",
              " ['he', 'can', 'read', 'it', '.'],\n",
              " ['he', 'can', 'read', '.'],\n",
              " ['he', 'found', 'it', '.'],\n",
              " ['they', 'found', 'it', '.'],\n",
              " ['he', 'has', 'wine', 'wine', '.'],\n",
              " ['he', 'is', 'happy', '.'],\n",
              " ['he', 'is', 'happy', '.'],\n",
              " ['he', 'is', 'lying', '.'],\n",
              " ['they', \"'\", 're', 'lying', '.'],\n",
              " ['he', 'is', 'young', '.'],\n",
              " ['he', 'resigned', '.'],\n",
              " ['he', 'resigned', '.'],\n",
              " ['he', 'stood', 'up', '.'],\n",
              " ['he', 'stood', 'up', '.'],\n",
              " ['he', 'stood', 'up', '.'],\n",
              " ['they', 'stood', '.'],\n",
              " ['how', 'many', 'you', 'know', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'strange', '!'],\n",
              " ['i', 'am', 'taller', '.'],\n",
              " ['i', 'came', 'back', '.'],\n",
              " ['i', 'came', 'back', '.'],\n",
              " ['i', 'can', 'dance', '.'],\n",
              " ['i', 'can', 'dance', '.'],\n",
              " ['i', 'can', 'dance', '.'],\n",
              " ['i', 'can', 'do', 'it', '.'],\n",
              " ['i', 'can', 'do', 'it', '.'],\n",
              " ['i', 'can', \"'\", 't', 'fly', '.'],\n",
              " ['i', 'can', \"'\", 't', 'say', '.'],\n",
              " ['i', 'can', \"'\", 't', 'tell', 'me', '.'],\n",
              " ['i', 'can', \"'\", 't', 'tell', 'you', '.'],\n",
              " ['i', 'don', \"'\", 't', 'cry', '.'],\n",
              " ['i', 'don', \"'\", 't', 'eat', 'it', '.'],\n",
              " ['i', 'drank', 'tea', '.'],\n",
              " ['i', 'drank', 'tea', '.'],\n",
              " ['i', 'eat', 'bread', '.'],\n",
              " ['i', 'eat', 'bread', '.'],\n",
              " ['i', 'eat', 'bread', '.'],\n",
              " ['i', 'eat', 'bread', '.'],\n",
              " ['i', 'eat', 'fruit', '.'],\n",
              " ['i', 'eat', 'fruit', '.'],\n",
              " ['i', 'exercised', '.'],\n",
              " ['i', 'feel', 'fine', '.'],\n",
              " ['i', 'felt', 'that', '.'],\n",
              " ['i', 'forgot', 'it', '.'],\n",
              " ['i', 'forgot', 'it', '.'],\n",
              " ['i', 'had', 'a', 'cat', '.'],\n",
              " ['i', 'have', 'time', '.'],\n",
              " ['i', 'have', 'wine', 'wine', '.'],\n",
              " ['i', 'knew', 'that', '.'],\n",
              " ['i', 'know', 'this', '.'],\n",
              " ['i', 'left', 'home', '.'],\n",
              " ['i', 'left', 'home', '.'],\n",
              " ['i', 'left', 'home', '.'],\n",
              " ['i', 'left', 'home', '.'],\n",
              " ['i', 'like', 'beer', '.'],\n",
              " ['i', 'like', 'both', '.'],\n",
              " ['i', 'like', 'cake', '.'],\n",
              " ['i', 'like', 'cats', '.'],\n",
              " ['i', 'like', 'fish', '.'],\n",
              " ['i', 'like', 'math', '.'],\n",
              " ['i', 'like', 'rice', '.'],\n",
              " ['i', 'like', 'that', 'i', '.'],\n",
              " ['i', 'like', 'that', 'likes', 'it', '.'],\n",
              " ['i', 'like', 'that', '.'],\n",
              " ['i', 'made', 'that', '.'],\n",
              " ['i', 'made', 'that', '.'],\n",
              " ['i', 'need', 'time', '.'],\n",
              " ['i', 'never', 'cry', '.'],\n",
              " ['i', 'saved', 'you', '.'],\n",
              " ['i', 'saved', 'you', '.'],\n",
              " ['i', 'smell', 'gas', '.'],\n",
              " ['i', 'trust', 'tom', '.'],\n",
              " ['i', 'want', 'more', '.'],\n",
              " ['i', 'want', 'more', '.'],\n",
              " ['i', 'want', 'them', '.'],\n",
              " ['i', 'want', 'them', '.'],\n",
              " ['i', 'want', 'them', '.'],\n",
              " ['i', 'want', 'this', '.'],\n",
              " ['i', 'want', 'this', '.'],\n",
              " ['i', 'want', 'time', '.'],\n",
              " ['i', 'was', 'alone', '.'],\n",
              " ['i', 'was', 'alone', '.'],\n",
              " ['i', 'was', 'naked', '.'],\n",
              " ['i', 'was', 'naked', '.'],\n",
              " ['i', 'was', 'naked', '.'],\n",
              " ['i', 'was', 'naked', '.'],\n",
              " ['i', 'was', 'stuck', '.'],\n",
              " ['i', 'was', 'stuck', '.'],\n",
              " ['i', 'was', 'tired', '.'],\n",
              " ['i', 'was', 'tired', '.'],\n",
              " ['i', 'went', ',', 'too', '.'],\n",
              " ['i', 'went', ',', 'too', '.'],\n",
              " ['i', \"'\", 'll', 'work', '.'],\n",
              " ['i', 'won', \"'\", 'm', 'not', 'lie', '.'],\n",
              " ['i', \"'\", 'll', 'buy', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'decide', '.'],\n",
              " ['i', \"'\", 'll', 'decide', '.'],\n",
              " ['i', \"'\", 'll', 'eat', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'finish', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'me', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'scream', '.'],\n",
              " ['i', \"'\", 'll', 'dance', '.'],\n",
              " ['i', \"'\", 'm', 'dancing', '.'],\n",
              " ['i', \"'\", 'm', 'dieting', '.'],\n",
              " ['i', \"'\", 'm', 'dieting', '.'],\n",
              " ['i', \"'\", 'm', 'falling', '.'],\n",
              " ['i', \"'\", 'm', 'falling', '.'],\n",
              " ['i', \"'\", 'm', 'healthy', '.'],\n",
              " ['i', \"'\", 'm', 'healthy', '.'],\n",
              " ['i', \"'\", 'm', 'in', 'jail', '.'],\n",
              " ['i', \"'\", 'm', 'in', 'jail', '.'],\n",
              " ['i', \"'\", 'm', 'not', 'tom', '.'],\n",
              " ['i', \"'\", 'm', 'popular', '.'],\n",
              " ['i', \"'\", 'm', 'reading', '.'],\n",
              " ['i', \"'\", 'm', 'reading', '.'],\n",
              " ['i', \"'\", 'm', 'resting', '.'],\n",
              " ['i', \"'\", 'm', 'resting', '.'],\n",
              " ['i', \"'\", 'm', 'so', 'full', '.'],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0InokXUYcOT",
        "outputId": "1cc54233-226b-4a2f-a559-1bfae02b0200"
      },
      "source": [
        "bleu_score(preds, trgs)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8057653903961182"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlRo6ULbG_nZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}