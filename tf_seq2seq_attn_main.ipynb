{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_seq2seq_attn_main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pCor_QpNH2v2",
        "Z5kGD2XaVs_I",
        "oRlvnjN2brFN",
        "BD2K-vW6YVSX",
        "MxTYP4xOZSEb",
        "PyUwjTBBDYUz"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP+Febp4zx6Tje80cywXRK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5669bd3563f4a249e8b0c61970e6373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_19b95e37990b4f6784e89872e5504093",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4a3a2cc4fa1e41c2b65e1487b59533c3",
              "IPY_MODEL_65769da98a59446fb39ea88c25ed735a"
            ]
          }
        },
        "19b95e37990b4f6784e89872e5504093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a3a2cc4fa1e41c2b65e1487b59533c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e97cc506abb84c0ea8acbad031cc47a1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 40751,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 40751,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94a8d44dff4e48c7bcecf15b5984104f"
          }
        },
        "65769da98a59446fb39ea88c25ed735a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de92aaaedeb848719abc00718b0bb91f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 40751/40751 [41:59&lt;00:00, 16.17it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c85fe942a224a2d926c49e6ebb8164c"
          }
        },
        "e97cc506abb84c0ea8acbad031cc47a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94a8d44dff4e48c7bcecf15b5984104f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de92aaaedeb848719abc00718b0bb91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c85fe942a224a2d926c49e6ebb8164c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devashish-Siwatch/marathi-neural-machine-translation/blob/main/tf_seq2seq_attn_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m8OV0wClQDQ"
      },
      "source": [
        "Before beginning this notebook, ensure that you have data.csv in available in the working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diGgpmn6_hO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c25783c-0a36-4f8b-d2c3-6ad035a3a3aa"
      },
      "source": [
        "!pip install torchtext==0.8.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkCLyMjMK0g_"
      },
      "source": [
        "## Hyperparameter declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VxIa3PK5ZA"
      },
      "source": [
        "from argparse import Namespace"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnp0_Z41K9OS"
      },
      "source": [
        "hype = Namespace(\r\n",
        "    LR = 0.0001,\r\n",
        "    BATCH_SIZE = 64,\r\n",
        "    NUM_EPOCHS = 100,\r\n",
        "    CLIP = 1,\r\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILw_JUj5MGTP"
      },
      "source": [
        "model_hype = Namespace(\r\n",
        "    EMBEDDING_SIZE = 128,\r\n",
        "    GRU_UNITS = 512,\r\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDHMJQQALwNZ",
        "outputId": "f7080b20-30af-44b0-fa34-68b70f15da55"
      },
      "source": [
        "#example usage\r\n",
        "hype.BATCH_SIZE"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqxGUlxQNzEo",
        "outputId": "cc94ec58-7a60-4ead-c2a6-ad4959707b90"
      },
      "source": [
        "#to dict\r\n",
        "vars(hype)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BATCH_SIZE': 64, 'CLIP': 1, 'LR': 0.0001, 'NUM_EPOCHS': 100}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wznA4tHCk3ZN"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRM-bUZo60qp"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2-a9XNPlOym"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "K1ftPojC5LWC",
        "outputId": "3d8fa94c-80b9-40c2-ccbe-4692b0bc6417"
      },
      "source": [
        "# importing the data from csv file\r\n",
        "data = pd.read_csv('data.csv', header=None)\r\n",
        "data.columns = ['english', 'marathi']\r\n",
        "data.tail()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>marathi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40746</th>\n",
              "      <td>Just saying you don't like fish because of the...</td>\n",
              "      <td>हड्डींमुळे मासे आवडत नाही असं म्हणणं हे काय मा...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40747</th>\n",
              "      <td>The Japanese Parliament today officially elect...</td>\n",
              "      <td>आज जपानी संसदेने अधिकृतरित्या र्‍यौतारौ हाशिमो...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40748</th>\n",
              "      <td>Tom tried to sell his old VCR instead of throw...</td>\n",
              "      <td>टॉमने त्याचा जुना व्ही.सी.आर फेकून टाकण्याऐवजी...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40749</th>\n",
              "      <td>You can't view Flash content on an iPad. Howev...</td>\n",
              "      <td>आयपॅडवर फ्लॅश आशय बघता येत नाही. पण तुम्ही त्य...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40750</th>\n",
              "      <td>In 1969, Roger Miller recorded a song called \"...</td>\n",
              "      <td>१९६९मध्ये रॉजर मिलरने \"यू डोन्ट वॉन्ट माय लव्ह...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 english                                            marathi\n",
              "40746  Just saying you don't like fish because of the...  हड्डींमुळे मासे आवडत नाही असं म्हणणं हे काय मा...\n",
              "40747  The Japanese Parliament today officially elect...  आज जपानी संसदेने अधिकृतरित्या र्‍यौतारौ हाशिमो...\n",
              "40748  Tom tried to sell his old VCR instead of throw...  टॉमने त्याचा जुना व्ही.सी.आर फेकून टाकण्याऐवजी...\n",
              "40749  You can't view Flash content on an iPad. Howev...  आयपॅडवर फ्लॅश आशय बघता येत नाही. पण तुम्ही त्य...\n",
              "40750  In 1969, Roger Miller recorded a song called \"...  १९६९मध्ये रॉजर मिलरने \"यू डोन्ट वॉन्ट माय लव्ह..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La4DKCxV65JZ"
      },
      "source": [
        "### Building tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0le-1MgY5N3",
        "outputId": "2805afba-d33c-4e26-f402-7ca493d40397"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing import text\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "\r\n",
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvkz5RLnbBnq"
      },
      "source": [
        "# Seperating punctuation: \"Hello, World!\" --> \"Hello , World ! \"\r\n",
        "def sep_punk(text):\r\n",
        "  for punk in string.punctuation:\r\n",
        "    text = text.replace(punk,\" \"+punk+\" \")\r\n",
        "  return text\r\n",
        "\r\n",
        "# Adding <sos> and <eos> to the sentences\r\n",
        "def add_init_token(sent_list):\r\n",
        "  new_sent_list = []\r\n",
        "  for sent in sent_list:\r\n",
        "    sent = sep_punk(sent)\r\n",
        "    sent = '<sos> ' + sent + ' <eos>'\r\n",
        "    new_sent_list.append(sent)\r\n",
        "\r\n",
        "  return new_sent_list    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rvtzx8peSqI"
      },
      "source": [
        "mar_list = list(data['marathi'])\r\n",
        "eng_list = list(data['english'])\r\n",
        "\r\n",
        "mar_list = add_init_token(mar_list)\r\n",
        "eng_list = add_init_token(eng_list)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MEWxWBge5KQ",
        "outputId": "3304c389-98d0-409c-bfae-5b076f012afa"
      },
      "source": [
        "print(mar_list[100])\r\n",
        "print(eng_list[100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> मी आहे !  <eos>\n",
            "<sos> It ' s me !  <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ0NLlyiYy-a"
      },
      "source": [
        "# <unk> --> <unk> x\r\n",
        "# Building tokenizers from keras.processing\r\n",
        "def tokenize(sent_list):\r\n",
        "  tokenizer = text.Tokenizer(filters='', oov_token='<unk>')\r\n",
        "  tokenizer.fit_on_texts(sent_list)\r\n",
        "\r\n",
        "  tensor_list = tokenizer.texts_to_sequences(sent_list)\r\n",
        "  tensor_list = sequence.pad_sequences(tensor_list, padding='post')\r\n",
        "  \r\n",
        "  return {'Tensors': tensor_list, 'Tokenizer': tokenizer} "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fWvfmqviaYG"
      },
      "source": [
        "marathi = tokenize(sent_list=mar_list)\r\n",
        "english = tokenize(sent_list=eng_list)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvatkmpznpnx",
        "outputId": "392fcb47-6f71-4c64-e50d-cabc7e68701d"
      },
      "source": [
        "mar_tokenizer = marathi['Tokenizer']\r\n",
        "eng_tokenizer = english['Tokenizer']\r\n",
        "\r\n",
        "print(f'The length of marathi vocab: {len(mar_tokenizer.word_index)}')\r\n",
        "print(f'The length of english vocab: {len(eng_tokenizer.word_index)}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of marathi vocab: 13842\n",
            "The length of english vocab: 5716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhYYsIzKnvY8",
        "outputId": "281d765f-c58d-409c-e99e-30ad6fe3685f"
      },
      "source": [
        "mar_tensors = marathi['Tensors']\r\n",
        "eng_tensors = english['Tensors']\r\n",
        "\r\n",
        "print(f'Max length of sequence: {len(mar_tensors[0])}')\r\n",
        "print(f'Max length of sequence: {len(eng_tensors[0])}')\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length of sequence: 44\n",
            "Max length of sequence: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V2vLbF8sXd6",
        "outputId": "8cc7dd21-5f7f-477a-90e0-50f012863b56"
      },
      "source": [
        "print(mar_tensors[0])\r\n",
        "print(eng_tensors[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  2 707   4   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0]\n",
            "[ 2 49  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SOsuFHSpZbA"
      },
      "source": [
        "### Implimenting TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM20hTegrjI3"
      },
      "source": [
        "from tensorflow.data import Dataset"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTSwVN5gsLY6"
      },
      "source": [
        "# Building TF Dataset --> Batching, Shuffling \r\n",
        "BUFFER_SIZE = len(mar_tensors)\r\n",
        "BATCH_SIZE = hype.BATCH_SIZE\r\n",
        "\r\n",
        "dataset = Dataset.from_tensor_slices((mar_tensors, eng_tensors)).shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\r\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDvbQm1DuduE",
        "outputId": "579e895e-6c70-4222-9480-5d4ae62ec126"
      },
      "source": [
        "ex_mar_batch, ex_eng_batch = next(iter(dataset))\r\n",
        "print(ex_mar_batch.shape)\r\n",
        "print(ex_eng_batch.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 44)\n",
            "(64, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C35L-D1utri"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LadoJbptvE4a"
      },
      "source": [
        "from tensorflow import nn\r\n",
        "from tensorflow.keras import layers, Model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7gTP9YY64sM"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMw78u1kvHu2"
      },
      "source": [
        "class Encoder(Model):\r\n",
        "  def __init__(self, vocab_size, embedding_size, enc_units, batch_size):\r\n",
        "    \r\n",
        "    super(Encoder, self).__init__()\r\n",
        "\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.enc_units = enc_units\r\n",
        "    \r\n",
        "    self.embedding = layers.Embedding(vocab_size, embedding_size)\r\n",
        "    self.gru = layers.GRU(enc_units, return_sequences= True, return_state= True, recurrent_initializer='glorot_uniform')\r\n",
        "    # Glorot_uniform --> Xavier Initialization. Xavier et al. --> uniform but some magnitude factor, helps model converge\r\n",
        "\r\n",
        "  def call(self, x, hidden):\r\n",
        "    # Shape of x: (batch_size,src_length)\r\n",
        "    x = self.embedding(x)\r\n",
        "    \r\n",
        "    #shape of x: (batch_size, src_length, embedding_size)\r\n",
        "    output, state = self.gru(x, initial_state=hidden)\r\n",
        "    \r\n",
        "    # shape of output: (batch_size, src_length, enc_units)\r\n",
        "    # shape of hidden: (batch_size, enc_units)\r\n",
        "\r\n",
        "    return output,state\r\n",
        "\r\n",
        "  def initialize_hidden_state(self):\r\n",
        "    return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKAKKKLC9a5l"
      },
      "source": [
        "mar_vocab_size = len(mar_tokenizer.word_index) + 1\r\n",
        "EMBEDDING_SIZE = model_hype.EMBEDDING_SIZE\r\n",
        "GRU_UNITS = model_hype.GRU_UNITS\r\n",
        "\r\n",
        "encoder = Encoder(mar_vocab_size, EMBEDDING_SIZE, GRU_UNITS, BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kugg8Jj1AgPV",
        "outputId": "4c73d01d-79e0-43a2-ff9c-5777cea9e255"
      },
      "source": [
        "#Example input\r\n",
        "\r\n",
        "ex_hidden = encoder.initialize_hidden_state()\r\n",
        "sample_out, sample_hidden = encoder(ex_mar_batch, ex_hidden)\r\n",
        "\r\n",
        "print(sample_out.shape)\r\n",
        "print(sample_hidden.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 44, 512)\n",
            "(64, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BwKYo4qBz0_"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqR_Q13dB5lf"
      },
      "source": [
        "class Attention(layers.Layer):\r\n",
        "  '''\r\n",
        "      Custom made Bahdanau Attention\r\n",
        "  '''\r\n",
        "  def __init__(self, units):\r\n",
        "    super(Attention, self).__init__()\r\n",
        "    self.W1 = layers.Dense(units)\r\n",
        "    self.W2 = layers.Dense(units)\r\n",
        "    self.V = layers.Dense(1)\r\n",
        "\r\n",
        "  def call(self, q, val):\r\n",
        "    #Adding the time axis to q\r\n",
        "    q_with_time_axis = tf.expand_dims(q, 1)\r\n",
        "    \r\n",
        "    # score = W * tanh(W*h_e + W*h_d)\r\n",
        "    score = self.V( nn.tanh( self.W1(q_with_time_axis) + self.W2(val) ) )\r\n",
        "\r\n",
        "    # getting attention weights \r\n",
        "    attention_w = nn.softmax(score, axis=1)\r\n",
        "\r\n",
        "    # creating context vec by multiplying with enc outputs\r\n",
        "    context_vec = attention_w * val\r\n",
        "    context_vec = tf.reduce_sum(context_vec, axis=1)\r\n",
        "\r\n",
        "    return context_vec, attention_w"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5mexiuZB6t5"
      },
      "source": [
        "ATTN_SIZE = 10\r\n",
        "attention = Attention(ATTN_SIZE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoyIpD_JHsn2",
        "outputId": "c1ec646b-02e7-4181-a8f9-8af7fda382ee"
      },
      "source": [
        "#example code\r\n",
        "attn_res ,  attn_w = attention(sample_hidden, sample_out)\r\n",
        "print(attn_res.shape)\r\n",
        "print(attn_w.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 512)\n",
            "(64, 44, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCor_QpNH2v2"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMpa-eiBIALo"
      },
      "source": [
        "class Decoder(Model):\r\n",
        "\r\n",
        "  def __init__(self, vocab_size, embedding_size, dec_units, batch_size):\r\n",
        "    super(Decoder,self).__init__()\r\n",
        "\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.dec_units = dec_units\r\n",
        "    \r\n",
        "    self.embedding = layers.Embedding(vocab_size, embedding_size)\r\n",
        "    self.gru = layers.GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\r\n",
        "\r\n",
        "    self.fc = layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    self.attention = Attention(dec_units)\r\n",
        "\r\n",
        "  def call(self, x, hidden, enc_out):\r\n",
        "    # Attention out to get context vectors\r\n",
        "    context_vec, attention_w = self.attention(hidden, enc_out)\r\n",
        "\r\n",
        "    # shape of x: (batch_size, trg_lenght)\r\n",
        "    x = self.embedding(x)\r\n",
        "\r\n",
        "    # adding context for info\r\n",
        "    x = tf.concat([tf.expand_dims(context_vec, 1), x], axis=-1)\r\n",
        "\r\n",
        "    # getting GRU\r\n",
        "    output, state = self.gru(x)\r\n",
        "\r\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\r\n",
        "\r\n",
        "    # Taking in random input and converting it to words --> max(vocab size) \r\n",
        "    x = self.fc(output)\r\n",
        "\r\n",
        "    return x, state, attention_w"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiZ7ouwZIBAV"
      },
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "decoder = Decoder(eng_vocab_size, EMBEDDING_SIZE, GRU_UNITS, BATCH_SIZE)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaQLmkzWVB9I",
        "outputId": "1a69ee19-64ad-485d-8348-b696816bc850"
      },
      "source": [
        "#example code\r\n",
        "\r\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),sample_hidden, sample_out)\r\n",
        "\r\n",
        "print (sample_decoder_output.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 5717)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwffpIeIVLqn"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5kGD2XaVs_I"
      },
      "source": [
        "### Optimizers and Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnh5QPXBVwW9"
      },
      "source": [
        "from tensorflow.keras import optimizers as optim\r\n",
        "from tensorflow.keras import losses"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD9cEv6fV_d7"
      },
      "source": [
        "optimizer = optim.Adam(learning_rate=hype.LR)\r\n",
        "criteria = losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  loss = criteria(real, pred)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\r\n",
        "  loss = loss*mask\r\n",
        "\r\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyA6rWvdU3qP",
        "outputId": "bea19f77-dd79-4362-d517-870ef3bc9621"
      },
      "source": [
        "optimizer.learning_rate"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRlvnjN2brFN"
      },
      "source": [
        "### Azure Blob set-up and loading\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-be280Axbu7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263e1c70-b5b7-44bf-a992-de963ba0bafa"
      },
      "source": [
        "!pip install azure-storage-blob"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting azure-storage-blob\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/00/6772472a99cd0a5e74e4e90f87947fa041b37981a3ff93d883cbc450518d/azure_storage_blob-12.7.1-py2.py3-none-any.whl (339kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 22.3MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/a1/49543f8ae3165c598e6c1393c54f9af8eaf7111f86e769ab4b897cdcf096/cryptography-3.4.4-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.6MB/s \n",
            "\u001b[?25hCollecting msrest>=0.6.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/cc/6c96bfb3d3cf4c3bdedfa6b46503223f4c2a4fa388377697e0f8082a4fed/msrest-0.6.21-py2.py3-none-any.whl (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.6MB/s \n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/2a/ad5f6bb3fcbb5c59087183041c82fe74d536204a03f9a662db825e1b3be4/azure_core-1.11.0-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 60.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.14.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob) (2020.12.5)\n",
            "Requirement already satisfied: requests~=2.16 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob) (1.3.0)\n",
            "Collecting isodate>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from azure-core<2.0.0,>=1.10.0->azure-storage-blob) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.18->azure-storage-blob) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.18->azure-storage-blob) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.18->azure-storage-blob) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob) (3.1.0)\n",
            "Installing collected packages: cryptography, isodate, msrest, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.11.0 azure-storage-blob-12.7.1 cryptography-3.4.4 isodate-0.6.0 msrest-0.6.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSo4NhBweZma",
        "outputId": "42562e14-b306-4c03-e211-823dbf10d983"
      },
      "source": [
        "import os, uuid\r\n",
        "from azure.storage import blob\r\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\r\n",
        "print(__version__)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vMdZ7xGDflY"
      },
      "source": [
        "if \"tf_checkpoint\" not in os.listdir():\r\n",
        "  os.mkdir(\"tf_checkpoint\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PDZyfMTb2OF"
      },
      "source": [
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=tfmodel;AccountKey=PinzJZWJy/mFOWDgkBcCTPA9Fnfr7/qvaZSbjxQVH4YGrBt4MseqbKYjUGNKYX9PpBh+zgAk6uDrVpmvejBCiw==;EndpointSuffix=core.windows.net\""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sc3KtIbeM05"
      },
      "source": [
        "blob_service_client =  BlobServiceClient.from_connection_string(connect_str)\r\n",
        "container_client = blob_service_client.get_container_client(\"tf-ckpt\")\r\n",
        "blob_list = [blob.name for blob in container_client.list_blobs()]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYKiqdOUmDLr"
      },
      "source": [
        "for file in blob_list:\r\n",
        "  blob_client = blob_service_client.get_blob_client('tf-ckpt', file)\r\n",
        "  with open('./tf_checkpoint/'+file, \"wb\") as f:\r\n",
        "    f.write(blob_client.download_blob().readall())"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD2K-vW6YVSX"
      },
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLZIwVbXYZri"
      },
      "source": [
        "checkpoint_path = './tf_checkpoint'\r\n",
        "checkpoint = tf.train.Checkpoint(epoch=tf.Variable(1),\r\n",
        "                                 optimizer = optimizer,\r\n",
        "                                 encoder=encoder,\r\n",
        "                                 decoder=decoder,\r\n",
        "                                 )\r\n",
        "manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=1)\r\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clFCC5V3Qk9I"
      },
      "source": [
        "#  manager.save()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7faMmu3U56K"
      },
      "source": [
        "# manager.checkpoints"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CWrLTO4JWLBk",
        "outputId": "1941af43-bcad-41b7-a0ea-14ff178f1270"
      },
      "source": [
        "manager.restore_or_initialize()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./tf_checkpoint/ckpt-17'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxTYP4xOZSEb"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtJK_B3xcks6"
      },
      "source": [
        "import time"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fFng4yJZVtN"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(src, trg, enc_hidden):\r\n",
        "  loss = 0\r\n",
        "\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    enc_out, enc_hidden = encoder(src, enc_hidden)\r\n",
        "\r\n",
        "    dec_hidden = enc_hidden\r\n",
        "\r\n",
        "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<sos>']]*BATCH_SIZE, 1)\r\n",
        "\r\n",
        "    for t in range(1, trg.shape[1]):\r\n",
        "\r\n",
        "      pred, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\r\n",
        "\r\n",
        "      loss += loss_function(trg[:,t], pred)\r\n",
        "\r\n",
        "      dec_input = tf.expand_dims(trg[:,t],1)\r\n",
        "\r\n",
        "  \r\n",
        "  batch_loss = loss/ int(trg.shape[1])\r\n",
        "\r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "\r\n",
        "  grads = tape.gradient(loss,variables)\r\n",
        "\r\n",
        "  optimizer.apply_gradients(zip(grads, variables))\r\n",
        "\r\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7JXc0-gxb_TV",
        "outputId": "86073389-dbb2-4b29-9577-f666304615af"
      },
      "source": [
        "EPOCHS = 50\r\n",
        "steps_per_epoch = len(mar_tensors)//BATCH_SIZE\r\n",
        "\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  enc_hidden = encoder.initialize_hidden_state()\r\n",
        "\r\n",
        "  total_loss = 0\r\n",
        "\r\n",
        "  for (batch, (src,trg)) in enumerate(dataset.take(steps_per_epoch)):\r\n",
        "\r\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\r\n",
        "\r\n",
        "    total_loss += batch_loss\r\n",
        "\r\n",
        "    if batch%100 == 0:\r\n",
        "      print(f'Epoch {epoch} Batch {batch} Loss{batch_loss.numpy():.4f}')\r\n",
        "\r\n",
        "  \r\n",
        "  if (epoch+1)%2 == 0:\r\n",
        "    manager.save()\r\n",
        "\r\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\r\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 0 Loss0.0049\n",
            "Epoch 0 Batch 100 Loss0.0050\n",
            "Epoch 0 Batch 200 Loss0.0048\n",
            "Epoch 0 Batch 300 Loss0.0062\n",
            "Epoch 0 Batch 400 Loss0.0039\n",
            "Epoch 0 Batch 500 Loss0.0018\n",
            "Epoch 0 Batch 600 Loss0.0035\n",
            "Epoch 1 Loss 0.0053\n",
            "Time taken for 1 epoch 170.73022413253784 sec\n",
            "\n",
            "Epoch 1 Batch 0 Loss0.0020\n",
            "Epoch 1 Batch 100 Loss0.0029\n",
            "Epoch 1 Batch 200 Loss0.0038\n",
            "Epoch 1 Batch 300 Loss0.0036\n",
            "Epoch 1 Batch 400 Loss0.0029\n",
            "Epoch 1 Batch 500 Loss0.0013\n",
            "Epoch 1 Batch 600 Loss0.0038\n",
            "Epoch 2 Loss 0.0028\n",
            "Time taken for 1 epoch 121.86319851875305 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss0.0016\n",
            "Epoch 2 Batch 100 Loss0.0017\n",
            "Epoch 2 Batch 200 Loss0.0052\n",
            "Epoch 2 Batch 300 Loss0.0023\n",
            "Epoch 2 Batch 400 Loss0.0029\n",
            "Epoch 2 Batch 500 Loss0.0044\n",
            "Epoch 2 Batch 600 Loss0.0021\n",
            "Epoch 3 Loss 0.0022\n",
            "Time taken for 1 epoch 121.60438752174377 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss0.0016\n",
            "Epoch 3 Batch 100 Loss0.0019\n",
            "Epoch 3 Batch 200 Loss0.0034\n",
            "Epoch 3 Batch 300 Loss0.0014\n",
            "Epoch 3 Batch 400 Loss0.0014\n",
            "Epoch 3 Batch 500 Loss0.0008\n",
            "Epoch 3 Batch 600 Loss0.0013\n",
            "Epoch 4 Loss 0.0019\n",
            "Time taken for 1 epoch 121.96021747589111 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss0.0015\n",
            "Epoch 4 Batch 100 Loss0.0015\n",
            "Epoch 4 Batch 200 Loss0.0017\n",
            "Epoch 4 Batch 300 Loss0.0014\n",
            "Epoch 4 Batch 400 Loss0.0018\n",
            "Epoch 4 Batch 500 Loss0.0007\n",
            "Epoch 4 Batch 600 Loss0.0026\n",
            "Epoch 5 Loss 0.0017\n",
            "Time taken for 1 epoch 121.90962409973145 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss0.0012\n",
            "Epoch 5 Batch 100 Loss0.0020\n",
            "Epoch 5 Batch 200 Loss0.0031\n",
            "Epoch 5 Batch 300 Loss0.0007\n",
            "Epoch 5 Batch 400 Loss0.0026\n",
            "Epoch 5 Batch 500 Loss0.0015\n",
            "Epoch 5 Batch 600 Loss0.0008\n",
            "Epoch 6 Loss 0.0017\n",
            "Time taken for 1 epoch 121.85828590393066 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss0.0015\n",
            "Epoch 6 Batch 100 Loss0.0027\n",
            "Epoch 6 Batch 200 Loss0.0004\n",
            "Epoch 6 Batch 300 Loss0.0030\n",
            "Epoch 6 Batch 400 Loss0.0004\n",
            "Epoch 6 Batch 500 Loss0.0026\n",
            "Epoch 6 Batch 600 Loss0.0031\n",
            "Epoch 7 Loss 0.0016\n",
            "Time taken for 1 epoch 122.13868570327759 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss0.0010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-5dc85fd6dc0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mOCVNMNqC9Km",
        "outputId": "30c35742-6294-4a32-f6f7-fc58bc7373eb"
      },
      "source": [
        "manager.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./tf_checkpoint/ckpt-17'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyUwjTBBDYUz"
      },
      "source": [
        "### Saving in blob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Snhyf3pmiR"
      },
      "source": [
        "**Caution:** Only change this in case you wish to permanently change the model file. Do not change this otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxkjZlsTi_Ks",
        "outputId": "75b0e3e3-0538-4882-8bbe-4c162f75cec7"
      },
      "source": [
        "blob_list = [blob.name for blob in container_client.list_blobs()]\r\n",
        "blob_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['checkpoint', 'ckpt-17.data-00000-of-00001', 'ckpt-17.index']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7YFVlchDcDG"
      },
      "source": [
        "# clear the blob\r\n",
        "for file_name in blob_list:\r\n",
        "  container_client.delete_blob(blob=file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukrq_eQNEVfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1c4431-7e14-41e5-c3a3-51ee4c7b8454"
      },
      "source": [
        "# getting the file names\r\n",
        "files = os.listdir('./tf_checkpoint')\r\n",
        "files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['checkpoint', 'ckpt-17.data-00000-of-00001', 'ckpt-17.index']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FegNMHjxEjGf"
      },
      "source": [
        "# uploading the files\r\n",
        "for file in files:\r\n",
        "  blob_client = container_client.get_blob_client(file)\r\n",
        "  with open(\"./tf_checkpoint/\" + file,\"rb\") as data:\r\n",
        "    blob_client.upload_blob(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0irQlukDUrU"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IBuyk-Dekcx"
      },
      "source": [
        "def evaluate(sentence):\r\n",
        "  attention_plot = np.zeros((eng_tensors.shape[1],mar_tensors.shape[1]))\r\n",
        "\r\n",
        "  sentence = sep_punk(sentence)\r\n",
        "\r\n",
        "  inputs = []\r\n",
        "  for i in sentence.lower().split(' '):\r\n",
        "    if i != '' :\r\n",
        "      if (i in mar_tokenizer.word_docs.keys()):\r\n",
        "        inputs.append(mar_tokenizer.word_index[i])\r\n",
        "      else: inputs.append(mar_tokenizer.word_index['<unk>'])\r\n",
        "\r\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n",
        "                                                         maxlen=mar_tensors.shape[1],\r\n",
        "                                                         padding='post')\r\n",
        "  inputs = tf.convert_to_tensor(inputs)\r\n",
        "\r\n",
        "  result = []\r\n",
        "\r\n",
        "  hidden = [tf.zeros((1, GRU_UNITS))]\r\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\r\n",
        "\r\n",
        "  dec_hidden = enc_hidden\r\n",
        "  dec_input = tf.expand_dims([eng_tokenizer.word_index['<sos>']], 0)\r\n",
        "\r\n",
        "  for t in range(eng_tensors.shape[1]):\r\n",
        "    predictions, dec_hidden, _ = decoder(dec_input,  dec_hidden, enc_out)\r\n",
        "\r\n",
        "    # storing the attention weights to plot later on\r\n",
        "    # attention_weights = tf.reshape(attention_weights, (-1, ))\r\n",
        "    # attention_plot[t] = attention_weights.numpy()\r\n",
        "\r\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\r\n",
        "\r\n",
        "    result.append(eng_tokenizer.index_word[predicted_id])\r\n",
        "\r\n",
        "    if eng_tokenizer.index_word[predicted_id] == '<eos>':\r\n",
        "      return result, sentence\r\n",
        "\r\n",
        "    # the predicted ID is fed back into the model\r\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "  return result, sentence"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbwQr01qsZI-",
        "outputId": "f65b0ed2-128f-4228-cfda-393a4f809987"
      },
      "source": [
        "start = time.time()\r\n",
        "res, sentence  = evaluate(data['marathi'][40749])\r\n",
        "end  = time.time()\r\n",
        "\r\n",
        "print(end-start)\r\n",
        "print(sentence)\r\n",
        "print(data['english'][40749])\r\n",
        "print(res)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.28397536277770996\n",
            "आयपॅडवर फ्लॅश आशय बघता येत नाही .  पण तुम्ही त्या वेब पानांचे यूआरएल स्वतःला ईमेल करून तोच आशय घरी पोहोचल्यावर आपल्या रोजच्या संगणकावर पाहू शकता . \n",
            "You can't view Flash content on an iPad. However, you can easily email yourself the URLs of these web pages and view that content on your regular computer when you get home.\n",
            "['you', 'can', \"'\", 't', 'view', 'flash', 'content', 'on', 'an', 'ipad', '.', 'however', ',', 'you', 'can', 'easily', 'email', 'yourself', 'the', 'urls', 'of', 'these', 'web', 'pages', 'and', 'view', 'that', 'content', 'on', 'your', 'regular', 'computer', 'when', 'you', 'get', 'home', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM5HtwZa5rpG",
        "outputId": "c52d1be2-ec1c-4af5-ff38-9fa1d5fa08d8"
      },
      "source": [
        "res, sentence = evaluate(\"तुझं नाव काय आहे?\")\r\n",
        "\r\n",
        "print(res)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['what', \"'\", 's', 'your', 'name', '?', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvmEypz-Ul-m",
        "outputId": "6afe1848-ad86-4970-a893-cfc4a6d95b68"
      },
      "source": [
        "if 'मराठी' in mar_tokenizer.word_docs.keys():\r\n",
        "  print(\"True\")\r\n",
        "else: print(\"False\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoyusI6ZQuFx"
      },
      "source": [
        "# BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW451gOXxAzQ"
      },
      "source": [
        "import torchtext\r\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwg9YX5dHXge"
      },
      "source": [
        "from tqdm.notebook import trange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS3GjYiXS2I6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a5669bd3563f4a249e8b0c61970e6373",
            "19b95e37990b4f6784e89872e5504093",
            "4a3a2cc4fa1e41c2b65e1487b59533c3",
            "65769da98a59446fb39ea88c25ed735a",
            "e97cc506abb84c0ea8acbad031cc47a1",
            "94a8d44dff4e48c7bcecf15b5984104f",
            "de92aaaedeb848719abc00718b0bb91f",
            "3c85fe942a224a2d926c49e6ebb8164c"
          ]
        },
        "outputId": "9eae6889-d51d-4ef9-ac27-24b67083e4bf"
      },
      "source": [
        "trgs = []\r\n",
        "preds = []\r\n",
        "\r\n",
        "for i in trange(len(data)):\r\n",
        "  \r\n",
        "  src = data['marathi'][i]\r\n",
        "  trg = data['english'][i]\r\n",
        "\r\n",
        "  trg = [tok.lower() for tok in sep_punk(trg).split(\" \") if tok!='']\r\n",
        "\r\n",
        "  pred, _ = evaluate(src)\r\n",
        "\r\n",
        "  #preds.append(trg)\r\n",
        "  preds.append(pred[:-1])\r\n",
        "  trgs.append([trg])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5669bd3563f4a249e8b0c61970e6373",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=40751.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjIhGoMiIh7p",
        "outputId": "275971fd-94c8-432a-8c22-e4fabe827b0e"
      },
      "source": [
        "trgs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['go', '.']],\n",
              " [['run', '!']],\n",
              " [['run', '!']],\n",
              " [['run', '!']],\n",
              " [['run', '!']],\n",
              " [['who', '?']],\n",
              " [['wow', '!']],\n",
              " [['fire', '!']],\n",
              " [['fire', '!']],\n",
              " [['help', '!']],\n",
              " [['help', '!']],\n",
              " [['jump', '!']],\n",
              " [['jump', '!']],\n",
              " [['jump', '.']],\n",
              " [['jump', '.']],\n",
              " [['stop', '!']],\n",
              " [['stop', '!']],\n",
              " [['wait', '!']],\n",
              " [['wait', '!']],\n",
              " [['hello', '!']],\n",
              " [['hurry', '!']],\n",
              " [['hurry', '!']],\n",
              " [['hurry', '!']],\n",
              " [['i', 'won', '!']],\n",
              " [['i', 'won', '!']],\n",
              " [['get', 'up', '.']],\n",
              " [['got', 'it', '!']],\n",
              " [['got', 'it', '?']],\n",
              " [['got', 'it', '?']],\n",
              " [['got', 'it', '?']],\n",
              " [['got', 'it', '?']],\n",
              " [['he', 'ran', '.']],\n",
              " [['he', 'ran', '.']],\n",
              " [['he', 'ran', '.']],\n",
              " [['he', 'ran', '.']],\n",
              " [['i', 'fell', '.']],\n",
              " [['i', 'fell', '.']],\n",
              " [['i', 'fell', '.']],\n",
              " [['i', 'fell', '.']],\n",
              " [['i', 'know', '.']],\n",
              " [['i', 'know', '.']],\n",
              " [['i', 'know', '.']],\n",
              " [['i', 'lost', '.']],\n",
              " [['i', 'lost', '.']],\n",
              " [['i', 'work', '.']],\n",
              " [['i', 'work', '.']],\n",
              " [['i', \"'\", 'm', 'ok', '.']],\n",
              " [['listen', '.']],\n",
              " [['listen', '.']],\n",
              " [['no', 'way', '!']],\n",
              " [['really', '?']],\n",
              " [['really', '?']],\n",
              " [['really', '?']],\n",
              " [['thanks', '.']],\n",
              " [['we', 'won', '.']],\n",
              " [['we', 'won', '.']],\n",
              " [['why', 'me', '?']],\n",
              " [['why', 'me', '?']],\n",
              " [['ask', 'tom', '.']],\n",
              " [['ask', 'tom', '.']],\n",
              " [['call', 'me', '.']],\n",
              " [['call', 'me', '.']],\n",
              " [['call', 'me', '.']],\n",
              " [['call', 'me', '.']],\n",
              " [['call', 'us', '.']],\n",
              " [['call', 'us', '.']],\n",
              " [['come', 'in', '.']],\n",
              " [['come', 'on', '!']],\n",
              " [['come', 'on', '!']],\n",
              " [['fold', 'it', '.']],\n",
              " [['fold', 'it', '.']],\n",
              " [['get', 'tom', '.']],\n",
              " [['get', 'tom', '.']],\n",
              " [['get', 'tom', '.']],\n",
              " [['get', 'out', '.']],\n",
              " [['get', 'out', '.']],\n",
              " [['go', 'home', '.']],\n",
              " [['he', 'came', '.']],\n",
              " [['he', 'came', '.']],\n",
              " [['he', 'left', '.']],\n",
              " [['he', 'left', '.']],\n",
              " [['he', 'runs', '.']],\n",
              " [['he', 'runs', '.']],\n",
              " [['he', 'runs', '.']],\n",
              " [['he', 'runs', '.']],\n",
              " [['help', 'me', '!']],\n",
              " [['help', 'me', '!']],\n",
              " [['help', 'me', '!']],\n",
              " [['help', 'me', '.']],\n",
              " [['help', 'me', '.']],\n",
              " [['help', 'us', '.']],\n",
              " [['help', 'us', '.']],\n",
              " [['help', 'us', '.']],\n",
              " [['i', \"'\", 'm', 'tom', '.']],\n",
              " [['i', \"'\", 'm', 'tom', '.']],\n",
              " [['i', \"'\", 'm', 'tom', '.']],\n",
              " [['i', \"'\", 'm', 'fat', '.']],\n",
              " [['i', \"'\", 'm', 'fat', '.']],\n",
              " [['i', \"'\", 'm', 'ill', '.']],\n",
              " [['it', \"'\", 's', 'ok', '.']],\n",
              " [['it', \"'\", 's', 'me', '!']],\n",
              " [['it', \"'\", 's', 'me', '.']],\n",
              " [['me', ',', 'too', '.']],\n",
              " [['me', ',', 'too', '.']],\n",
              " [['open', 'up', '.']],\n",
              " [['open', 'up', '.']],\n",
              " [['perfect', '!']],\n",
              " [['show', 'me', '.']],\n",
              " [['show', 'me', '.']],\n",
              " [['shut', 'up', '!']],\n",
              " [['shut', 'up', '!']],\n",
              " [['shut', 'up', '!']],\n",
              " [['tell', 'me', '.']],\n",
              " [['tell', 'me', '.']],\n",
              " [['tom', 'ran', '.']],\n",
              " [['tom', 'ran', '.']],\n",
              " [['tom', 'won', '.']],\n",
              " [['wake', 'up', '!']],\n",
              " [['wake', 'up', '!']],\n",
              " [['wake', 'up', '!']],\n",
              " [['we', 'care', '.']],\n",
              " [['we', 'care', '.']],\n",
              " [['we', 'know', '.']],\n",
              " [['we', 'know', '.']],\n",
              " [['we', 'lost', '.']],\n",
              " [['we', 'lost', '.']],\n",
              " [['welcome', '.']],\n",
              " [['welcome', '.']],\n",
              " [['welcome', '.']],\n",
              " [['welcome', '.']],\n",
              " [['who', 'ate', '?']],\n",
              " [['who', 'ran', '?']],\n",
              " [['who', 'ran', '?']],\n",
              " [['who', 'won', '?']],\n",
              " [['why', 'not', '?']],\n",
              " [['you', 'won', '.']],\n",
              " [['you', 'won', '.']],\n",
              " [['you', 'won', '.']],\n",
              " [['back', 'off', '!']],\n",
              " [['be', 'quiet', '.']],\n",
              " [['be', 'quiet', '.']],\n",
              " [['beats', 'me', '.']],\n",
              " [['beats', 'me', '.']],\n",
              " [['call', 'tom', '.']],\n",
              " [['call', 'tom', '.']],\n",
              " [['call', 'tom', '.']],\n",
              " [['call', 'tom', '.']],\n",
              " [['get', 'down', '.']],\n",
              " [['grab', 'tom', '.']],\n",
              " [['grab', 'tom', '.']],\n",
              " [['grab', 'him', '.']],\n",
              " [['have', 'fun', '.']],\n",
              " [['he', 'spoke', '.']],\n",
              " [['he', 'spoke', '.']],\n",
              " [['i', 'can', 'go', '.']],\n",
              " [['i', 'can', 'go', '.']],\n",
              " [['i', 'forgot', '.']],\n",
              " [['i', 'forgot', '.']],\n",
              " [['i', 'got', 'it', '.']],\n",
              " [['i', 'got', 'it', '.']],\n",
              " [['i', 'looked', '.']],\n",
              " [['i', 'looked', '.']],\n",
              " [['i', 'phoned', '.']],\n",
              " [['i', 'prayed', '.']],\n",
              " [['i', 'shaved', '.']],\n",
              " [['i', 'use', 'it', '.']],\n",
              " [['i', 'use', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'try', '.']],\n",
              " [['i', \"'\", 'm', 'full', '.']],\n",
              " [['i', \"'\", 'm', 'game', '.']],\n",
              " [['i', \"'\", 'm', 'late', '.']],\n",
              " [['i', \"'\", 'm', 'lazy', '.']],\n",
              " [['i', \"'\", 'm', 'poor', '.']],\n",
              " [['i', \"'\", 've', 'won', '.']],\n",
              " [['i', \"'\", 've', 'won', '.']],\n",
              " [['it', \"'\", 's', 'hot', '.']],\n",
              " [['it', \"'\", 's', 'new', '.']],\n",
              " [['it', \"'\", 's', 'new', '.']],\n",
              " [['it', \"'\", 's', 'old', '.']],\n",
              " [['it', \"'\", 's', 'old', '.']],\n",
              " [['it', \"'\", 's', 'old', '.']],\n",
              " [['kiss', 'tom', '.']],\n",
              " [['leave', 'it', '.']],\n",
              " [['leave', 'it', '.']],\n",
              " [['leave', 'it', '.']],\n",
              " [['leave', 'it', '.']],\n",
              " [['leave', 'me', '.']],\n",
              " [['leave', 'me', '.']],\n",
              " [['leave', 'me', '.']],\n",
              " [['look', 'out', '!']],\n",
              " [['marry', 'me', '.']],\n",
              " [['marry', 'me', '.']],\n",
              " [['she', 'came', '.']],\n",
              " [['she', 'came', '.']],\n",
              " [['she', 'died', '.']],\n",
              " [['she', 'died', '.']],\n",
              " [['she', 'died', '.']],\n",
              " [['she', 'died', '.']],\n",
              " [['sit', 'down', '!']],\n",
              " [['sit', 'down', '!']],\n",
              " [['sit', 'here', '.']],\n",
              " [['sit', 'here', '.']],\n",
              " [['stand', 'up', '!']],\n",
              " [['stand', 'up', '!']],\n",
              " [['stand', 'up', '!']],\n",
              " [['stop', 'tom', '.']],\n",
              " [['stop', 'tom', '.']],\n",
              " [['terrific', '!']],\n",
              " [['they', 'won', '.']],\n",
              " [['they', 'won', '.']],\n",
              " [['tom', 'came', '.']],\n",
              " [['tom', 'died', '.']],\n",
              " [['tom', 'fell', '.']],\n",
              " [['tom', 'knew', '.']],\n",
              " [['tom', 'left', '.']],\n",
              " [['tom', 'left', '.']],\n",
              " [['tom', 'left', '.']],\n",
              " [['tom', 'lied', '.']],\n",
              " [['tom', 'lies', '.']],\n",
              " [['tom', 'lost', '.']],\n",
              " [['tom', 'wept', '.']],\n",
              " [['tom', \"'\", 's', 'up', '.']],\n",
              " [['use', 'this', '.']],\n",
              " [['use', 'this', '.']],\n",
              " [['warn', 'tom', '.']],\n",
              " [['warn', 'tom', '.']],\n",
              " [['watch', 'me', '.']],\n",
              " [['watch', 'me', '.']],\n",
              " [['watch', 'us', '.']],\n",
              " [['watch', 'us', '.']],\n",
              " [['we', \"'\", 'll', 'go', '.']],\n",
              " [['we', \"'\", 'll', 'go', '.']],\n",
              " [['what', 'for', '?']],\n",
              " [['what', 'for', '?']],\n",
              " [['who', 'am', 'i', '?']],\n",
              " [['who', 'came', '?']],\n",
              " [['who', 'died', '?']],\n",
              " [['who', 'fell', '?']],\n",
              " [['who', \"'\", 's', 'he', '?']],\n",
              " [['who', \"'\", 's', 'he', '?']],\n",
              " [['answer', 'me', '.']],\n",
              " [['answer', 'me', '.']],\n",
              " [['birds', 'fly', '.']],\n",
              " [['calm', 'down', '!']],\n",
              " [['catch', 'him', '.']],\n",
              " [['catch', 'him', '.']],\n",
              " [['catch', 'him', '.']],\n",
              " [['catch', 'him', '.']],\n",
              " [['come', 'here', '.']],\n",
              " [['come', 'here', '.']],\n",
              " [['come', 'home', '.']],\n",
              " [['come', 'home', '.']],\n",
              " [['did', 'i', 'win', '?']],\n",
              " [['did', 'i', 'win', '?']],\n",
              " [['dogs', 'bark', '.']],\n",
              " [['don', \"'\", 't', 'ask', '.']],\n",
              " [['don', \"'\", 't', 'ask', '.']],\n",
              " [['don', \"'\", 't', 'cry', '.']],\n",
              " [['don', \"'\", 't', 'cry', '.']],\n",
              " [['don', \"'\", 't', 'die', '.']],\n",
              " [['don', \"'\", 't', 'die', '.']],\n",
              " [['don', \"'\", 't', 'lie', '.']],\n",
              " [['don', \"'\", 't', 'lie', '.']],\n",
              " [['don', \"'\", 't', 'run', '.']],\n",
              " [['don', \"'\", 't', 'run', '.']],\n",
              " [['don', \"'\", 't', 'run', '.']],\n",
              " [['don', \"'\", 't', 'run', '.']],\n",
              " [['forget', 'it', '.']],\n",
              " [['forget', 'it', '.']],\n",
              " [['forget', 'me', '.']],\n",
              " [['get', 'ready', '.']],\n",
              " [['go', 'inside', '.']],\n",
              " [['grab', 'that', '.']],\n",
              " [['grab', 'that', '.']],\n",
              " [['grab', 'that', '.']],\n",
              " [['grab', 'that', '.']],\n",
              " [['grab', 'this', '.']],\n",
              " [['grab', 'this', '.']],\n",
              " [['he', 'is', 'ill', '.']],\n",
              " [['he', 'is', 'ill', '.']],\n",
              " [['he', \"'\", 's', 'a', 'dj', '.']],\n",
              " [['he', \"'\", 's', 'a', 'dj', '.']],\n",
              " [['he', \"'\", 's', 'mine', '.']],\n",
              " [['he', \"'\", 's', 'mine', '.']],\n",
              " [['he', \"'\", 's', 'sexy', '.']],\n",
              " [['he', \"'\", 's', 'sexy', '.']],\n",
              " [['hold', 'this', '.']],\n",
              " [['hold', 'this', '.']],\n",
              " [['how', 'is', 'it', '?']],\n",
              " [['how', 'is', 'it', '?']],\n",
              " [['how', 'is', 'it', '?']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'run', '.']],\n",
              " [['i', 'can', 'ski', '.']],\n",
              " [['i', 'can', 'ski', '.']],\n",
              " [['i', 'can', 'ski', '.']],\n",
              " [['i', 'can', 'win', '.']],\n",
              " [['i', 'can', 'win', '.']],\n",
              " [['i', 'fainted', '.']],\n",
              " [['i', 'got', 'fat', '.']],\n",
              " [['i', 'got', 'fat', '.']],\n",
              " [['i', 'hit', 'tom', '.']],\n",
              " [['i', 'hit', 'tom', '.']],\n",
              " [['i', 'hit', 'tom', '.']],\n",
              " [['i', 'laughed', '.']],\n",
              " [['i', 'laughed', '.']],\n",
              " [['i', 'met', 'him', '.']],\n",
              " [['i', 'met', 'him', '.']],\n",
              " [['i', 'saw', 'tom', '.']],\n",
              " [['i', 'saw', 'tom', '.']],\n",
              " [['i', 'saw', 'tom', '.']],\n",
              " [['i', 'saw', 'you', '.']],\n",
              " [['i', 'saw', 'you', '.']],\n",
              " [['i', 'saw', 'you', '.']],\n",
              " [['i', 'saw', 'you', '.']],\n",
              " [['i', 'shouted', '.']],\n",
              " [['i', 'shouted', '.']],\n",
              " [['i', 'want', 'it', '.']],\n",
              " [['i', 'want', 'it', '.']],\n",
              " [['i', 'want', 'it', '.']],\n",
              " [['i', 'want', 'it', '.']],\n",
              " [['i', 'will', 'go', '.']],\n",
              " [['i', \"'\", 'll', 'call', '.']],\n",
              " [['i', \"'\", 'll', 'sing', '.']],\n",
              " [['i', \"'\", 'll', 'stop', '.']],\n",
              " [['i', \"'\", 'll', 'talk', '.']],\n",
              " [['i', \"'\", 'll', 'walk', '.']],\n",
              " [['i', \"'\", 'll', 'work', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'man', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'man', '.']],\n",
              " [['i', \"'\", 'm', 'awake', '.']],\n",
              " [['i', \"'\", 'm', 'awake', '.']],\n",
              " [['i', \"'\", 'm', 'bored', '.']],\n",
              " [['i', \"'\", 'm', 'clean', '.']],\n",
              " [['i', \"'\", 'm', 'dying', '.']],\n",
              " [['i', \"'\", 'm', 'dying', '.']],\n",
              " [['i', \"'\", 'm', 'going', '.']],\n",
              " [['i', \"'\", 'm', 'going', '.']],\n",
              " [['i', \"'\", 'm', 'quiet', '.']],\n",
              " [['i', \"'\", 'm', 'right', '.']],\n",
              " [['i', \"'\", 'm', 'young', '.']],\n",
              " [['it', 'burned', '.']],\n",
              " [['it', 'burned', '.']],\n",
              " [['it', 'burned', '.']],\n",
              " [['it', 'rained', '.']],\n",
              " [['it', 'snowed', '.']],\n",
              " [['it', \"'\", 's', '7', ':', '45', '.']],\n",
              " [['it', \"'\", 's', 'cold', '.']],\n",
              " [['it', \"'\", 's', 'easy', '.']],\n",
              " [['it', \"'\", 's', 'food', '.']],\n",
              " [['it', \"'\", 's', 'good', '.']],\n",
              " [['it', \"'\", 's', 'here', '.']],\n",
              " [['it', \"'\", 's', 'here', '.']],\n",
              " [['it', \"'\", 's', 'mine', '.']],\n",
              " [['it', \"'\", 's', 'ours', '.']],\n",
              " [['it', \"'\", 's', 'ours', '.']],\n",
              " [['it', \"'\", 's', 'sand', '.']],\n",
              " [['it', \"'\", 's', 'time', '.']],\n",
              " [['it', \"'\", 's', 'work', '.']],\n",
              " [['keep', 'them', '.']],\n",
              " [['keep', 'them', '.']],\n",
              " [['leave', 'now', '.']],\n",
              " [['leave', 'now', '.']],\n",
              " [['let', 'me', 'go', '!']],\n",
              " [['let', 'me', 'go', '!']],\n",
              " [['let', 'me', 'go', '.']],\n",
              " [['let', 'me', 'go', '.']],\n",
              " [['let', 'me', 'in', '.']],\n",
              " [['let', 'me', 'in', '.']],\n",
              " [['let', 'us', 'go', '.']],\n",
              " [['let', 'us', 'go', '.']],\n",
              " [['let', 'us', 'go', '.']],\n",
              " [['let', 'us', 'go', '.']],\n",
              " [['let', 'us', 'in', '.']],\n",
              " [['let', 'us', 'in', '.']],\n",
              " [['let', \"'\", 's', 'ask', '.']],\n",
              " [['look', 'back', '!']],\n",
              " [['look', 'back', '!']],\n",
              " [['look', 'back', '.']],\n",
              " [['look', 'back', '.']],\n",
              " [['read', 'this', '.']],\n",
              " [['read', 'this', '.']],\n",
              " [['search', 'me', '.']],\n",
              " [['search', 'me', '.']],\n",
              " [['see', 'above', '.']],\n",
              " [['see', 'below', '.']],\n",
              " [['she', 'cried', '.']],\n",
              " [['she', 'cried', '.']],\n",
              " [['she', 'tried', '.']],\n",
              " [['she', 'walks', '.']],\n",
              " [['she', 'walks', '.']],\n",
              " [['sign', 'here', '.']],\n",
              " [['sign', 'here', '.']],\n",
              " [['sit', 'there', '.']],\n",
              " [['sit', 'there', '.']],\n",
              " [['start', 'now', '.']],\n",
              " [['start', 'now', '.']],\n",
              " [['stay', 'away', '.']],\n",
              " [['stay', 'back', '.']],\n",
              " [['stay', 'home', '.']],\n",
              " [['stay', 'thin', '.']],\n",
              " [['stop', 'them', '.']],\n",
              " [['stop', 'them', '.']],\n",
              " [['take', 'care', '!']],\n",
              " [['take', 'care', '!']],\n",
              " [['take', 'care', '.']],\n",
              " [['take', 'care', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'mine', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['take', 'this', '.']],\n",
              " [['thank', 'you', '.']],\n",
              " [['that', \"'\", 's', 'it', '.']],\n",
              " [['that', \"'\", 's', 'it', '.']],\n",
              " [['they', 'fell', '.']],\n",
              " [['they', 'fell', '.']],\n",
              " [['they', 'left', '.']],\n",
              " [['they', 'left', '.']],\n",
              " [['they', 'lied', '.']],\n",
              " [['they', 'lied', '.']],\n",
              " [['they', 'lost', '.']],\n",
              " [['they', 'lost', '.']],\n",
              " [['tom', 'cried', '.']],\n",
              " [['tom', 'drank', '.']],\n",
              " [['tom', 'knits', '.']],\n",
              " [['tom', 'knows', '.']],\n",
              " [['tom', 'moved', '.']],\n",
              " [['tom', 'tried', '.']],\n",
              " [['tom', 'tried', '.']],\n",
              " [['tom', 'tries', '.']],\n",
              " [['tom', 'voted', '.']],\n",
              " [['tom', 'voted', '.']],\n",
              " [['tom', 'walks', '.']],\n",
              " [['tom', 'works', '.']],\n",
              " [['try', 'again', '.']],\n",
              " [['try', 'again', '.']],\n",
              " [['try', 'it', 'on', '.']],\n",
              " [['try', 'it', 'on', '.']],\n",
              " [['wait', 'here', '.']],\n",
              " [['wait', 'here', '.']],\n",
              " [['watch', 'tom', '.']],\n",
              " [['watch', 'tom', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'saw', 'it', '.']],\n",
              " [['we', 'talked', '.']],\n",
              " [['we', 'waited', '.']],\n",
              " [['we', 'waited', '.']],\n",
              " [['we', 'waited', '.']],\n",
              " [['we', 'waited', '.']],\n",
              " [['we', \"'\", 're', 'shy', '.']],\n",
              " [['we', \"'\", 're', 'shy', '.']],\n",
              " [['well', 'done', '!']],\n",
              " [['what', \"'\", 's', 'up', '?']],\n",
              " [['what', \"'\", 's', 'up', '?']],\n",
              " [['what', \"'\", 's', 'up', '?']],\n",
              " [['who', 'is', 'he', '?']],\n",
              " [['who', 'is', 'he', '?']],\n",
              " [['who', 'is', 'it', '?']],\n",
              " [['who', 'knows', '?']],\n",
              " [['who', 'knows', '?']],\n",
              " [['who', \"'\", 'll', 'go', '?']],\n",
              " [['who', \"'\", 's', 'tom', '?']],\n",
              " [['who', \"'\", 's', 'she', '?']],\n",
              " [['who', \"'\", 's', 'she', '?']],\n",
              " [['who', \"'\", 's', 'she', '?']],\n",
              " [['who', \"'\", 's', 'she', '?']],\n",
              " [['you', 'drive', '.']],\n",
              " [['you', 'drive', '.']],\n",
              " [['you', 'idiot', '!']],\n",
              " [['you', 'tried', '.']],\n",
              " [['you', 'tried', '.']],\n",
              " [['are', 'you', 'ok', '?']],\n",
              " [['are', 'you', 'ok', '?']],\n",
              " [['are', 'you', 'ok', '?']],\n",
              " [['ask', 'anyone', '.']],\n",
              " [['be', 'careful', '!']],\n",
              " [['be', 'on', 'time', '.']],\n",
              " [['be', 'on', 'time', '.']],\n",
              " [['birds', 'sing', '.']],\n",
              " [['bring', 'wine', '.']],\n",
              " [['bring', 'wine', '.']],\n",
              " [['come', 'again', '.']],\n",
              " [['come', 'again', '.']],\n",
              " [['come', 'on', 'in', '.']],\n",
              " [['come', 'quick', '!']],\n",
              " [['come', 'quick', '!']],\n",
              " [['definitely', '!']],\n",
              " [['do', 'men', 'cry', '?']],\n",
              " [['do', 'men', 'cry', '?']],\n",
              " [['don', \"'\", 't', 'look', '.']],\n",
              " [['don', \"'\", 't', 'look', '.']],\n",
              " [['don', \"'\", 't', 'move', '.']],\n",
              " [['don', \"'\", 't', 'move', '.']],\n",
              " [['don', \"'\", 't', 'talk', '!']],\n",
              " [['don', \"'\", 't', 'talk', '!']],\n",
              " [['don', \"'\", 't', 'talk', '.']],\n",
              " [['don', \"'\", 't', 'talk', '.']],\n",
              " [['fill', 'it', 'up', '.']],\n",
              " [['fire', 'burns', '.']],\n",
              " [['fire', 'burns', '.']],\n",
              " [['forget', 'tom', '.']],\n",
              " [['forget', 'tom', '.']],\n",
              " [['forget', 'tom', '.']],\n",
              " [['forget', 'him', '.']],\n",
              " [['forget', 'him', '.']],\n",
              " [['forget', 'him', '.']],\n",
              " [['forget', 'him', '.']],\n",
              " [['god', 'exists', '.']],\n",
              " [['he', 'is', 'here', '!']],\n",
              " [['he', 'is', 'here', '!']],\n",
              " [['he', 'is', 'here', '!']],\n",
              " [['he', 'is', 'late', '.']],\n",
              " [['he', 'is', 'nice', '.']],\n",
              " [['he', 'is', 'nice', '.']],\n",
              " [['he', 'laughed', '.']],\n",
              " [['he', 'laughed', '.']],\n",
              " [['he', \"'\", 's', 'swiss', '.']],\n",
              " [['he', \"'\", 's', 'smart', '.']],\n",
              " [['he', \"'\", 's', 'smart', '.']],\n",
              " [['hold', 'still', '.']],\n",
              " [['hold', 'still', '.']],\n",
              " [['i', 'am', 'a', 'man', '.']],\n",
              " [['i', 'am', 'a', 'man', '.']],\n",
              " [['i', 'am', 'ready', '.']],\n",
              " [['i', 'can', 'read', '.']],\n",
              " [['i', 'can', 'read', '.']],\n",
              " [['i', 'can', 'read', '.']],\n",
              " [['i', 'can', 'swim', '.']],\n",
              " [['i', 'can', 'swim', '.']],\n",
              " [['i', 'can', 'swim', '.']],\n",
              " [['i', 'can', 'walk', '.']],\n",
              " [['i', 'can', 'walk', '.']],\n",
              " [['i', 'eat', 'here', '.']],\n",
              " [['i', 'eat', 'here', '.']],\n",
              " [['i', 'eat', 'meat', '.']],\n",
              " [['i', 'eat', 'meat', '.']],\n",
              " [['i', 'eat', 'rice', '.']],\n",
              " [['i', 'eat', 'rice', '.']],\n",
              " [['i', 'help', 'tom', '.']],\n",
              " [['i', 'help', 'tom', '.']],\n",
              " [['i', 'just', 'ate', '.']],\n",
              " [['i', 'like', 'him', '.']],\n",
              " [['i', 'like', 'him', '.']],\n",
              " [['i', 'like', 'tea', '.']],\n",
              " [['i', 'like', 'you', '.']],\n",
              " [['i', 'like', 'you', '.']],\n",
              " [['i', 'like', 'you', '.']],\n",
              " [['i', 'love', 'you', '.']],\n",
              " [['i', 'love', 'you', '.']],\n",
              " [['i', 'made', 'tea', '.']],\n",
              " [['i', 'miss', 'you', '.']],\n",
              " [['i', 'miss', 'you', '.']],\n",
              " [['i', 'ran', 'away', '.']],\n",
              " [['i', 'ran', 'away', '.']],\n",
              " [['i', 'remember', '.']],\n",
              " [['i', 'remember', '.']],\n",
              " [['i', 'remember', '.']],\n",
              " [['i', 'screamed', '.']],\n",
              " [['i', 'screamed', '.']],\n",
              " [['i', 'see', 'that', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'see', 'them', '.']],\n",
              " [['i', 'shot', 'tom', '.']],\n",
              " [['i', 'want', 'tom', '.']],\n",
              " [['i', 'want', 'tom', '.']],\n",
              " [['i', 'want', 'you', '.']],\n",
              " [['i', 'want', 'you', '.']],\n",
              " [['i', 'want', 'you', '.']],\n",
              " [['i', 'was', 'good', '.']],\n",
              " [['i', 'was', 'good', '.']],\n",
              " [['i', 'was', 'late', '.']],\n",
              " [['i', 'was', 'sick', '.']],\n",
              " [['i', 'was', 'sick', '.']],\n",
              " [['i', 'work', 'out', '.']],\n",
              " [['i', 'work', 'out', '.']],\n",
              " [['i', \"'\", 'll', 'leave', '.']],\n",
              " [['i', \"'\", 'll', 'start', '.']],\n",
              " [['i', \"'\", 'll', 'start', '.']],\n",
              " [['i', \"'\", 'll', 'start', '.']],\n",
              " [['i', \"'\", 'm', '30', 'now', '.']],\n",
              " [['i', \"'\", 'm', '30', 'now', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'liar', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'liar', '.']],\n",
              " [['i', \"'\", 'm', 'a', 'poet', '.']],\n",
              " [['i', \"'\", 'm', 'coming', '.']],\n",
              " [['i', \"'\", 'm', 'coming', '.']],\n",
              " [['i', \"'\", 'm', 'hungry', '!']],\n",
              " [['i', \"'\", 'm', 'hungry', '.']],\n",
              " [['i', \"'\", 'm', 'scared', '.']],\n",
              " [['i', \"'\", 'm', 'scared', '.']],\n",
              " [['i', \"'\", 'm', 'sleepy', '!']],\n",
              " [['i', \"'\", 'm', 'so', 'fat', '.']],\n",
              " [['i', \"'\", 'm', 'so', 'fat', '.']],\n",
              " [['i', \"'\", 'm', 'trying', '.']],\n",
              " [['i', \"'\", 'm', 'trying', '.']],\n",
              " [['is', 'tom', 'big', '?']],\n",
              " [['is', 'he', 'tall', '?']],\n",
              " [['is', 'he', 'tall', '?']],\n",
              " [['is', 'it', 'done', '?']],\n",
              " [['is', 'it', 'free', '?']],\n",
              " [['is', 'it', 'free', '?']],\n",
              " [['is', 'it', 'hard', '?']],\n",
              " [['is', 'it', 'here', '?']],\n",
              " [['is', 'it', 'nice', '?']],\n",
              " [['is', 'it', 'time', '?']],\n",
              " [['is', 'it', 'true', '?']],\n",
              " [['is', 'that', 'so', '?']],\n",
              " [['is', 'this', 'it', '?']],\n",
              " [['is', 'this', 'it', '?']],\n",
              " [['is', 'this', 'it', '?']],\n",
              " [['is', 'this', 'it', '?']],\n",
              " [['it', \"'\", 's', 'alive', '.']],\n",
              " [['it', \"'\", 's', 'my', 'cd', '.']],\n",
              " [['it', \"'\", 's', 'my', 'cd', '.']],\n",
              " [['it', \"'\", 's', 'night', '.']],\n",
              " [['it', \"'\", 's', 'ready', '.']],\n",
              " [['it', \"'\", 's', 'white', '.']],\n",
              " [['it', \"'\", 's', 'white', '.']],\n",
              " [['it', \"'\", 's', 'white', '.']],\n",
              " [['it', \"'\", 's', 'white', '.']],\n",
              " [['it', \"'\", 's', 'yours', '.']],\n",
              " [['it', \"'\", 's', 'yours', '.']],\n",
              " [['it', \"'\", 's', 'yours', '.']],\n",
              " [['it', \"'\", 's', 'yours', '.']],\n",
              " [['jesus', 'wept', '.']],\n",
              " [['keep', 'quiet', '!']],\n",
              " [['keep', 'quiet', '!']],\n",
              " [['keep', 'quiet', '!']],\n",
              " [['keep', 'quiet', '.']],\n",
              " [['keep', 'quiet', '.']],\n",
              " [['keep', 'quiet', '.']],\n",
              " [['let', 'tom', 'go', '.']],\n",
              " [['let', 'tom', 'go', '.']],\n",
              " [['let', 'tom', 'go', '.']],\n",
              " [['let', 'tom', 'go', '.']],\n",
              " [['let', 'tom', 'in', '.']],\n",
              " [['let', 'tom', 'in', '.']],\n",
              " [['let', 'him', 'go', '!']],\n",
              " [['let', 'him', 'go', '!']],\n",
              " [['let', 'me', 'see', '.']],\n",
              " [['let', 'me', 'see', '.']],\n",
              " [['let', \"'\", 's', 'chat', '.']],\n",
              " [['let', \"'\", 's', 'kiss', '.']],\n",
              " [['let', \"'\", 's', 'talk', '.']],\n",
              " [['look', 'again', '.']],\n",
              " [['look', 'again', '.']],\n",
              " [['look', 'ahead', '.']],\n",
              " [['look', 'ahead', '.']],\n",
              " [['look', 'ahead', '.']],\n",
              " [['look', 'at', 'me', '.']],\n",
              " [['look', 'at', 'me', '.']],\n",
              " [['look', 'there', '.']],\n",
              " [['look', 'there', '.']],\n",
              " [['love', 'lasts', '.']],\n",
              " [['never', 'mind', '!']],\n",
              " [['never', 'mind', '!']],\n",
              " [['never', 'mind', '!']],\n",
              " [['never', 'mind', '!']],\n",
              " [['no', 'one', 'ran', '.']],\n",
              " [['quiet', 'down', '.']],\n",
              " [['quiet', 'down', '.']],\n",
              " [['shadow', 'him', '.']],\n",
              " [['shadow', 'him', '.']],\n",
              " [['shadow', 'him', '.']],\n",
              " [['shadow', 'him', '.']],\n",
              " [['she', 'is', 'old', '.']],\n",
              " [['she', 'is', 'old', '.']],\n",
              " [['smell', 'this', '.']],\n",
              " [['smell', 'this', '.']],\n",
              " [['stand', 'back', '!']],\n",
              " [['stand', 'back', '!']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['start', 'here', '.']],\n",
              " [['stay', 'alert', '.']],\n",
              " [['stay', 'alert', '.']],\n",
              " [['stay', 'still', '.']],\n",
              " [['step', 'aside', '.']],\n",
              " [['step', 'aside', '.']],\n",
              " [['study', 'hard', '.']],\n",
              " [['study', 'hard', '.']],\n",
              " [['take', 'a', 'bus', '.']],\n",
              " [['take', 'a', 'bus', '.']],\n",
              " [['take', 'a', 'bus', '.']],\n",
              " [['take', 'these', '.']],\n",
              " [['take', 'these', '.']],\n",
              " [['take', 'these', '.']],\n",
              " [['take', 'these', '.']],\n",
              " [['talk', 'to', 'me', '.']],\n",
              " [['talk', 'to', 'us', '.']],\n",
              " [['that', 'a', 'boy', '!']],\n",
              " [['that', \"'\", 's', 'tom', '.']],\n",
              " [['that', \"'\", 's', 'tom', '.']],\n",
              " [['that', \"'\", 's', 'wet', '.']],\n",
              " [['they', 'stood', '.']],\n",
              " [['they', 'stood', '.']],\n",
              " [['they', 'tried', '.']],\n",
              " [['this', 'is', 'me', '.']],\n",
              " [['this', 'is', 'me', '.']],\n",
              " [['time', 'is', 'up', '.']],\n",
              " [['time', 'is', 'up', '.']],\n",
              " [['tom', 'bit', 'me', '.']],\n",
              " [['tom', 'burped', '.']],\n",
              " [['tom', 'called', '.']],\n",
              " [['tom', 'called', '.']],\n",
              " [['tom', 'danced', '.']],\n",
              " [['tom', 'drinks', '.']],\n",
              " [['tom', 'failed', '.']],\n",
              " [['tom', 'failed', '.']],\n",
              " [['tom', 'forgot', '.']],\n",
              " [['tom', 'fought', '.']],\n",
              " [['tom', 'helped', '.']],\n",
              " [['tom', 'is', 'fat', '.']],\n",
              " [['tom', 'is', 'ill', '.']],\n",
              " [['tom', 'is', 'out', '.']],\n",
              " [['tom', 'is', 'out', '.']],\n",
              " [['tom', 'jumped', '.']],\n",
              " [['tom', 'looked', '.']],\n",
              " [['tom', 'moaned', '.']],\n",
              " [['tom', 'phoned', '.']],\n",
              " [['tom', 'saw', 'me', '.']],\n",
              " [['tom', 'saw', 'me', '.']],\n",
              " [['tom', 'shaved', '.']],\n",
              " [['tom', 'snores', '.']],\n",
              " [['tom', 'talked', '.']],\n",
              " [['tom', 'waited', '.']],\n",
              " [['tom', 'waited', '.']],\n",
              " [['tom', 'yawned', '.']],\n",
              " [['tom', 'yelled', '.']],\n",
              " [['tom', \"'\", 'll', 'die', '.']],\n",
              " [['tom', \"'\", 's', 'deaf', '.']],\n",
              " [['tom', \"'\", 's', 'sick', '.']],\n",
              " [['tom', \"'\", 's', 'ugly', '.']],\n",
              " [['turn', 'right', '.']],\n",
              " [['turn', 'right', '.']],\n",
              " [['watch', 'this', '.']],\n",
              " [['watch', 'this', '.']],\n",
              " [['we', 'are', 'men', '.']],\n",
              " [['we', 'are', 'men', '.']],\n",
              " [['we', 'had', 'fun', '.']],\n",
              " [['we', 'had', 'fun', '.']],\n",
              " [['we', 'laughed', '.']],\n",
              " [['we', 'laughed', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'like', 'it', '.']],\n",
              " [['we', 'want', 'it', '.']],\n",
              " [['we', 'want', 'it', '.']],\n",
              " [['we', 'want', 'it', '.']],\n",
              " [['we', \"'\", 'll', 'help', '.']],\n",
              " [['we', \"'\", 'll', 'help', '.']],\n",
              " [['we', \"'\", 'll', 'sing', '.']],\n",
              " [['we', \"'\", 'll', 'sing', '.']],\n",
              " [['we', \"'\", 'll', 'wait', '.']],\n",
              " [['we', \"'\", 'll', 'wait', '.']],\n",
              " [['we', \"'\", 'll', 'work', '.']],\n",
              " [['we', \"'\", 're', 'back', '.']],\n",
              " [['we', \"'\", 're', 'back', '.']],\n",
              " [['we', \"'\", 're', 'boys', '.']],\n",
              " [['we', \"'\", 're', 'boys', '.']],\n",
              " [['we', \"'\", 're', 'fine', '.']],\n",
              " [['we', \"'\", 're', 'fine', '.']],\n",
              " [['we', \"'\", 're', 'fine', '.']],\n",
              " [['we', \"'\", 're', 'fine', '.']],\n",
              " [['where', 'am', 'i', '?']],\n",
              " [['who', 'are', 'we', '?']],\n",
              " [['who', 'are', 'we', '?']],\n",
              " [['who', 'did', 'it', '?']],\n",
              " [['who', 'has', 'it', '?']],\n",
              " [['who', 'has', 'it', '?']],\n",
              " [['who', 'is', 'tom', '?']],\n",
              " [['who', 'is', 'she', '?']],\n",
              " [['who', 'is', 'she', '?']],\n",
              " [['who', 'phoned', '?']],\n",
              " [['who', 'talked', '?']],\n",
              " [['who', 'was', 'it', '?']],\n",
              " [['who', 'yelled', '?']],\n",
              " [['who', \"'\", 's', 'here', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['who', \"'\", 's', 'that', '?']],\n",
              " [['wood', 'burns', '.']],\n",
              " [['you', 'decide', '.']],\n",
              " [['you', 'decide', '.']],\n",
              " [['you', 'decide', '.']],\n",
              " [['you', 'decide', '.']],\n",
              " [['you', 'may', 'go', '.']],\n",
              " [['you', 'may', 'go', '.']],\n",
              " [['you', 'may', 'go', '.']],\n",
              " [['you', 'may', 'go', '.']],\n",
              " [['you', \"'\", 've', 'won', '.']],\n",
              " [['you', \"'\", 've', 'won', '.']],\n",
              " [['you', \"'\", 've', 'won', '.']],\n",
              " [['anyone', 'home', '?']],\n",
              " [['anyone', 'hurt', '?']],\n",
              " [['are', 'we', 'done', '?']],\n",
              " [['are', 'you', 'tom', '?']],\n",
              " [['are', 'you', 'tom', '?']],\n",
              " [['are', 'you', 'tom', '?']],\n",
              " [['are', 'you', 'tom', '?']],\n",
              " [['are', 'you', 'new', '?']],\n",
              " [['are', 'you', 'new', '?']],\n",
              " [['are', 'you', 'new', '?']],\n",
              " [['be', 'prepared', '.']],\n",
              " [['breathe', 'out', '.']],\n",
              " [['breathe', 'out', '.']],\n",
              " [['come', 'inside', '.']],\n",
              " [['come', 'inside', '.']],\n",
              " [['did', 'tom', 'die', '?']],\n",
              " [['did', 'tom', 'die', '?']],\n",
              " [['did', 'tom', 'eat', '?']],\n",
              " [['did', 'tom', 'win', '?']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'argue', '.']],\n",
              " [['don', \"'\", 't', 'fight', '.']],\n",
              " [['don', \"'\", 't', 'fight', '.']],\n",
              " [['don', \"'\", 't', 'fight', '.']],\n",
              " [['don', \"'\", 't', 'leave', '.']],\n",
              " [['don', \"'\", 't', 'leave', '.']],\n",
              " [['don', \"'\", 't', 'leave', '.']],\n",
              " [['don', \"'\", 't', 'leave', '.']],\n",
              " [['don', \"'\", 't', 'shout', '.']],\n",
              " [['don', \"'\", 't', 'shout', '.']],\n",
              " [['eat', 'with', 'us', '.']],\n",
              " [['forgive', 'tom', '.']],\n",
              " [['forgive', 'tom', '.']],\n",
              " [['get', 'changed', '.']],\n",
              " [['get', 'changed', '.']],\n",
              " [['get', 'started', '.']],\n",
              " [['get', 'started', '.']],\n",
              " [['get', 'started', '.']],\n",
              " [['get', 'started', '.']],\n",
              " [['go', 'have', 'fun', '.']],\n",
              " [['go', 'have', 'fun', '.']],\n",
              " [['go', 'help', 'tom', '.']],\n",
              " [['have', 'a', 'seat', '.']],\n",
              " [['have', 'a', 'seat', '.']],\n",
              " [['he', 'can', 'come', '.']],\n",
              " [['he', 'can', 'come', '.']],\n",
              " [['he', 'can', 'read', '.']],\n",
              " [['he', 'can', 'read', '.']],\n",
              " [['he', 'found', 'it', '.']],\n",
              " [['he', 'found', 'it', '.']],\n",
              " [['he', 'has', 'wine', '.']],\n",
              " [['he', 'is', 'happy', '.']],\n",
              " [['he', 'is', 'happy', '.']],\n",
              " [['he', 'is', 'lying', '.']],\n",
              " [['he', 'is', 'lying', '.']],\n",
              " [['he', 'is', 'young', '.']],\n",
              " [['he', 'resigned', '.']],\n",
              " [['he', 'resigned', '.']],\n",
              " [['he', 'stood', 'up', '.']],\n",
              " [['he', 'stood', 'up', '.']],\n",
              " [['he', 'stood', 'up', '.']],\n",
              " [['he', 'stood', 'up', '.']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'are', 'you', '?']],\n",
              " [['how', 'strange', '!']],\n",
              " [['i', 'am', 'taller', '.']],\n",
              " [['i', 'came', 'back', '.']],\n",
              " [['i', 'came', 'back', '.']],\n",
              " [['i', 'can', 'dance', '.']],\n",
              " [['i', 'can', 'dance', '.']],\n",
              " [['i', 'can', 'dance', '.']],\n",
              " [['i', 'can', 'do', 'it', '.']],\n",
              " [['i', 'can', 'do', 'it', '.']],\n",
              " [['i', 'can', \"'\", 't', 'fly', '.']],\n",
              " [['i', 'can', \"'\", 't', 'say', '.']],\n",
              " [['i', 'can', \"'\", 't', 'say', '.']],\n",
              " [['i', 'can', \"'\", 't', 'say', '.']],\n",
              " [['i', 'don', \"'\", 't', 'cry', '.']],\n",
              " [['i', 'don', \"'\", 't', 'eat', '.']],\n",
              " [['i', 'drank', 'tea', '.']],\n",
              " [['i', 'drank', 'tea', '.']],\n",
              " [['i', 'eat', 'bread', '.']],\n",
              " [['i', 'eat', 'bread', '.']],\n",
              " [['i', 'eat', 'bread', '.']],\n",
              " [['i', 'eat', 'bread', '.']],\n",
              " [['i', 'eat', 'fruit', '.']],\n",
              " [['i', 'eat', 'fruit', '.']],\n",
              " [['i', 'exercised', '.']],\n",
              " [['i', 'feel', 'fine', '.']],\n",
              " [['i', 'felt', 'that', '.']],\n",
              " [['i', 'forgot', 'it', '.']],\n",
              " [['i', 'forgot', 'it', '.']],\n",
              " [['i', 'had', 'a', 'cat', '.']],\n",
              " [['i', 'have', 'time', '.']],\n",
              " [['i', 'have', 'wine', '.']],\n",
              " [['i', 'knew', 'that', '.']],\n",
              " [['i', 'know', 'this', '.']],\n",
              " [['i', 'left', 'home', '.']],\n",
              " [['i', 'left', 'home', '.']],\n",
              " [['i', 'left', 'home', '.']],\n",
              " [['i', 'left', 'home', '.']],\n",
              " [['i', 'like', 'beer', '.']],\n",
              " [['i', 'like', 'both', '.']],\n",
              " [['i', 'like', 'cake', '.']],\n",
              " [['i', 'like', 'cats', '.']],\n",
              " [['i', 'like', 'fish', '.']],\n",
              " [['i', 'like', 'math', '.']],\n",
              " [['i', 'like', 'rice', '.']],\n",
              " [['i', 'like', 'that', '.']],\n",
              " [['i', 'like', 'that', '.']],\n",
              " [['i', 'like', 'that', '.']],\n",
              " [['i', 'made', 'that', '.']],\n",
              " [['i', 'made', 'that', '.']],\n",
              " [['i', 'need', 'time', '.']],\n",
              " [['i', 'never', 'cry', '.']],\n",
              " [['i', 'saved', 'you', '.']],\n",
              " [['i', 'saved', 'you', '.']],\n",
              " [['i', 'smell', 'gas', '.']],\n",
              " [['i', 'trust', 'tom', '.']],\n",
              " [['i', 'want', 'more', '.']],\n",
              " [['i', 'want', 'more', '.']],\n",
              " [['i', 'want', 'them', '.']],\n",
              " [['i', 'want', 'them', '.']],\n",
              " [['i', 'want', 'them', '.']],\n",
              " [['i', 'want', 'this', '.']],\n",
              " [['i', 'want', 'this', '.']],\n",
              " [['i', 'want', 'time', '.']],\n",
              " [['i', 'was', 'alone', '.']],\n",
              " [['i', 'was', 'alone', '.']],\n",
              " [['i', 'was', 'naked', '.']],\n",
              " [['i', 'was', 'naked', '.']],\n",
              " [['i', 'was', 'naked', '.']],\n",
              " [['i', 'was', 'naked', '.']],\n",
              " [['i', 'was', 'stuck', '.']],\n",
              " [['i', 'was', 'stuck', '.']],\n",
              " [['i', 'was', 'tired', '.']],\n",
              " [['i', 'was', 'tired', '.']],\n",
              " [['i', 'went', ',', 'too', '.']],\n",
              " [['i', 'went', ',', 'too', '.']],\n",
              " [['i', 'will', 'work', '.']],\n",
              " [['i', 'won', \"'\", 't', 'lie', '.']],\n",
              " [['i', \"'\", 'll', 'buy', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'decide', '.']],\n",
              " [['i', \"'\", 'll', 'decide', '.']],\n",
              " [['i', \"'\", 'll', 'eat', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'by', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'get', 'it', '.']],\n",
              " [['i', \"'\", 'll', 'scream', '.']],\n",
              " [['i', \"'\", 'm', 'dancing', '.']],\n",
              " [['i', \"'\", 'm', 'dancing', '.']],\n",
              " [['i', \"'\", 'm', 'dieting', '.']],\n",
              " [['i', \"'\", 'm', 'dieting', '.']],\n",
              " [['i', \"'\", 'm', 'falling', '.']],\n",
              " [['i', \"'\", 'm', 'falling', '.']],\n",
              " [['i', \"'\", 'm', 'healthy', '.']],\n",
              " [['i', \"'\", 'm', 'healthy', '.']],\n",
              " [['i', \"'\", 'm', 'in', 'jail', '.']],\n",
              " [['i', \"'\", 'm', 'in', 'jail', '.']],\n",
              " [['i', \"'\", 'm', 'not', 'tom', '.']],\n",
              " [['i', \"'\", 'm', 'popular', '.']],\n",
              " [['i', \"'\", 'm', 'reading', '.']],\n",
              " [['i', \"'\", 'm', 'reading', '.']],\n",
              " [['i', \"'\", 'm', 'resting', '.']],\n",
              " [['i', \"'\", 'm', 'resting', '.']],\n",
              " [['i', \"'\", 'm', 'so', 'full', '.']],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRl-xuzjbu-3",
        "outputId": "0f727a7f-d8b5-453b-9480-fab32f8f237a"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['go', '.'],\n",
              " ['jump', '!'],\n",
              " ['jump', '!'],\n",
              " ['jump', '!'],\n",
              " ['jump', '!'],\n",
              " ['who', \"'\", 's', 'up', '?'],\n",
              " ['never', 'open', '!'],\n",
              " ['come', '!'],\n",
              " ['go', '!'],\n",
              " ['help', '!'],\n",
              " ['help', 'me', '!'],\n",
              " ['jump', '!'],\n",
              " ['jump', '!'],\n",
              " ['jump', '.'],\n",
              " ['jump', '.'],\n",
              " ['come', '!'],\n",
              " ['come', '!'],\n",
              " ['come', '!'],\n",
              " ['come', '!'],\n",
              " ['it', \"'\", 's', 'popularity', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['i', 'won', '!'],\n",
              " ['i', 'won', '!'],\n",
              " ['get', 'up', '.'],\n",
              " ['come', 'on', '!'],\n",
              " ['who', 'did', 'it', 'go', '?'],\n",
              " ['who', 'did', 'you', 'get', '?'],\n",
              " ['did', 'you', 'see', '?'],\n",
              " ['did', 'you', 'see', 'it', '?'],\n",
              " ['he', 'ran', '.'],\n",
              " ['he', 'ran', '.'],\n",
              " ['he', 'ran', '.'],\n",
              " ['he', 'ran', '.'],\n",
              " ['i', 'fell', '.'],\n",
              " ['i', 'fell', '.'],\n",
              " ['i', 'fell', '.'],\n",
              " ['i', 'fell', '.'],\n",
              " ['i', 'know', '.'],\n",
              " ['i', 'know', '.'],\n",
              " ['there', 'knows', 'don', \"'\", 't', 'know', '.'],\n",
              " ['i', 'lost', '.'],\n",
              " ['i', 'lost', '.'],\n",
              " ['i', 'work', '.'],\n",
              " ['i', 'work', '.'],\n",
              " ['i', \"'\", 'm', 'ok', '.'],\n",
              " ['get', 'this', '.'],\n",
              " ['listen', 'to', 'me', '.'],\n",
              " ['no', 'one', 'has', '!'],\n",
              " ['what', \"'\", 't', 'you', '?'],\n",
              " ['what', \"'\", 't', 'you', '?'],\n",
              " ['really', '?'],\n",
              " ['do', 'you', 'do', '.'],\n",
              " ['we', 'won', '.'],\n",
              " ['we', 'won', '.'],\n",
              " ['can', \"'\", 's', 'it', 'show', 'me', '?'],\n",
              " ['why', 'should', 'i', 'show', 'me', '?'],\n",
              " ['ask', 'tom', '.'],\n",
              " ['ask', 'tom', '.'],\n",
              " ['call', 'me', '.'],\n",
              " ['call', 'me', '.'],\n",
              " ['call', 'me', '.'],\n",
              " ['call', 'me', '.'],\n",
              " ['call', 'us', '.'],\n",
              " ['call', 'us', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['come', 'on', '!'],\n",
              " ['come', 'on', '!'],\n",
              " ['fold', 'it', '.'],\n",
              " ['fold', 'it', '.'],\n",
              " ['take', 'tom', 'to', 'tom', '.'],\n",
              " ['get', 'tom', '.'],\n",
              " ['grab', 'tom', '.'],\n",
              " ['get', 'out', '.'],\n",
              " ['get', 'out', '.'],\n",
              " ['let', \"'\", 's', 'home', '.'],\n",
              " ['he', 'came', '.'],\n",
              " ['he', 'came', '.'],\n",
              " ['he', 'left', '.'],\n",
              " ['let', 'go', 'out', '.'],\n",
              " ['he', 'runs', '.'],\n",
              " ['he', 'runs', '.'],\n",
              " ['he', 'runs', '.'],\n",
              " ['he', 'runs', '.'],\n",
              " ['help', '!'],\n",
              " ['help', 'me', '!'],\n",
              " ['help', 'me', '!'],\n",
              " ['help', 'me', '.'],\n",
              " ['help', 'me', '.'],\n",
              " ['help', 'us', '.'],\n",
              " ['help', 'us', '.'],\n",
              " ['help', 'us', '.'],\n",
              " ['i', \"'\", 'm', 'tom', '.'],\n",
              " ['tom', \"'\", 'm', 'tom', '.'],\n",
              " ['i', \"'\", 'm', 'tom', '.'],\n",
              " ['i', \"'\", 'm', 'fat', '.'],\n",
              " ['i', \"'\", 'm', 'fat', '.'],\n",
              " ['i', \"'\", 'm', 'ill', '.'],\n",
              " ['it', \"'\", 's', 'fine', '.'],\n",
              " ['that', \"'\", 'm', 'my', 'lawyer', '.'],\n",
              " ['it', \"'\", 'm', '.'],\n",
              " ['i', ',', 'i', '.'],\n",
              " ['i', 'can', 'me', '.'],\n",
              " ['open', '.'],\n",
              " ['open', '.'],\n",
              " ['never', 'mind', '!'],\n",
              " ['show', 'me', 'to', 'me', '.'],\n",
              " ['show', 'me', 'to', 'me', '.'],\n",
              " ['be', '!'],\n",
              " ['shut', 'up', '!'],\n",
              " ['shut', 'up', '!'],\n",
              " ['tell', 'me', 'to', 'me', '.'],\n",
              " ['tell', 'me', 'to', 'me', '.'],\n",
              " ['tom', 'ran', 'away', '.'],\n",
              " ['tom', 'ran', '.'],\n",
              " ['tom', 'won', '.'],\n",
              " ['wake', 'up', '!'],\n",
              " ['wake', 'up', '!'],\n",
              " ['wake', 'up', '!'],\n",
              " ['we', 'care', '.'],\n",
              " ['we', 'care', '.'],\n",
              " ['we', 'know', '.'],\n",
              " ['we', 'know', '.'],\n",
              " ['we', 'lost', '.'],\n",
              " ['we', 'lost', '.'],\n",
              " ['welcome', '.'],\n",
              " ['welcome', '.'],\n",
              " ['welcome', '.'],\n",
              " ['welcome', '.'],\n",
              " ['who', 'ate', '?'],\n",
              " ['who', 'won', '?'],\n",
              " ['who', \"'\", 's', 'shot', '?'],\n",
              " ['who', \"'\", 's', 'why', '?'],\n",
              " ['why', 'don', \"'\", 't', 'you', 'do', 'that', '?'],\n",
              " ['you', 'won', '.'],\n",
              " ['you', 'won', '.'],\n",
              " ['you', 'won', '.'],\n",
              " ['back', 'away', '!'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['there', 'can', 'know', 'what', '.'],\n",
              " ['i', 'know', 'so', '.'],\n",
              " ['call', 'tom', 'to', 'tom', '.'],\n",
              " ['call', 'tom', '.'],\n",
              " ['call', 'tom', 'called', '.'],\n",
              " ['call', 'tom', 'called', '.'],\n",
              " ['write', 'down', '.'],\n",
              " ['grab', 'tom', '.'],\n",
              " ['grab', 'tom', '.'],\n",
              " ['grab', 'him', '.'],\n",
              " ['money', 'have', 'fun', '.'],\n",
              " ['he', 'said', '.'],\n",
              " ['he', 'did', 'that', '.'],\n",
              " ['i', 'can', 'go', '.'],\n",
              " ['i', 'can', 'go', '.'],\n",
              " ['i', 'forgot', '.'],\n",
              " ['i', 'forgot', '.'],\n",
              " ['i', 'got', 'it', '.'],\n",
              " ['it', 'went', 've', 'to', '.'],\n",
              " ['i', 'looked', '.'],\n",
              " ['i', 'looked', '.'],\n",
              " ['i', 'phoned', '.'],\n",
              " ['i', 'prayed', '.'],\n",
              " ['i', 'shaved', '.'],\n",
              " ['i', 'use', 'it', '.'],\n",
              " ['i', 'use', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'try', '.'],\n",
              " ['i', \"'\", 'm', 'full', '.'],\n",
              " ['i', \"'\", 'm', 'game', '.'],\n",
              " ['i', \"'\", 'll', 'be', 'late', '.'],\n",
              " ['i', \"'\", 'm', 'lazy', '.'],\n",
              " ['i', \"'\", 'm', 'poor', '.'],\n",
              " ['i', \"'\", 've', 'won', '.'],\n",
              " ['i', \"'\", 've', 'won', \"'\", 've', 'arrived', '.'],\n",
              " ['it', \"'\", 's', 'hot', '.'],\n",
              " ['it', \"'\", 's', 'new', '.'],\n",
              " ['that', 'is', 'new', '.'],\n",
              " ['it', \"'\", 's', 'old', '.'],\n",
              " ['it', \"'\", 's', 'old', '.'],\n",
              " ['it', \"'\", 's', 'old', '.'],\n",
              " ['kiss', 'tom', '.'],\n",
              " ['leave', 'it', '.'],\n",
              " ['leave', 'it', '.'],\n",
              " ['leave', 'it', '.'],\n",
              " ['leave', 'it', '.'],\n",
              " ['leave', 'me', 'to', 'me', '.'],\n",
              " ['leave', 'me', '.'],\n",
              " ['leave', 'me', 'to', 'me', '.'],\n",
              " ['come', 'on', '!'],\n",
              " ['do', 'it', 'to', 'me', '.'],\n",
              " ['come', 'with', 'me', '.'],\n",
              " ['she', 'came', '.'],\n",
              " ['she', 'came', '.'],\n",
              " ['she', 'died', '.'],\n",
              " ['she', 'died', '.'],\n",
              " ['she', 'died', '.'],\n",
              " ['she', 'died', '.'],\n",
              " ['sit', 'down', '!'],\n",
              " ['sit', 'down', '!'],\n",
              " ['here', \"'\", 's', 'room', '.'],\n",
              " ['here', \"'\", 's', 'room', '.'],\n",
              " ['come', 'back', '!'],\n",
              " ['stand', 'up', '!'],\n",
              " ['stand', 'up', '!'],\n",
              " ['stop', 'tom', '.'],\n",
              " ['stop', 'tom', '.'],\n",
              " ['terrific', '!'],\n",
              " ['they', 'won', '.'],\n",
              " ['they', 'won', '.'],\n",
              " ['tom', 'came', '.'],\n",
              " ['tom', 'died', '.'],\n",
              " ['tom', 'fell', '.'],\n",
              " ['tom', 'knew', 'tom', '.'],\n",
              " ['tom', 'left', '.'],\n",
              " ['tom', 'left', '.'],\n",
              " ['tom', 'left', '.'],\n",
              " ['tom', 'lied', '.'],\n",
              " ['tom', 'lies', '.'],\n",
              " ['tom', 'lost', '.'],\n",
              " ['tom', 'cried', '.'],\n",
              " ['tom', \"'\", 's', 'up', '.'],\n",
              " ['use', 'this', '.'],\n",
              " ['use', 'this', '.'],\n",
              " ['warn', 'tom', '.'],\n",
              " ['warn', 'tom', '.'],\n",
              " ['watch', 'me', '.'],\n",
              " ['watch', 'me', '.'],\n",
              " ['watch', 'us', '.'],\n",
              " ['watch', 'us', '.'],\n",
              " ['we', \"'\", 'll', 'go', '.'],\n",
              " ['we', \"'\", 'll', 'go', '.'],\n",
              " ['what', 'for', '?'],\n",
              " ['what', 'for', '?'],\n",
              " ['who', 'am', 'i', 'am', '?'],\n",
              " ['who', 'was', '?'],\n",
              " ['who', \"'\", 's', 'why', '?'],\n",
              " ['who', 'was', 'it', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['answer', 'me', '.'],\n",
              " ['answer', 'me', '.'],\n",
              " ['children', 'fly', '.'],\n",
              " ['never', 'run', '!'],\n",
              " ['grab', 'him', '.'],\n",
              " ['grab', 'him', '.'],\n",
              " ['catch', 'him', '.'],\n",
              " ['catch', 'him', '.'],\n",
              " ['come', 'here', '.'],\n",
              " ['come', 'here', '.'],\n",
              " ['come', 'home', '.'],\n",
              " ['come', 'home', '.'],\n",
              " ['did', 'i', 'win', '?'],\n",
              " ['did', 'i', 'win', '?'],\n",
              " ['dogs', 'dogs', ',', 'too', '.'],\n",
              " ['don', \"'\", 't', 'ask', 'me', '.'],\n",
              " ['don', \"'\", 't', 'ask', 'me', '.'],\n",
              " ['don', \"'\", 't', 'drink', '.'],\n",
              " ['don', \"'\", 't', 'drink', '.'],\n",
              " ['don', \"'\", 't', 'die', '.'],\n",
              " ['don', \"'\", 't', 'die', '.'],\n",
              " ['don', \"'\", 't', 'lie', '.'],\n",
              " ['don', \"'\", 't', 'lie', '.'],\n",
              " ['don', \"'\", 't', 'run', '.'],\n",
              " ['don', \"'\", 't', 'run', '.'],\n",
              " ['don', \"'\", 't', 'run', '.'],\n",
              " ['don', \"'\", 't', 'run', '.'],\n",
              " ['forget', 'it', '.'],\n",
              " ['forget', 'it', '.'],\n",
              " ['forget', 'me', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['go', 'in', '.'],\n",
              " ['grab', 'that', '.'],\n",
              " ['grab', 'that', '.'],\n",
              " ['grab', 'that', '.'],\n",
              " ['grab', 'that', '.'],\n",
              " ['grab', 'this', '.'],\n",
              " ['grab', 'this', '.'],\n",
              " ['he', 'is', 'ill', '.'],\n",
              " ['they', 'are', 'sick', '.'],\n",
              " ['he', \"'\", 's', 'a', 'dj', '.'],\n",
              " ['he', \"'\", 's', 'a', 'dj', '.'],\n",
              " ['he', \"'\", 's', 'mine', '.'],\n",
              " ['that', 'are', 'mine', '.'],\n",
              " ['he', \"'\", 's', 'sexy', '.'],\n",
              " ['he', \"'\", 's', 'sexy', '.'],\n",
              " ['hold', 'this', '.'],\n",
              " ['hold', 'this', '.'],\n",
              " ['how', 'is', 'it', '?'],\n",
              " ['how', \"'\", 's', 'there', '?'],\n",
              " ['how', \"'\", 's', 'there', '?'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'run', '.'],\n",
              " ['i', 'can', 'ski', '.'],\n",
              " ['i', 'can', 'ski', '.'],\n",
              " ['i', 'can', 'ski', '.'],\n",
              " ['i', 'can', 'win', '.'],\n",
              " ['i', 'can', 'win', '.'],\n",
              " ['i', 'fainted', '.'],\n",
              " ['i', 'got', 'fat', '.'],\n",
              " ['i', 'got', 'fat', '.'],\n",
              " ['i', 'hit', 'tom', '.'],\n",
              " ['i', 'hit', 'tom', '.'],\n",
              " ['i', 'hit', 'tom', '.'],\n",
              " ['i', 'laughed', '.'],\n",
              " ['i', 'laughed', '.'],\n",
              " ['i', 'met', 'him', 'him', '.'],\n",
              " ['i', 'met', 'him', 'him', '.'],\n",
              " ['i', 'saw', 'tom', '.'],\n",
              " ['i', 'saw', 'tom', '.'],\n",
              " ['i', 'saw', 'tom', '.'],\n",
              " ['i', 'saw', 'you', '.'],\n",
              " ['i', 'saw', 'you', '.'],\n",
              " ['i', 'saw', 'you', '.'],\n",
              " ['i', 'saw', 'you', '.'],\n",
              " ['i', 'shouted', '.'],\n",
              " ['i', 'shouted', '.'],\n",
              " ['i', 'want', 'it', '.'],\n",
              " ['i', 'want', 'it', '.'],\n",
              " ['i', 'want', 'it', '.'],\n",
              " ['i', 'like', 'i', '.'],\n",
              " ['i', 'will', 'go', '.'],\n",
              " ['i', \"'\", 'll', 'call', '.'],\n",
              " ['i', \"'\", 'll', 'sing', '.'],\n",
              " ['i', \"'\", 'll', 'wait', '.'],\n",
              " ['i', \"'\", 'll', 'catch', 'him', '.'],\n",
              " ['i', \"'\", 'll', 'walk', '.'],\n",
              " ['i', \"'\", 'll', 'work', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'man', '.'],\n",
              " ['i', 'am', 'a', 'man', '.'],\n",
              " ['i', \"'\", 'm', 'awake', '.'],\n",
              " ['i', \"'\", 'm', 'awake', '.'],\n",
              " ['i', \"'\", 'm', 'bored', '.'],\n",
              " ['i', \"'\", 'm', 'clean', '.'],\n",
              " ['i', \"'\", 'm', 'dying', '.'],\n",
              " ['i', \"'\", 'm', 'dying', '.'],\n",
              " ['i', \"'\", 'm', 'going', 'to', 'me', '.'],\n",
              " ['i', \"'\", 'm', 'going', 'to', 'me', '.'],\n",
              " ['i', \"'\", 'm', 'quiet', '.'],\n",
              " ['i', \"'\", 'm', 'right', '.'],\n",
              " ['i', \"'\", 'm', 'young', '.'],\n",
              " ['it', 'went', '.'],\n",
              " ['it', 'went', '.'],\n",
              " ['it', 'went', '.'],\n",
              " ['it', 'rained', '.'],\n",
              " ['it', 'went', '.'],\n",
              " ['it', \"'\", 'clock', '.'],\n",
              " ['it', 'is', 'cold', '.'],\n",
              " ['it', \"'\", 's', 'easy', '.'],\n",
              " ['there', \"'\", 's', 'food', '.'],\n",
              " ['it', \"'\", 's', 'good', '.'],\n",
              " ['it', \"'\", 's', 'here', '.'],\n",
              " ['it', \"'\", 's', 'here', '.'],\n",
              " ['it', \"'\", 's', 'mine', '.'],\n",
              " ['it', \"'\", 's', 'ours', '.'],\n",
              " ['it', \"'\", 's', 'ours', '.'],\n",
              " ['it', \"'\", 's', 'dead', '.'],\n",
              " ['it', \"'\", 's', 'time', '.'],\n",
              " ['it', \"'\", 's', 'work', '.'],\n",
              " ['keep', 'it', '.'],\n",
              " ['keep', 'them', '.'],\n",
              " ['leave', 'now', '.'],\n",
              " ['leave', 'now', '.'],\n",
              " ['let', 'me', 'go', 'and', 'go', '.'],\n",
              " ['let', 'me', 'go', '!'],\n",
              " ['let', 'me', 'go', '.'],\n",
              " ['let', 'me', 'go', '.'],\n",
              " ['let', 'me', 'in', '.'],\n",
              " ['let', 'me', 'in', '.'],\n",
              " ['let', 'us', 'go', '.'],\n",
              " ['let', 'us', 'go', '.'],\n",
              " ['let', 'us', 'go', '.'],\n",
              " ['let', 'us', 'go', '.'],\n",
              " ['let', 'us', 'in', '.'],\n",
              " ['let', 'us', 'in', '.'],\n",
              " ['either', 'ask', 'me', '.'],\n",
              " ['look', 'back', '!'],\n",
              " ['stop', 'back', '!'],\n",
              " ['look', 'behind', 'you', '.'],\n",
              " ['stop', 'you', '.'],\n",
              " ['read', 'this', '.'],\n",
              " ['read', 'this', '.'],\n",
              " ['search', 'me', '.'],\n",
              " ['search', 'me', '.'],\n",
              " ['stop', 'again', '.'],\n",
              " ['see', 'it', 'down', '.'],\n",
              " ['she', 'cried', '.'],\n",
              " ['she', 'cried', '.'],\n",
              " ['she', 'tried', '.'],\n",
              " ['she', 'walks', '.'],\n",
              " ['she', 'walks', '.'],\n",
              " ['here', 'here', '.'],\n",
              " ['here', 'here', '.'],\n",
              " ['let', \"'\", 's', 'there', \"'\", 's', 'go', '.'],\n",
              " ['come', 'there', 'there', '.'],\n",
              " ['come', 'now', '.'],\n",
              " ['come', 'now', '.'],\n",
              " ['stay', 'away', '.'],\n",
              " ['stay', 'back', '.'],\n",
              " ['stay', 'home', '.'],\n",
              " ['stay', 'thin', '.'],\n",
              " ['stop', 'him', '.'],\n",
              " ['stop', 'them', '.'],\n",
              " ['take', 'care', '!'],\n",
              " ['take', 'care', '!'],\n",
              " ['take', 'care', '.'],\n",
              " ['take', 'care', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['take', 'mine', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', '.'],\n",
              " ['here', '.'],\n",
              " ['do', 'you', 'do', '.'],\n",
              " ['that', \"'\", 's', '.'],\n",
              " ['that', \"'\", 's', 'that', '.'],\n",
              " ['they', 'fell', '.'],\n",
              " ['they', 'fell', '.'],\n",
              " ['let', 'go', 'out', '.'],\n",
              " ['they', 'left', '.'],\n",
              " ['they', 'lied', '.'],\n",
              " ['they', 'lied', '.'],\n",
              " ['they', 'lost', '.'],\n",
              " ['they', 'lost', '.'],\n",
              " ['tom', 'cried', '.'],\n",
              " ['tom', 'drank', 'the', 'hat', '.'],\n",
              " ['tom', 'knits', '.'],\n",
              " ['tom', 'knows', '.'],\n",
              " ['tom', 'moved', 'to', 'me', '.'],\n",
              " ['tom', 'tried', '.'],\n",
              " ['tom', 'tried', '.'],\n",
              " ['tom', 'tries', '.'],\n",
              " ['tom', 'voted', '.'],\n",
              " ['tom', 'voted', '.'],\n",
              " ['tom', 'walks', '.'],\n",
              " ['tom', 'works', '.'],\n",
              " ['try', 'again', '.'],\n",
              " ['try', 'again', '.'],\n",
              " ['i', 'try', 'it', 'on', '.'],\n",
              " ['i', \"'\", 's', 'on', '.'],\n",
              " ['wait', 'here', '.'],\n",
              " ['wait', 'here', '.'],\n",
              " ['watch', 'tom', '.'],\n",
              " ['give', 'tom', '.'],\n",
              " ['we', 'saw', 'it', '.'],\n",
              " ['we', 'saw', 'you', '.'],\n",
              " ['we', 'saw', 'it', '.'],\n",
              " ['we', 'saw', 'you', '.'],\n",
              " ['we', 'saw', 'it', '.'],\n",
              " ['we', 'saw', 'it', '.'],\n",
              " ['we', 'talked', '.'],\n",
              " ['we', 'waited', '.'],\n",
              " ['we', 'waited', '.'],\n",
              " ['we', 'waited', '.'],\n",
              " ['we', 'waited', '.'],\n",
              " ['we', \"'\", 're', 'shy', '.'],\n",
              " ['we', \"'\", 're', 'shy', '.'],\n",
              " ['that', 'a', 'aunt', 'loves', '!'],\n",
              " ['what', \"'\", 's', 'going', 'on', '?'],\n",
              " ['what', \"'\", 's', 'up', '?'],\n",
              " ['what', 'did', 'you', 'do', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'the', 'pain', '?'],\n",
              " ['who', 'do', 'you', 'know', '?'],\n",
              " ['who', 'knows', 'anybody', '?'],\n",
              " ['who', \"'\", 's', 'go', '?'],\n",
              " ['who', \"'\", 's', 'tom', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'she', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['you', 'drive', '.'],\n",
              " ['you', 'drive', '.'],\n",
              " ['you', 'idiot', '!'],\n",
              " ['you', 'tried', '.'],\n",
              " ['you', 'tried', '.'],\n",
              " ['are', 'you', 'all', 'right', '?'],\n",
              " ['are', 'you', 'all', 'right', '?'],\n",
              " ['are', 'you', 'all', 'right', '?'],\n",
              " ['ask', 'anyone', '.'],\n",
              " ['be', 'careful', '!'],\n",
              " ['come', 'on', 'time', '.'],\n",
              " ['come', 'on', 'time', '.'],\n",
              " ['birds', 'sing', '.'],\n",
              " ['bring', 'wine', '.'],\n",
              " ['leave', 'wine', '.'],\n",
              " ['come', 'again', '.'],\n",
              " ['come', 'again', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['come', 'back', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['come', 'back', '!'],\n",
              " ['do', 'men', 'cry', '?'],\n",
              " ['do', 'people', 'cry', '?'],\n",
              " ['don', \"'\", 't', 'you', 'don', \"'\", 't', 'look', 'at', 'it', '.'],\n",
              " ['don', \"'\", 't', 'forget', 'it', 'to', '.'],\n",
              " ['don', \"'\", 't', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'talk', 'of', 'me', '!'],\n",
              " ['don', \"'\", 't', 'talk', 'of', 'me', '!'],\n",
              " ['don', \"'\", 't', 'talk', 'to', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'talk', 'to', 'tom', '.'],\n",
              " ['do', 'it', 'up', '.'],\n",
              " ['fire', 'burns', '.'],\n",
              " ['fire', 'burns', '.'],\n",
              " ['forget', 'tom', '.'],\n",
              " ['forget', 'tom', '.'],\n",
              " ['forget', 'tom', '.'],\n",
              " ['forget', 'him', '.'],\n",
              " ['forget', 'him', '.'],\n",
              " ['forget', 'him', '.'],\n",
              " ['forget', 'him', '.'],\n",
              " ['there', \"'\", 't', 'get', '.'],\n",
              " ['he', 'is', 'here', '!'],\n",
              " ['here', 'is', 'here', '.'],\n",
              " ['it', 'is', 'here', '!'],\n",
              " ['he', 'is', 'late', '.'],\n",
              " ['he', 'is', 'nice', '.'],\n",
              " ['they', \"'\", 're', 'nice', '.'],\n",
              " ['he', 'laughed', '.'],\n",
              " ['they', 'laughed', '.'],\n",
              " ['he', \"'\", 's', 'swiss', '.'],\n",
              " ['he', \"'\", 's', 'smart', '.'],\n",
              " ['they', \"'\", 're', 'smart', '.'],\n",
              " ['don', \"'\", 't', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'tom', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'man', '.'],\n",
              " ['i', 'am', 'a', 'man', '.'],\n",
              " ['i', \"'\", 'm', 'game', '.'],\n",
              " ['i', 'can', 'read', '.'],\n",
              " ['i', 'can', 'read', '.'],\n",
              " ['i', 'can', 'read', '.'],\n",
              " ['i', 'can', 'swim', '.'],\n",
              " ['i', 'can', 'swim', '.'],\n",
              " ['i', 'can', 'swim', '.'],\n",
              " ['i', 'can', 'walk', '.'],\n",
              " ['i', 'can', 'walk', '.'],\n",
              " ['i', 'eat', 'here', '.'],\n",
              " ['i', 'eat', 'here', '.'],\n",
              " ['i', 'eat', 'meat', '.'],\n",
              " ['i', 'eat', 'meat', '.'],\n",
              " ['i', 'eat', 'rice', '.'],\n",
              " ['i', 'eat', 'rice', '.'],\n",
              " ['i', 'help', 'tom', '.'],\n",
              " ['i', 'help', 'tom', '.'],\n",
              " ['i', 'just', 'ate', '.'],\n",
              " ['i', 'like', 'that', 'i', '.'],\n",
              " ['i', 'like', 'them', '.'],\n",
              " ['i', 'like', 'tea', '.'],\n",
              " ['i', 'like', 'you', '.'],\n",
              " ['i', 'like', 'you', '.'],\n",
              " ['i', 'like', 'you', '.'],\n",
              " ['i', 'love', 'you', '.'],\n",
              " ['i', 'love', 'you', '.'],\n",
              " ['i', 'made', 'tea', '.'],\n",
              " ['i', 'miss', 'you', '.'],\n",
              " ['i', 'miss', 'you', '.'],\n",
              " ['i', 'ran', 'away', '.'],\n",
              " ['i', 'ran', 'away', '.'],\n",
              " ['i', 'remember', 'it', '.'],\n",
              " ['i', 'remember', '.'],\n",
              " ['i', 'remember', '.'],\n",
              " ['i', 'screamed', '.'],\n",
              " ['i', 'screamed', '.'],\n",
              " ['i', 'see', 'me', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'see', 'them', '.'],\n",
              " ['i', 'shot', 'tom', '.'],\n",
              " ['i', 'want', 'tom', '.'],\n",
              " ['i', 'want', 'tom', '.'],\n",
              " ['i', 'want', 'you', 'back', '.'],\n",
              " ['i', 'want', 'you', '.'],\n",
              " ['i', 'want', 'you', '.'],\n",
              " ['i', 'was', 'good', '.'],\n",
              " ['i', 'was', 'good', '.'],\n",
              " ['i', 'was', 'late', '.'],\n",
              " ['i', 'was', 'sick', '.'],\n",
              " ['i', 'was', 'sick', '.'],\n",
              " ['i', 'work', 'out', '.'],\n",
              " ['i', 'work', 'out', '.'],\n",
              " ['i', \"'\", 'll', 'leave', '.'],\n",
              " ['i', \"'\", 'll', 'start', '.'],\n",
              " ['i', \"'\", 'll', 'start', '.'],\n",
              " ['i', \"'\", 'll', 'start', '.'],\n",
              " ['i', 'am', '30', 'now', '.'],\n",
              " ['i', 'am', '30', 'now', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'liar', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'liar', '.'],\n",
              " ['i', \"'\", 'm', 'a', 'poet', '.'],\n",
              " ['i', \"'\", 'm', 'coming', '.'],\n",
              " ['i', \"'\", 'm', 'coming', '.'],\n",
              " ['i', \"'\", 'm', 'hungry', '!'],\n",
              " ['i', \"'\", 'm', 'hungry', '.'],\n",
              " ['i', \"'\", 'm', 'scared', '.'],\n",
              " ['i', \"'\", 'm', 'scared', 'of', 'it', '.'],\n",
              " ['i', \"'\", 'm', 'sleepy', '!'],\n",
              " ['i', \"'\", 'm', 'so', 'fat', '.'],\n",
              " ['i', \"'\", 'm', 'so', 'fat', '.'],\n",
              " ['i', \"'\", 'm', 'trying', '.'],\n",
              " ['i', \"'\", 'm', 'trying', '.'],\n",
              " ['is', 'tom', 'big', '?'],\n",
              " ['is', 'he', 'tall', '?'],\n",
              " ['is', 'he', 'tall', '?'],\n",
              " ['is', 'it', 'done', '?'],\n",
              " ['is', 'it', 'free', '?'],\n",
              " ['is', 'it', 'free', '?'],\n",
              " ['is', 'it', 'hard', 'to', 'do', '?'],\n",
              " ['is', 'it', 'here', '?'],\n",
              " ['is', 'it', 'nice', '?'],\n",
              " ['is', 'there', 'time', '?'],\n",
              " ['is', 'this', 'true', '?'],\n",
              " ['is', 'this', '?'],\n",
              " ['is', 'this', '?'],\n",
              " ['is', 'this', '?'],\n",
              " ['is', 'it', 'true', '?'],\n",
              " ['is', 'this', '?'],\n",
              " ['it', \"'\", 's', 'alive', '.'],\n",
              " ['it', \"'\", 's', 'my', 'cd', '.'],\n",
              " ['it', \"'\", 's', 'my', 'cd', '.'],\n",
              " ['it', 'is', 'already', '.'],\n",
              " ['it', \"'\", 's', 'ready', '.'],\n",
              " ['it', 'is', 'white', '.'],\n",
              " ['it', \"'\", 's', 'white', '.'],\n",
              " ['it', 'is', 'white', '.'],\n",
              " ['it', \"'\", 's', 'white', '.'],\n",
              " ['it', \"'\", 's', 'not', 'yours', '.'],\n",
              " ['it', \"'\", 's', 'yours', '.'],\n",
              " ['it', \"'\", 's', 'yours', '.'],\n",
              " ['it', \"'\", 's', 'yours', 'is', '.'],\n",
              " ['eat', 'half', 'of', 'me', '.'],\n",
              " ['never', 'run', '!'],\n",
              " ['keep', 'quiet', '!'],\n",
              " ['keep', 'calm', '!'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['let', 'tom', 'to', 'tom', '.'],\n",
              " ['let', 'tom', 'go', '.'],\n",
              " ['let', 'tom', 'go', '.'],\n",
              " ['let', 'tom', 'go', '.'],\n",
              " ['let', 'tom', 'in', '.'],\n",
              " ['let', 'tom', 'in', '.'],\n",
              " ['let', 'him', '!'],\n",
              " ['let', 'him', 'go', '!'],\n",
              " ['let', 'me', 'take', 'it', '.'],\n",
              " ['let', 'me', 'take', 'it', '.'],\n",
              " ['let', \"'\", 's', 'chat', '.'],\n",
              " ['let', \"'\", 's', 'kiss', '.'],\n",
              " ['let', \"'\", 's', 'talk', 'about', '.'],\n",
              " ['leave', 'again', '.'],\n",
              " ['leave', 'tom', '.'],\n",
              " ['stop', 'it', '.'],\n",
              " ['stop', 'it', '.'],\n",
              " ['take', 'it', '.'],\n",
              " ['look', 'at', 'me', '.'],\n",
              " ['look', 'at', 'me', '.'],\n",
              " ['look', 'there', '.'],\n",
              " ['look', 'there', '.'],\n",
              " ['love', 'love', '.'],\n",
              " ['never', 'leave', '!'],\n",
              " ['let', 'go', '!'],\n",
              " ['let', \"'\", 's', 'go', 'out', '.'],\n",
              " ['let', \"'\", 's', 'go', 'out', '.'],\n",
              " ['no', 'one', 'ran', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['keep', 'quiet', '.'],\n",
              " ['shadow', 'him', '.'],\n",
              " ['shadow', 'him', '.'],\n",
              " ['shadow', 'him', '.'],\n",
              " ['shadow', 'him', '.'],\n",
              " ['she', 'is', 'old', '.'],\n",
              " ['she', 'is', 'old', '.'],\n",
              " ['smell', 'this', '.'],\n",
              " ['smell', 'this', '.'],\n",
              " ['stand'],\n",
              " ['stand', 'back', '.'],\n",
              " ['here', '.'],\n",
              " ['here', '.'],\n",
              " ['here', '.'],\n",
              " ['here', '.'],\n",
              " ['start', 'here', '.'],\n",
              " ['start', 'here', '.'],\n",
              " ['stay', 'away', '.'],\n",
              " ['stay', 'away', '.'],\n",
              " ['stay', 'still', '.'],\n",
              " ['shut', 'it', '!'],\n",
              " ['shut', 'up', '!'],\n",
              " ['study', 'hard', '.'],\n",
              " ['study', 'hard', '.'],\n",
              " ['take', 'a', 'bus', '.'],\n",
              " ['take', 'a', 'bus', '.'],\n",
              " ['take', 'a', 'bus', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['here', \"'\", 's', '.'],\n",
              " ['take', 'these', '.'],\n",
              " ['take', 'these', '.'],\n",
              " ['talk', 'to', 'me', '.'],\n",
              " ['come', 'with', 'us', 'with', 'us', '.'],\n",
              " ['that', 'a', 'aunt', 'loves', '!'],\n",
              " ['he', 'is', 'tom', '.'],\n",
              " ['he', \"'\", 's', 'tom', '.'],\n",
              " ['that', \"'\", 's', 'wet', '.'],\n",
              " ['they', 'stood', '.'],\n",
              " ['they', 'stood', '.'],\n",
              " ['they', 'tried', '.'],\n",
              " ['i', \"'\", 'll', 'catch', 'me', '.'],\n",
              " ['i', \"'\", 'll', 'catch', 'me', '.'],\n",
              " ['time', 'is', 'time', '.'],\n",
              " ['it', \"'\", 's', 'time', '.'],\n",
              " ['tom', 'bit', 'me', '.'],\n",
              " ['tom', 'burped', '.'],\n",
              " ['tom', 'phoned', '.'],\n",
              " ['tom', 'called', '.'],\n",
              " ['tom', 'danced', '.'],\n",
              " ['tom', 'drinks', '.'],\n",
              " ['tom', 'failed', '.'],\n",
              " ['tom', 'failed', '.'],\n",
              " ['tom', 'forgot', '.'],\n",
              " ['tom', 'fought', '.'],\n",
              " ['tom', 'helped', '.'],\n",
              " ['tom', 'is', 'fat', '.'],\n",
              " ['tom', 'is', 'ill', '.'],\n",
              " ['tom', 'is', 'outside', '.'],\n",
              " ['tom', 'is', 'out', '.'],\n",
              " ['tom', 'jumped', '.'],\n",
              " ['tom', 'looked', '.'],\n",
              " ['tom', 'moaned', '.'],\n",
              " ['tom', 'phoned', '.'],\n",
              " ['tom', 'saw', 'me', '.'],\n",
              " ['tom', 'saw', 'me', '.'],\n",
              " ['tom', 'shaved', '.'],\n",
              " ['tom', 'snores', '.'],\n",
              " ['tom', 'talked', '.'],\n",
              " ['tom', 'waited', '.'],\n",
              " ['tom', 'waited', '.'],\n",
              " ['tom', 'yawned', '.'],\n",
              " ['tom', 'yelled', '.'],\n",
              " ['tom', \"'\", 'll', 'die', '.'],\n",
              " ['tom', \"'\", 's', 'deaf', '.'],\n",
              " ['tom', 'is', 'ill', '.'],\n",
              " ['tom', \"'\", 's', 'ugly', '.'],\n",
              " ['go', '.'],\n",
              " ['go', 'home', '.'],\n",
              " ['take', 'a', 'look', '.'],\n",
              " ['take', 'you', '.'],\n",
              " ['we', 'are', 'men', '.'],\n",
              " ['we', 'are', 'men', '.'],\n",
              " ['we', 'had', 'fun', '.'],\n",
              " ['we', 'had', 'fun', '.'],\n",
              " ['we', 'laughed', '.'],\n",
              " ['we', 'laughed', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'like', 'it', '.'],\n",
              " ['we', 'want', 'it', '.'],\n",
              " ['we', 'want', 'it', '.'],\n",
              " ['we', 'want', 'it', '.'],\n",
              " ['we', \"'\", 'll', 'help', '.'],\n",
              " ['we', \"'\", 'll', 'help', '.'],\n",
              " ['we', \"'\", 'll', 'sing', '.'],\n",
              " ['we', \"'\", 'll', 'sing', '.'],\n",
              " ['we', \"'\", 'll', 'wait', '.'],\n",
              " ['we', \"'\", 'll', 'wait', '.'],\n",
              " ['we', \"'\", 'll', 'work', '.'],\n",
              " ['we', \"'\", 're', 'back', 'now', '.'],\n",
              " ['we', \"'\", 're', 'back', 'back', '.'],\n",
              " ['we', \"'\", 're', 'boys', '.'],\n",
              " ['we', \"'\", 're', 'boys', '.'],\n",
              " ['we', \"'\", 're', 'fine', '.'],\n",
              " ['we', \"'\", 're', 'fine', '.'],\n",
              " ['we', \"'\", 're', 'fine', '.'],\n",
              " ['we', \"'\", 're', 'going', 'anywhere', '.'],\n",
              " ['where', 'am', 'i', 'have', '?'],\n",
              " ['who', 'are', 'we', '?'],\n",
              " ['who', 'are', 'we', '?'],\n",
              " ['who', 'did', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', 'has', 'anybody', '?'],\n",
              " ['who', \"'\", 's', 'tom', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'she', '?'],\n",
              " ['who', 'phoned', '?'],\n",
              " ['who', 'was', '?'],\n",
              " ['who', 'was', 'it', '?'],\n",
              " ['who', \"'\", 's', 'shot', '?'],\n",
              " ['who', \"'\", 's', 'here', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['who', \"'\", 's', 'that', '?'],\n",
              " ['wood', 'burns', '.'],\n",
              " ['you', 'do', 'you', '.'],\n",
              " ['you', 'decide', 'why', '.'],\n",
              " ['you', 'decide', '.'],\n",
              " ['you', 'decide', '.'],\n",
              " ['you', 'may', 'go', '.'],\n",
              " ['you', 'may', 'go', '.'],\n",
              " ['you', 'can', 'leave', '.'],\n",
              " ['you', 'may', 'go', '.'],\n",
              " ['you', \"'\", 've', 'won', '.'],\n",
              " ['you', \"'\", 've', 'won', '.'],\n",
              " ['you', \"'\", 've', 'won', '.'],\n",
              " ['is', 'anybody', 'home', '?'],\n",
              " ['why', 'did', 'anyone', 'hurt', '?'],\n",
              " ['is', 'it', 'done', '?'],\n",
              " ['are', 'you', 'tom', '?'],\n",
              " ['are', 'you', 'tom', '?'],\n",
              " ['are', 'you', 'tom', '?'],\n",
              " ['are', 'you', 'tom', '?'],\n",
              " ['are', 'you', 'new', '?'],\n",
              " ['are', 'you', 'new', '?'],\n",
              " ['are', 'you', 'new', '?'],\n",
              " ['be', 'prepared', '.'],\n",
              " ['breathe', 'out', '.'],\n",
              " ['breathe', 'out', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['come', 'in', '.'],\n",
              " ['did', 'tom', 'die', '?'],\n",
              " ['did', 'tom', 'die', '?'],\n",
              " ['did', 'tom', 'eat', '?'],\n",
              " ['did', 'tom', 'win', '?'],\n",
              " ['don', \"'\", 't', 'argue', '?'],\n",
              " ['don', \"'\", 't', 'argue', '?'],\n",
              " ['don', \"'\", 't', 'argue', 'with', 'tom', '.'],\n",
              " ['don', \"'\", 't', 'argue', '.'],\n",
              " ['don', \"'\", 't', 'argue', '.'],\n",
              " ['don', \"'\", 't', 'you', 'don', \"'\", 's', 'broken', '.'],\n",
              " ['don', \"'\", 't', 'you', 'don', \"'\", 's', 'broken', '.'],\n",
              " ['don', \"'\", 't', 'fight', '.'],\n",
              " ['don', \"'\", 't', 'leave', 'it', '.'],\n",
              " ['don', \"'\", 't', 'leave', '.'],\n",
              " ['don', \"'\", 't', 'leave', 'it', '.'],\n",
              " ['don', \"'\", 't', 'leave', '.'],\n",
              " ['don', \"'\", 't', 'shout', '.'],\n",
              " ['don', \"'\", 't', 'shout', '.'],\n",
              " ['let', \"'\", 's', 'with', 'us', 'with', 'us', '.'],\n",
              " ['forgive', 'tom', '.'],\n",
              " ['forgive', 'tom', '.'],\n",
              " ['get', 'using', 'cake', '.'],\n",
              " ['change', 'your', 'clothes', '.'],\n",
              " ['either', '.'],\n",
              " ['get', 'started', '.'],\n",
              " ['get', 'started', '.'],\n",
              " ['get', 'started', '.'],\n",
              " ['go', 'to', '.'],\n",
              " ['go', 'have', 'come', 'to', 'go', '.'],\n",
              " ['go', 'help', 'tom', '.'],\n",
              " ['let', \"'\", 's', 'bus', '.'],\n",
              " ['come', 'put', '.'],\n",
              " ['he', 'can', 'come', '.'],\n",
              " ['he', 'can', 'come', '.'],\n",
              " ['he', 'can', 'read', 'it', '.'],\n",
              " ['he', 'can', 'read', '.'],\n",
              " ['he', 'found', 'it', '.'],\n",
              " ['they', 'found', 'it', '.'],\n",
              " ['he', 'has', 'wine', 'wine', '.'],\n",
              " ['he', 'is', 'happy', '.'],\n",
              " ['he', 'is', 'happy', '.'],\n",
              " ['he', 'is', 'lying', '.'],\n",
              " ['they', \"'\", 're', 'lying', '.'],\n",
              " ['he', 'is', 'young', '.'],\n",
              " ['he', 'resigned', '.'],\n",
              " ['he', 'resigned', '.'],\n",
              " ['he', 'stood', 'up', '.'],\n",
              " ['he', 'stood', 'up', '.'],\n",
              " ['he', 'stood', 'up', '.'],\n",
              " ['they', 'stood', '.'],\n",
              " ['how', 'many', 'you', 'know', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'are', 'you', '?'],\n",
              " ['how', 'strange', '!'],\n",
              " ['i', 'am', 'taller', '.'],\n",
              " ['i', 'came', 'back', '.'],\n",
              " ['i', 'came', 'back', '.'],\n",
              " ['i', 'can', 'dance', '.'],\n",
              " ['i', 'can', 'dance', '.'],\n",
              " ['i', 'can', 'dance', '.'],\n",
              " ['i', 'can', 'do', 'it', '.'],\n",
              " ['i', 'can', 'do', 'it', '.'],\n",
              " ['i', 'can', \"'\", 't', 'fly', '.'],\n",
              " ['i', 'can', \"'\", 't', 'say', '.'],\n",
              " ['i', 'can', \"'\", 't', 'tell', 'me', '.'],\n",
              " ['i', 'can', \"'\", 't', 'tell', 'you', '.'],\n",
              " ['i', 'don', \"'\", 't', 'cry', '.'],\n",
              " ['i', 'don', \"'\", 't', 'eat', 'it', '.'],\n",
              " ['i', 'drank', 'tea', '.'],\n",
              " ['i', 'drank', 'tea', '.'],\n",
              " ['i', 'eat', 'bread', '.'],\n",
              " ['i', 'eat', 'bread', '.'],\n",
              " ['i', 'eat', 'bread', '.'],\n",
              " ['i', 'eat', 'bread', '.'],\n",
              " ['i', 'eat', 'fruit', '.'],\n",
              " ['i', 'eat', 'fruit', '.'],\n",
              " ['i', 'exercised', '.'],\n",
              " ['i', 'feel', 'fine', '.'],\n",
              " ['i', 'felt', 'that', '.'],\n",
              " ['i', 'forgot', 'it', '.'],\n",
              " ['i', 'forgot', 'it', '.'],\n",
              " ['i', 'had', 'a', 'cat', '.'],\n",
              " ['i', 'have', 'time', '.'],\n",
              " ['i', 'have', 'wine', 'wine', '.'],\n",
              " ['i', 'knew', 'that', '.'],\n",
              " ['i', 'know', 'this', '.'],\n",
              " ['i', 'left', 'home', '.'],\n",
              " ['i', 'left', 'home', '.'],\n",
              " ['i', 'left', 'home', '.'],\n",
              " ['i', 'left', 'home', '.'],\n",
              " ['i', 'like', 'beer', '.'],\n",
              " ['i', 'like', 'both', '.'],\n",
              " ['i', 'like', 'cake', '.'],\n",
              " ['i', 'like', 'cats', '.'],\n",
              " ['i', 'like', 'fish', '.'],\n",
              " ['i', 'like', 'math', '.'],\n",
              " ['i', 'like', 'rice', '.'],\n",
              " ['i', 'like', 'that', 'i', '.'],\n",
              " ['i', 'like', 'that', 'likes', 'it', '.'],\n",
              " ['i', 'like', 'that', '.'],\n",
              " ['i', 'made', 'that', '.'],\n",
              " ['i', 'made', 'that', '.'],\n",
              " ['i', 'need', 'time', '.'],\n",
              " ['i', 'never', 'cry', '.'],\n",
              " ['i', 'saved', 'you', '.'],\n",
              " ['i', 'saved', 'you', '.'],\n",
              " ['i', 'smell', 'gas', '.'],\n",
              " ['i', 'trust', 'tom', '.'],\n",
              " ['i', 'want', 'more', '.'],\n",
              " ['i', 'want', 'more', '.'],\n",
              " ['i', 'want', 'them', '.'],\n",
              " ['i', 'want', 'them', '.'],\n",
              " ['i', 'want', 'them', '.'],\n",
              " ['i', 'want', 'this', '.'],\n",
              " ['i', 'want', 'this', '.'],\n",
              " ['i', 'want', 'time', '.'],\n",
              " ['i', 'was', 'alone', '.'],\n",
              " ['i', 'was', 'alone', '.'],\n",
              " ['i', 'was', 'naked', '.'],\n",
              " ['i', 'was', 'naked', '.'],\n",
              " ['i', 'was', 'naked', '.'],\n",
              " ['i', 'was', 'naked', '.'],\n",
              " ['i', 'was', 'stuck', '.'],\n",
              " ['i', 'was', 'stuck', '.'],\n",
              " ['i', 'was', 'tired', '.'],\n",
              " ['i', 'was', 'tired', '.'],\n",
              " ['i', 'went', ',', 'too', '.'],\n",
              " ['i', 'went', ',', 'too', '.'],\n",
              " ['i', \"'\", 'll', 'work', '.'],\n",
              " ['i', 'won', \"'\", 'm', 'not', 'lie', '.'],\n",
              " ['i', \"'\", 'll', 'buy', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'decide', '.'],\n",
              " ['i', \"'\", 'll', 'decide', '.'],\n",
              " ['i', \"'\", 'll', 'eat', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'finish', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'me', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'get', 'it', '.'],\n",
              " ['i', \"'\", 'll', 'scream', '.'],\n",
              " ['i', \"'\", 'll', 'dance', '.'],\n",
              " ['i', \"'\", 'm', 'dancing', '.'],\n",
              " ['i', \"'\", 'm', 'dieting', '.'],\n",
              " ['i', \"'\", 'm', 'dieting', '.'],\n",
              " ['i', \"'\", 'm', 'falling', '.'],\n",
              " ['i', \"'\", 'm', 'falling', '.'],\n",
              " ['i', \"'\", 'm', 'healthy', '.'],\n",
              " ['i', \"'\", 'm', 'healthy', '.'],\n",
              " ['i', \"'\", 'm', 'in', 'jail', '.'],\n",
              " ['i', \"'\", 'm', 'in', 'jail', '.'],\n",
              " ['i', \"'\", 'm', 'not', 'tom', '.'],\n",
              " ['i', \"'\", 'm', 'popular', '.'],\n",
              " ['i', \"'\", 'm', 'reading', '.'],\n",
              " ['i', \"'\", 'm', 'reading', '.'],\n",
              " ['i', \"'\", 'm', 'resting', '.'],\n",
              " ['i', \"'\", 'm', 'resting', '.'],\n",
              " ['i', \"'\", 'm', 'so', 'full', '.'],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0InokXUYcOT",
        "outputId": "1cc54233-226b-4a2f-a559-1bfae02b0200"
      },
      "source": [
        "bleu_score(preds, trgs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8057653903961182"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlRo6ULbG_nZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BOwKeD-Uky6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}